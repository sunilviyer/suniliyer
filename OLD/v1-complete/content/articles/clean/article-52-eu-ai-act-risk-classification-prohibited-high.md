
![Article 52: EU AI Act Risk Classification ‚Äì Prohibited, High-Risk, and Beyond]({{IMAGE_PLACEHOLDER_article-52-eu-ai-act-risk-classification-prohibited-high-ris}})

---
category: "AI Risks & Principles"
learning_objectives:

  - "Understand the key concepts and principles of regulatory requirements"
  - "Implement best practices in real-world scenarios"
  - "Evaluate risk assessment frameworks for organizational compliance"
seo_keywords:

  - "article"
  - "risk"
  - "artificial intelligence"
  - "act risk classification"
  - "prohibited"
word_count: 2088
processed_date: "2025-12-18T20:00:53.876Z"
---


## The Risk Pyramid: Four Levels

Think of AI regulation under the EU AI Act as a pyramid:

```
          /\
         /  \
        / üö´ \     PROHIBITED (Banned entirely)
       /------\
      /   ‚ö†Ô∏è   \    HIGH-RISK (Heavy regulation)
     /----------\
    /    ‚ìò      \   LIMITED RISK (Transparency rules)
   /--------------\
  /     ‚úì         \  MINIMAL RISK (No AI-specific rules)
 /------------------\
```

The higher you go on the pyramid, the fewer AI systems there are‚Äîbut the stricter the rules.

---


## Level 1: Prohibited AI Practices (The Red Zone)

Some AI applications are considered so dangerous to fundamental rights and democratic values that the EU banned them outright. These aren't just heavily regulated‚Äîthey're **illegal**.


### What's Banned

**1. Social Scoring Systems**
AI that evaluates people's "trustworthiness" based on social behavior and uses that score to treat them differently.

*Real-world example*: China's social credit system, where citizens lose points for things like jaywalking or posting "fake news," and those points affect whether they can get loans, buy plane tickets, or enroll their kids in good schools.

*Why it's banned*: It creates a surveillance society where every action is judged and recorded, chilling free speech and behavior.

**2. Subliminal Manipulation**
AI that manipulates people through techniques they can't consciously perceive.

*Real-world example*: Imagine an AI that uses subliminal audio cues or micro-visual signals to make you buy products or vote for certain candidates without you realizing you're being influenced.

*Why it's banned*: People can't protect themselves from manipulation they don't know is happening.

**3. Exploitation of Vulnerabilities**
AI that specifically targets children, elderly people, or people with disabilities to manipulate their behavior in harmful ways.

*Real-world example*: An AI toy that encourages children to engage in dangerous behavior, or an AI assistant that exploits a confused elderly person to make purchases.

*Why it's banned*: It preys on those least able to protect themselves.

**4. Real-Time Biometric Identification in Public Spaces (for law enforcement)**
Facial recognition cameras scanning everyone in a public square to identify specific individuals.

*Limited exceptions*: Finding kidnapping victims, preventing imminent terrorist attacks, or locating suspects in serious crimes‚Äîbut only with judicial authorization and strict safeguards.

*Why it's largely banned*: Mass surveillance threatens privacy and freedom of assembly for everyone, not just criminals.

**5. Emotion Recognition in Workplaces and Schools**
AI that infers emotions from biometric data (facial expressions, voice analysis) in employment and educational contexts.

*Real-world example*: Software that monitors whether call center employees are smiling enough, or analyzes student facial expressions to gauge engagement.

*Why it's banned*: It creates coercive environments where people feel constantly watched and judged for their emotional states.

**6. Biometric Categorization Based on Sensitive Characteristics**
AI that categorizes people by race, political opinions, sexual orientation, or religious beliefs using biometric data.

*Real-world example*: AI that claims to identify sexual orientation from facial features, or systems that categorize people by ethnicity from photos.

*Why it's banned*: It enables discrimination and violates human dignity.

**7. Untargeted Scraping for Facial Recognition Databases**
Creating databases of faces by scraping images from the internet or CCTV without consent.

*Real-world example*: Clearview AI scraped billions of photos from social media to create a facial recognition database sold to law enforcement.

*Why it's banned*: It violates privacy rights and enables mass surveillance without consent.


### Timeline

These prohibitions took effect on **February 2, 2025**.

---


## Level 2: High-Risk AI Systems (The Yellow Zone)

This is where most of the regulatory action happens. High-risk AI systems aren't banned, but they face **extensive requirements** before and after being placed on the market.


### What Makes AI "High-Risk"?

The EU AI Act uses two pathways to classify AI as high-risk:

**Pathway 1: Safety Components (Annex I)**
AI that serves as a safety component of products already covered by EU product safety legislation:

- Medical devices
- Vehicles
- Toys
- Aviation equipment
- Machinery
- Marine equipment

*Example*: The AI system that helps an autonomous vehicle decide when to brake.

**Pathway 2: Specific Use Cases (Annex III)**
AI used in these sensitive areas, regardless of what product it's part of:

| Area | Examples |
|------|----------|
| **Biometrics** | Facial recognition, fingerprint matching (where allowed) |
| **Critical Infrastructure** | AI managing electricity grids, water systems, traffic |
| **Education** | AI that grades exams, admits students, detects cheating |
| **Employment** | Resume screening, interview analysis, performance monitoring |
| **Essential Services** | Credit scoring, loan approval, insurance pricing |
| **Law Enforcement** | Risk assessment tools, evidence analysis, crime prediction |
| **Migration/Border Control** | Visa application assessment, document verification |
| **Justice** | AI assisting judges with sentencing or case research |
| **Democratic Processes** | AI used to influence voting behavior |


### Important Exception

Even if an AI system falls into an Annex III category, it's NOT considered high-risk if it:

- Performs a narrow procedural task
- Improves the result of a previously completed human activity
- Detects decision-making patterns without replacing human judgment
- Only performs preparatory tasks for assessments

*Example*: AI that formats a resume for easier reading isn't high-risk, but AI that scores candidates is.


### What High-Risk Providers Must Do

If your AI is high-risk, you must comply with requirements we'll cover in detail in Article 53, including:

1. Establish a risk management system
2. Meet data quality and governance requirements
3. Create and maintain technical documentation
4. Enable record-keeping (logging)
5. Provide transparency and information to users
6. Allow for human oversight
7. Ensure accuracy, robustness, and cybersecurity
8. Conduct conformity assessment before market placement

---


## Level 3: Limited Risk AI (The Amber Zone)

Some AI systems don't face the full weight of high-risk requirements but still need to meet **specific transparency obligations**.


### The Transparency Requirement

The principle is simple: **people should know when they're interacting with AI or when content was created by AI**.


### What Systems Are Covered

**1. AI Systems Interacting with Humans (Chatbots)**
If someone is talking to an AI, they must be told they're talking to an AI.

*Example*: A customer service chatbot must clearly state "You are chatting with an AI assistant" or similar disclosure.

*Exception*: If it's obvious from the context (like a clearly-labeled AI character in a video game).

**2. Emotion Recognition and Biometric Categorization Systems**
If an AI is analyzing someone's emotions or categorizing them biometrically, that person must be informed.

*Example*: If a store uses AI to analyze customer emotions, signage must inform customers.

*Note*: Remember, emotion recognition in workplaces and schools is banned entirely‚Äîthis applies to other contexts where it's permitted.

**3. Deepfakes and AI-Generated Content**
Content that's artificially generated or manipulated must be labeled as such.

*Example*: An AI-generated image of a public figure must be marked as artificially created.

*Exception*: Artistic or satirical work where the artificial nature is part of the creative intent.

**4. AI-Generated Text for Public Information**
Text generated by AI on matters of public interest must be disclosed as AI-generated.

*Example*: An AI-written news article must disclose that it was generated by artificial intelligence.

---


## Level 4: Minimal Risk AI (The Green Zone)

The vast majority of AI systems fall here and face **no AI-specific regulations** under the EU AI Act.


### Examples of Minimal Risk AI

- Spam filters
- Video game AI
- Inventory management systems
- Music recommendation algorithms
- Photo filters that add bunny ears
- Predictive text on your phone
- AI-powered search engines (for general search)


### Why No Special Rules?

These systems pose minimal risk to fundamental rights. If something goes wrong‚Äîyour spam filter misses an email, or your game's AI opponent does something weird‚Äîthe consequences are minor.

That doesn't mean these systems are completely unregulated. They still must comply with:

- Existing consumer protection laws
- Data protection laws (GDPR)
- Product safety regulations
- Competition law

They just don't have AI-specific obligations under the EU AI Act.

---


## How to Classify Your AI System: A Practical Walkthrough

Let's say you've developed an AI system and need to figure out its classification. Here's the step-by-step process:


### Step 1: Check the Prohibited List

Go through the list of prohibited practices. Does your AI:

- Assign social scores?
- Use subliminal techniques?
- Exploit vulnerable groups?
- Conduct real-time biometric ID in public (without qualifying exceptions)?
- Do emotion recognition in workplaces/schools?
- Categorize people by sensitive traits using biometrics?
- Scrape faces for databases?

If yes to any: **STOP. Your AI is prohibited.**


### Step 2: Check Annex I (Safety Components)

Is your AI a safety component of a product covered by EU product safety legislation?

If yes: **Your AI is high-risk.**


### Step 3: Check Annex III (Sensitive Uses)

Is your AI used in any of the eight sensitive areas listed in Annex III?

If yes, consider: Does it qualify for an exception (narrow procedural task, etc.)?

If no exception applies: **Your AI is high-risk.**


### Step 4: Check Transparency Obligations

Is your AI:

- Interacting with humans (chatbot-style)?
- Recognizing emotions or categorizing biometrics?
- Generating or manipulating content (deepfakes)?

If yes: **Your AI has limited-risk transparency obligations.**


### Step 5: Default to Minimal Risk

If none of the above apply: **Your AI is minimal risk.**

---


## Real-World Classification Examples

| AI System | Classification | Reasoning |
|-----------|---------------|-----------|
| Resume screening software | High-Risk | Employment decision-making (Annex III) |
| Customer service chatbot | Limited Risk | Interacts with humans; must disclose |
| Credit scoring algorithm | High-Risk | Access to essential services (Annex III) |
| Email spam filter | Minimal Risk | None of the above categories apply |
| Social media content recommendation | Minimal Risk* | General recommendation; not targeting vulnerabilities |
| Student exam proctoring | High-Risk | Educational context (Annex III) |
| Facial recognition door access | High-Risk | Biometric identification (Annex III) |
| AI-generated marketing image | Limited Risk | Must be labeled as AI-generated |
| Self-driving car AI | High-Risk | Safety component (Annex I) |
| Emotion monitoring for drivers | Potentially Prohibited | Depends on context; workplace = banned |

*Note: Large platforms with recommender systems have separate obligations under the EU Digital Services Act.

---


## Common Classification Mistakes


### Mistake 1: Assuming "Internal Use" Means No Regulation

If your company uses AI only internally (not sold to others), you're a "deployer," not a "provider." But high-risk AI used internally still triggers deployer obligations.


### Mistake 2: Thinking "We Just Aggregate Data"

If your AI makes or significantly influences decisions about people, the fact that it's "just" processing data doesn't exempt it.


### Mistake 3: Ignoring Downstream Uses

If you build a foundation model or general-purpose AI, you may have obligations even though you don't control how it's ultimately used.


### Mistake 4: Over-Classifying to Be Safe

Some organizations assume everything is high-risk "just to be safe." This wastes resources and creates unnecessary compliance burdens. Accurate classification matters.

---


## Conclusion

Risk classification is the foundation of EU AI Act compliance. Everything else‚Äîdocumentation, conformity assessment, monitoring‚Äîflows from how your AI system is classified.

The good news: the system is logical. High-risk categories target AI that can significantly affect people's lives: their jobs, their education, their access to services, their freedom. Most AI systems will fall into minimal risk with no special requirements.

The challenge: you need to honestly assess your AI systems against these criteria. Wishful thinking doesn't change your classification‚Äîand regulators will eventually check.

Get the classification right first. Everything else follows from there.

---


## Sources

1. **European Union.** "Regulation (EU) 2024/1689 of the European Parliament and of the Council (EU AI Act)." *Official Journal of the European Union*, Articles 5-7 and Annexes I-III, 2024. [EUR-Lex](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)

2. **European Commission.** "AI Act: Different Rules for Different Risk Levels." 2024. [European Commission Digital Strategy](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)

3. **Future of Life Institute.** "EU Artificial Intelligence Act: Risk-Based Approach." 2024. [artificialintelligenceact.eu](https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-checker/)

4. **IAPP.** "EU AI Act Risk Classification Analysis." 2024. [iapp.org](https://iapp.org/resources/article/eu-ai-act/)

5. **European Parliament.** "Artificial Intelligence Act: Deal on Comprehensive Rules for Trustworthy AI." Press Release, December 2023. [europarl.europa.eu](https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/)

6. **Stanford HAI.** "Analyzing the EU AI Act: What You Need to Know." Human-Centered Artificial Intelligence, 2024. [hai.stanford.edu](https://hai.stanford.edu/)

7. **Veale, Michael and Zuiderveen Borgesius, Frederik.** "Demystifying the Draft EU Artificial Intelligence Act." *Computer Law Review International*, 2021.
