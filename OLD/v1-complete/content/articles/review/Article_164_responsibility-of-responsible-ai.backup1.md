---
title: "Responsibility of Responsible AI for Organizations"
slug: "responsibility-of-responsible-ai"
author: "Sunil Iyer"
date: "2024-05-10"
readTime: "2 min read"
category: "AI Governance"
tags: ["responsible AI", "governance", "ethics", "EU AI Act", "organizational structure"]
featured: false
description: "Understanding why everyone in an organization shares responsibility for AI governance, and the three major pitfalls to avoid when building a governance structure."
---

# Responsibility of Responsible AI for Organizations

> Without proper controls, AI systems can amplify, perpetuate, or exacerbate inequitable or undesirable outcomes for individuals and communities — **NIST**

Responsible AI for an organization is not limited to developers or vendors—rather, everyone has equal responsibility towards adhering to responsible AI. Be it users who identify potential malicious injection, QA who are responsible for quality controls, data scientists who need to ensure data robustness, and more importantly, the leadership who need to set the tone for ethical AI practices.

## The Three Major Pitfalls

The most common pitfalls when it comes to building a governance structure around responsible AI can be classified into three major categories:

- **A) Ill-defined governance structure**
- **B) Unclear governance mandate**
- **C) Lack of foresight**

While most of the current research and frameworks focus on explainability, bias, and transparency, there is limited research on how to address key challenges for an organization when defining a responsible AI governance model.

### Ill-defined Governance Structure

Ensuring involvement of everyone in some capacity, keeping open lines of communication with all users to ensure issues are identified and reported to the relevant teams can only be possible when there is a clear structure around AI business operations.

### Unclear Governance Mandate

Creating a governance structure with limited resources or improper authorization creates unnecessary red-tape around risk mitigation. Empowering the committee to quickly react to potential risks and regulatory changes would allow organizations to simplify AI operations.

### Lack of Foresight

Unethical AI practices and misunderstood biases often cause undue hardship indirectly to people who might not be directly connected to the usage of AI.

**Case Study: Mata vs Avianca Airlines**

In this case, the plaintiff was impacted because the lawyers used ChatGPT to build their case with nonexistent examples. Mr. Mata was impacted by the decision of his lawyers to use ChatGPT and due to ChatGPT's hallucinations which were not recognized by the lawyers.

Ensuring that organizations account for how the data may be used, provide guidance on the trustworthiness of the data, or provide relevant source material to offer explainability are critical when designing a system.

## The EU AI Act Imperative

Under the EU AI Act, non-compliance of the AI system will now incur monetary fines, and as such makes it imperative for organizations to build a robust governance structure to ensure there are appropriate guardrails before considering the use of AI systems to improve business processes and client services.
