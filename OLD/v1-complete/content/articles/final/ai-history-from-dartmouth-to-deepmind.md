---
title: AI History — From Dartmouth to DeepMind
tldr: This article offers practical guidance for implementing AI governance systems.
  It delivers practical tools and strategies for real-world application.
category: AI Fundamentals
learning_objectives:
- Understand the key concepts and principles of ai governance principles
- Implement implementation strategies in real-world scenarios
- Evaluate compliance frameworks for organizational compliance
seo_keywords:
- history
- from
- AI governance
- artificial intelligence
- from dartmouth
components:
- type: image_prompt
  label: Article Hero Image
  section: Header
  id: image-prompt-hero
  prompt: educational diagram showing AI concepts, neural network visualization, technology icons, professional illustration, modern flat design style, clean and authoritative, high quality, blue and gray color scheme with accent colors, suitable for professional article header
  label: Article Hero Image
  section: Header
  id: image-prompt-hero
  prompt: educational diagram showing AI concepts, neural network visualization, technology
    icons, professional illustration, modern flat design style, clean and authoritative,
    high quality, blue and gray color scheme with accent colors, suitable for professional
    article header
- type: template
  label: 'The Transformer Revolution: Attention Changes Everything'
  section: 'The Transformer Revolution: Attention Changes Everything'
  id: template-the-transformer-revolution-attention-changes-everything
  template_link: /templates/the-transformer-revolution-attention-changes-everything.md
topic_fingerprint:
- neural network
- reinforcement learning
- deep learning
- transformer
- claude
named_examples:
- alphago
- deepmind
- google
- ibm
- openai
- stanford
word_count: 1151
processed_date: '2025-12-18T20:04:28.344Z'
---


# AI History — From Dartmouth to DeepMind


## The Summer That Started It All


![AI History — From Dartmouth to DeepMind]({{IMAGE_PLACEHOLDER_ai-history-from-dartmouth-to-deepmind}})

In the summer of 1956, ten scientists gathered at Dartmouth College with a rather ambitious plan: solve artificial intelligence in about eight weeks. Their proposal stated with remarkable confidence that "every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it." They requested funding for a two-month workshop, believing that a "significant advance" could be made if a carefully selected group worked on the problem together.

This might be the most spectacularly wrong time estimate in the history of science. Nearly seven decades later, we're still working on those original goals. But what emerged from that New Hampshire summer wasn't a solution—it was the birth of an entirely new field.


## The Dartmouth Conference: Where AI Got Its Name

The Dartmouth Summer Research Project on Artificial Intelligence ran from June 18 to August 17, 1956. The organizers—John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon—were intellectual heavyweights. McCarthy would later invent the programming language LISP. Shannon had essentially created information theory. These weren't dreamers; they were serious scientists who happened to be wildly optimistic.

The conference didn't produce the breakthrough its organizers envisioned, but it accomplished something arguably more important: it unified a scattered research community under a single banner. Before Dartmouth, researchers working on machine intelligence called their field by various names—cybernetics, automata theory, complex information processing. McCarthy's term "artificial intelligence" stuck, giving the field an identity and a mission.

The early years following Dartmouth buzzed with genuine achievements. Arthur Samuel's checkers program learned to beat its creator. Newell and Simon's Logic Theorist proved mathematical theorems. ELIZA, an early chatbot, convinced some users they were talking to a human therapist. Optimism ran high. Herbert Simon predicted in 1965 that "machines will be capable, within twenty years, of doing any work a man can do."


## The Winters: When Reality Bit Back

Then reality intervened, as it tends to do with overpromised technology.

The first AI Winter arrived in the early 1970s. The Lighthill Report of 1973, commissioned by the British government, delivered a devastating assessment: AI research had failed to deliver on its promises. DARPA funding plummeted from approximately $30 million annually to almost nothing by 1974. The Stanford AI Lab, once a flagship research center, closed in 1979. Researchers found that problems which seemed simple to humans—recognizing faces, understanding natural language, navigating a room—were extraordinarily difficult to program.

A brief revival came in the 1980s with expert systems—programs that captured human expertise in narrow domains. Companies invested billions. Japan launched its Fifth Generation Computer project with $850 million in funding. The United States and Europe scrambled to compete. For a moment, it seemed AI's time had finally arrived.

It hadn't. Expert systems proved brittle and expensive to maintain. Knowledge changed faster than it could be encoded. Markets turned out to be smaller than projected. By 1987, the specialized LISP machine market collapsed almost overnight as cheaper general-purpose computers caught up. The second AI Winter had begun, and it would last nearly a decade.

The term "artificial intelligence" became toxic in funding proposals. Researchers started calling their work "informatics" or "computational intelligence"—anything to avoid the stigma.


## The Milestones: Machines Start Winning

The thaw began with games—specifically, with a chess match that captivated the world.

On May 11, 1997, IBM's Deep Blue defeated world chess champion Garry Kasparov. It was the first time a computer had beaten a reigning world champion under standard tournament conditions. Deep Blue could evaluate 200 million chess positions per second through sheer computational force. Kasparov, stunned by his loss, accused IBM of cheating. He wasn't entirely wrong to be suspicious—there had never been anything like this before.

But Deep Blue was a specialist, and chess, while complex, has clear rules. The ancient Chinese game of Go, with more possible board configurations than atoms in the universe, remained safely beyond computer reach. Experts predicted it would take another decade or more for AI to challenge top human players.

Those experts were wrong by about fifteen years.

In March 2016, Google DeepMind's AlphaGo defeated Lee Sedol, one of the greatest Go players in history, four games to one. Over 200 million people watched online. Unlike Deep Blue's brute-force approach, AlphaGo used neural networks and reinforcement learning—it had taught itself by playing millions of games against itself. In Game Two, AlphaGo played Move 37, a move so unconventional it had a 1 in 10,000 chance of being chosen by a human player. Commentators initially thought it was a mistake. It wasn't. It was creative.

Meanwhile, a quieter revolution had begun four years earlier. In September 2012, a neural network called AlexNet won the ImageNet image recognition challenge by a staggering margin—its error rate was 10.8 percentage points better than the runner-up. AlexNet proved that deep learning, long dismissed as impractical, could achieve breakthrough results when combined with large datasets and powerful GPUs. The researchers—Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton—had trained it on two gaming graphics cards in a bedroom.

<!-- component:template:template-the-transformer-revolution-attention-changes-everything -->

## The Transformer Revolution: Attention Changes Everything

The modern AI explosion traces to a 2017 paper with a clever title: "Attention Is All You Need." Eight Google researchers introduced the Transformer architecture, a new way for neural networks to process sequences of data. Previous approaches required processing information step by step; Transformers could examine entire sequences simultaneously, dramatically speeding up training and improving results.

The paper's authors couldn't have fully anticipated what they'd unleashed. Within a year, OpenAI released GPT-1, a language model built on the Transformer architecture. GPT-2 followed in 2019, then GPT-3 in 2020 with 175 billion parameters—over a thousand times larger than its predecessor.

On November 30, 2022, OpenAI released ChatGPT to the public. Within five days, it had a million users. Within two months, it had 100 million. For the first time, ordinary people could hold conversations with AI systems that felt genuinely intelligent. The AI boom that had been building for a decade exploded into public consciousness.


## From Dartmouth to Today: What Have We Learned?

The scientists at Dartmouth in 1956 weren't wrong about AI's potential—they were wrong about the timeline by roughly six decades. The field's history is a cautionary tale about the gap between vision and implementation, between laboratory demonstrations and real-world deployment.

Each AI Winter taught the same lesson: overpromising leads to underfunding. Each breakthrough—from Deep Blue to AlphaGo to ChatGPT—reminded us that progress often comes suddenly, after long periods of apparent stagnation.

For AI governance professionals, this history matters. We're not dealing with a technology that emerged fully formed; we're dealing with one that has cycled through hype and disappointment for nearly seventy years. The current capabilities are real, but so is the tendency toward overconfidence. Understanding where AI came from helps us navigate where it's going—and perhaps avoid promising we'll solve everything in eight weeks.
