# AI Governance Framework Builder

**Purpose**: Step-by-step guide for organizations to build comprehensive AI governance from scratch
**Use Case**: Governance teams, compliance officers, executives launching AI governance programs
**Based on**: OECD AI Principles, ISO/IEC standards, regulatory best practices

---

## Overview

Building AI governance isn't about creating more bureaucracyâ€”it's about enabling responsible AI deployment at scale. This template guides you through establishing governance that balances innovation with accountability, speed with safety, and opportunity with risk management.

**What You'll Build**:
1. AI governance structure (roles, responsibilities, committees)
2. AI policy framework (principles, policies, procedures)
3. AI inventory and risk assessment process
4. Oversight and monitoring mechanisms
5. Incident response and continuous improvement

**Time Investment**: 3-6 months for initial framework, ongoing refinement

**Prerequisites**:
- Executive sponsorship
- Cross-functional team (legal, IT, risk, ethics, business units)
- Budget for tools, training, potential external support

---

## Phase 1: Foundation (Weeks 1-4)

### Step 1: Establish Executive Sponsorship

**Why It Matters**: Governance without executive buy-in fails. You need authority, budget, and cross-functional cooperation.

**Actions**:

```
EXECUTIVE SPONSORSHIP CHECKLIST

â–¡ Identify executive sponsor (CTO, CIO, Chief Risk Officer, or CEO)

â–¡ Prepare business case for AI governance:
  - Regulatory risk (EU AI Act, national laws, sector regulations)
  - Reputational risk (bias scandals, failures)
  - Operational risk (AI failures disrupting business)
  - Opportunity risk (competitors with better governance gain advantage)
  - Financial quantification (estimate risk exposure)

â–¡ Secure commitment:
  - Budget allocation (FTE, tools, training, external support)
  - Authority for governance team
  - Access to business units and AI systems
  - Board-level reporting (for high-risk AI)

â–¡ Define success metrics:
  - % of AI systems inventoried and classified
  - # of high-risk systems with required oversight
  - Incident rate and resolution time
  - Compliance audit pass rate
  - Time-to-deployment (ensure governance doesn't block innovation)

OUTCOME: Executive sponsor memo committing to AI governance initiative
```

---

### Step 2: Assemble Governance Team

**Why It Matters**: AI governance requires diverse expertise. No single discipline (legal, technical, ethics) can govern AI alone.

**Core Team Composition**:

```
AI GOVERNANCE TEAM STRUCTURE

GOVERNANCE LEAD (1 FTE)
- Coordinates overall governance program
- Reports to executive sponsor
- Facilitates cross-functional collaboration
- Background: Program management, risk, compliance, or technology

LEGAL / COMPLIANCE (0.5-1 FTE)
- Interprets regulations (EU AI Act, GDPR, sector laws)
- Reviews contracts with AI vendors
- Manages liability and regulatory compliance
- Background: Technology law, privacy law, regulatory compliance

TECHNICAL / DATA SCIENCE (0.5-1 FTE)
- Understands AI/ML capabilities and limitations
- Reviews technical documentation (model cards, datasheets)
- Assesses technical risks (bias, robustness, security)
- Background: Machine learning, data science, AI engineering

RISK MANAGEMENT (0.5 FTE)
- Conducts risk assessments
- Integrates AI governance into enterprise risk framework
- Monitors risk metrics
- Background: Enterprise risk, operational risk, cybersecurity

ETHICS / POLICY (0.5 FTE)
- Develops ethical principles and policies
- Assesses societal impacts
- Engages stakeholders (affected communities, civil society)
- Background: Ethics, philosophy, social sciences, policy

BUSINESS LIAISON (0.5 FTE per major business unit)
- Understands business context and use cases
- Ensures governance supports (doesn't block) innovation
- Identifies AI systems in their unit
- Background: Business operations, product management

TOTAL: 3-5 FTE for initial buildout, 2-3 FTE ongoing

EXTENDED TEAM (As Needed):
- Privacy / Data Protection Officer
- Information Security
- Internal Audit
- Human Resources (for hiring AI governance)
- Procurement (for AI vendor management)
- Domain experts (healthcare, finance, etc. as relevant)
```

**Advisory Board (Optional but Recommended)**:
- External AI ethics experts
- Academics (fairness, transparency, AI safety)
- Civil society representatives
- Regulators (informal advisors)
- Industry peers

---

### Step 3: Define Governance Scope and Principles

**What's In Scope?**

```
SCOPE DEFINITION

AI SYSTEMS COVERED:
â–¡ All AI systems (purchased and developed internally)
â–¡ Only high-risk AI systems
â–¡ Only externally facing AI systems
â–¡ Custom definition: _______________

LIFECYCLE STAGES COVERED:
â–¡ Research & Development
â–¡ Procurement / Vendor selection
â–¡ Development / Fine-tuning
â–¡ Testing / Validation
â–¡ Deployment
â–¡ Monitoring / Operations
â–¡ Decommissioning

ORGANIZATIONAL SCOPE:
â–¡ Entire organization
â–¡ Specific business units: _______________
â–¡ Specific geographies: _______________

OUT OF SCOPE (Explicitly):
- _______________________________
- _______________________________
```

**Governance Principles**:

Adapt OECD AI Principles to your organizational context:

```
[ORGANIZATION NAME] AI GOVERNANCE PRINCIPLES

1. HUMAN-CENTERED & FAIR
   AI systems must respect human rights, diversity, and fairness.
   We will:
   - Test for bias across demographic groups
   - Provide transparency about AI use
   - Enable human oversight for high-stakes decisions
   - Ensure accessibility for people with disabilities

2. TRANSPARENT & EXPLAINABLE
   Stakeholders should understand how AI systems work and how decisions are made.
   We will:
   - Document AI systems (model cards, datasheets)
   - Disclose AI use to affected individuals
   - Provide explanations for consequential decisions
   - Publish transparency reports

3. ROBUST, SECURE & SAFE
   AI systems must perform reliably and safely throughout their lifecycle.
   We will:
   - Test for robustness and failure modes
   - Implement security controls
   - Monitor performance continuously
   - Have incident response plans

4. ACCOUNTABLE
   Clear responsibility for AI system outcomes.
   We will:
   - Assign ownership for each AI system
   - Maintain audit trails
   - Establish oversight mechanisms
   - Enable redress for those harmed

5. BENEFICIAL & RESPONSIBLE
   AI should create value while minimizing harm.
   We will:
   - Assess benefits and risks before deployment
   - Consider societal impacts, not just business value
   - Decommission AI that causes more harm than benefit
   - Support inclusive growth and sustainability

[Add organization-specific principles as needed]

APPROVED BY: _______________  DATE: _______________
```

---

## Phase 2: Policies & Procedures (Weeks 5-10)

### Step 4: Create AI Policy Framework

**Three-Tier Policy Framework**:

1. **Principles** (High-level, enduring) â† Already defined in Step 3
2. **Policies** (What must/must not be done)
3. **Procedures** (How to do it)

**Core Policies to Create**:

```
AI POLICY INVENTORY

1. AI DEFINITION & INVENTORY POLICY
   - What qualifies as "AI" requiring governance
   - AI system registration requirements
   - Inventory maintenance procedures
   - Responsibility: Governance Lead

2. AI RISK ASSESSMENT & CLASSIFICATION POLICY
   - Risk assessment methodology (use OECD framework)
   - High-risk determination criteria
   - Risk mitigation requirements by level
   - Responsibility: Risk Management + Governance Lead

3. AI PROCUREMENT POLICY
   - Vendor AI disclosure requirements
   - Due diligence for AI purchases
   - Contractual requirements (liability, audits, termination)
   - Prohibited AI systems
   - Responsibility: Procurement + Legal + Governance Lead

4. AI DEVELOPMENT POLICY
   - Requirements for internal AI development
   - Approval process for new AI projects
   - Documentation requirements (model cards, datasheets)
   - Testing and validation requirements
   - Responsibility: Technical Lead + Governance Lead

5. DATA GOVERNANCE FOR AI POLICY
   - Data quality standards for AI training
   - Bias assessment in training data
   - Data privacy and consent requirements
   - Data retention and deletion
   - Responsibility: Data Protection Officer + Governance Lead

6. BIAS & FAIRNESS POLICY
   - Fairness testing requirements
   - Disaggregated performance metrics
   - Mitigation strategies for detected bias
   - Ongoing monitoring requirements
   - Responsibility: Ethics Lead + Technical Lead

7. TRANSPARENCY & EXPLAINABILITY POLICY
   - When to disclose AI use to users/customers
   - Explanation requirements for decisions
   - Model documentation requirements
   - Public transparency reporting
   - Responsibility: Legal + Ethics Lead

8. HUMAN OVERSIGHT POLICY
   - When human-in-loop required (high-stakes decisions)
   - Human review procedures
   - Override mechanisms
   - Training for human reviewers
   - Responsibility: Business Liaisons + Governance Lead

9. AI INCIDENT MANAGEMENT POLICY
   - Incident definition and classification
   - Reporting requirements
   - Investigation procedures
   - Remediation and communication
   - Responsibility: Risk Management + Governance Lead

10. AI DECOMMISSIONING POLICY
    - Criteria for retiring AI systems
    - Decommissioning procedures
    - Data handling during decommissioning
    - Communication to stakeholders
    - Responsibility: Governance Lead

PRIORITIZATION:
- Phase 1 (Essential): Policies 1, 2, 6, 9 (Definition, Risk, Bias, Incidents)
- Phase 2 (Important): Policies 3, 4, 7, 8 (Procurement, Development, Transparency, Oversight)
- Phase 3 (Complete): Policies 5, 10 (Data, Decommissioning)
```

**Policy Development Process**:

For each policy:

1. **Draft** (1-2 weeks): Governance team drafts based on template, regulations, industry best practices
2. **Stakeholder Review** (1-2 weeks): Circulate to legal, business units, IT, affected teams for feedback
3. **Revise** (1 week): Incorporate feedback
4. **Approve** (1 week): Executive sponsor or governance committee approval
5. **Communicate** (1 week): Publish, train affected teams, Q&A sessions
6. **Implement** (Ongoing): Enforce, monitor compliance

---

### Step 5: Design AI Inventory & Classification Process

**Purpose**: You can't govern AI you don't know about.

**AI Inventory Template**:

```
AI SYSTEM INVENTORY RECORD

BASIC INFORMATION
- System Name: _______________
- Description: _______________
- Owner (Business Unit): _______________
- Technical Owner: _______________
- Status: â–¡ Development  â–¡ Testing  â–¡ Production  â–¡ Decommissioned
- Deployment Date: _______________
- Last Updated: _______________

CLASSIFICATION (Use OECD Framework)
- People & Planet Impact: â–¡ Minimal  â–¡ Limited  â–¡ Significant  â–¡ Critical
- Economic Context: _______________
- Data & Input Summary: _______________
- AI Model Type: _______________
- Task & Output: _______________
- Overall Risk Level: â–¡ Minimal  â–¡ Limited  â–¡ High  â–¡ Unacceptable

REGULATORY MAPPING
- EU AI Act Category: â–¡ Prohibited  â–¡ High-Risk  â–¡ Limited  â–¡ Minimal
- Other Applicable Regulations: _______________

GOVERNANCE REQUIREMENTS (Based on Risk)
- Pre-Deployment: _______________
- Deployment: _______________
- Post-Deployment: _______________

DOCUMENTATION LINKS
- Model Card: [Link]
- Datasheet: [Link]
- Risk Assessment: [Link]
- Testing Results: [Link]
- Audit Reports: [Link]

COMPLIANCE STATUS
- Bias Audit: â–¡ Complete  â–¡ In Progress  â–¡ Overdue  â–¡ N/A
- Performance Monitoring: â–¡ Active  â–¡ Inactive
- Last Audit Date: _______________
- Issues Identified: _______________
```

**Discovery Process (How to Find AI Systems)**:

```
AI SYSTEM DISCOVERY PLAN

METHOD 1: TOP-DOWN (IT/Procurement Records)
â–¡ Review IT asset inventory for AI/ML tools
â–¡ Review cloud service subscriptions (AWS SageMaker, Azure ML, etc.)
â–¡ Review software purchases (AI vendor licenses)
â–¡ Review API usage (OpenAI, Anthropic, etc.)

METHOD 2: BOTTOM-UP (Business Unit Survey)
â–¡ Survey each business unit: "What AI systems do you use or plan to use?"
â–¡ Survey product managers: "What AI features are in your products?"
â–¡ Survey data science teams: "What models have you deployed?"

METHOD 3: USE CASE MAPPING
â–¡ Identify high-risk use cases (hiring, lending, healthcare, etc.)
â–¡ Investigate if AI is used in those contexts
â–¡ Proactively classify and govern

METHOD 4: VENDOR QUESTIONNAIRES
â–¡ Send questionnaire to all major vendors: "Do your products use AI?"
â–¡ For yes: Request details (what AI, how used, risk level)

CONSOLIDATION:
â–¡ Merge all discovered systems into central inventory
â–¡ Remove duplicates
â–¡ Classify each system using OECD framework
â–¡ Prioritize high-risk systems for governance
```

**Ongoing Inventory Maintenance**:
- Require registration of new AI systems before deployment
- Quarterly reviews to catch shadow AI
- Integrate with IT change management processes

---

## Phase 3: Implementation (Weeks 11-20)

### Step 6: Establish Oversight Mechanisms

**Governance Committee Structure**:

```
AI GOVERNANCE COMMITTEE

COMPOSITION:
- Chair: Executive Sponsor
- Members:
  - Governance Lead
  - Legal/Compliance Lead
  - Chief Technology Officer or delegate
  - Chief Risk Officer or delegate
  - Business Unit Leaders (rotation)
  - External Ethics Advisor (optional)

FREQUENCY: Quarterly (more frequent for high-risk AI organizations)

RESPONSIBILITIES:
â–¡ Approve high-risk AI deployments
â–¡ Review incident reports
â–¡ Review governance metrics and trends
â–¡ Approve policy changes
â–¡ Escalate issues to Board (if needed)
â–¡ Resource allocation for governance program

DECISION AUTHORITY:
- Can block deployment of high-risk AI
- Can require decommissioning of non-compliant AI
- Can allocate budget for remediation
- Reports to Board on AI governance status
```

**AI Review Process for New Deployments**:

```
AI DEPLOYMENT APPROVAL PROCESS

STEP 1: REGISTRATION & CLASSIFICATION
- Team proposes new AI system
- Completes inventory record
- Governance team classifies (OECD framework)

STEP 2: RISK ASSESSMENT
- If Minimal/Limited Risk â†’ Streamlined review (approval in 1-2 weeks)
- If High Risk â†’ Full review (approval in 4-8 weeks)
- If Unacceptable Risk â†’ Rejected

STEP 3: DOCUMENTATION REVIEW (High-Risk Only)
Governance team reviews:
â–¡ Model card / technical documentation
â–¡ Datasheet (training data)
â–¡ Bias audit results (disaggregated performance metrics)
â–¡ Fairness assessment
â–¡ Security assessment
â–¡ Privacy impact assessment (if personal data)
â–¡ Human oversight plan
â–¡ Monitoring plan

STEP 4: GOVERNANCE COMMITTEE APPROVAL (High-Risk Only)
- Present to committee
- Committee can:
  â–¡ Approve
  â–¡ Approve with conditions
  â–¡ Request additional information
  â–¡ Deny

STEP 5: DEPLOYMENT WITH MONITORING
- System deployed with required oversight
- Monitoring begins immediately
- First review: 30 days post-deployment
- Ongoing reviews: Quarterly (high-risk), Annually (limited risk)
```

---

### Step 7: Implement Monitoring & Metrics

**What to Monitor**:

```
AI GOVERNANCE MONITORING DASHBOARD

INVENTORY METRICS
- Total AI systems: ___
- By risk level:
  - High-risk: ___
  - Limited risk: ___
  - Minimal risk: ___
- By status:
  - Production: ___
  - Development: ___
  - Decommissioned: ___

COMPLIANCE METRICS
- % of AI systems with required documentation: ___%
- % of high-risk AI with bias audits: ___%
- % of high-risk AI with human oversight: ___%
- Overdue audits: ___
- Policy violations: ___

PERFORMANCE METRICS (Per AI System)
- Accuracy (overall): ___%
- Accuracy by demographic group: ___
- Fairness metric: ___
- Drift detected: â–¡ Yes  â–¡ No
- User complaints: ___
- False positive rate: ___%
- False negative rate: ___%

INCIDENT METRICS
- Incidents reported: ___
- By severity:
  - Critical: ___
  - High: ___
  - Medium: ___
  - Low: ___
- Average time to resolution: ___ days
- Repeat incidents (same root cause): ___

BUSINESS METRICS
- Time to deploy AI (avg): ___ days
- AI projects blocked by governance: ___
- AI projects improved by governance: ___
- Value created by AI: $___
- Risk avoided by governance: $___

RED FLAGS (Trigger Immediate Review):
ðŸš© Bias audit shows >10% performance gap by demographic
ðŸš© Drift detected (accuracy dropped >5%)
ðŸš© Critical incident reported
ðŸš© Regulatory inquiry
ðŸš© High-risk AI deployed without approval
ðŸš© User complaints spiking (>20% increase)
```

**Automated Monitoring Tools**:
- Integrate with ML observability platforms (Arize, Fiddler, Weights & Biases)
- Set up alerts for drift, performance degradation, bias
- Dashboard for real-time visibility

---

### Step 8: Create Incident Response Plan

**AI Incident Classification**:

```
AI INCIDENT SEVERITY LEVELS

CRITICAL (P1)
- Severe harm to individuals (safety, health, rights violations)
- Massive bias causing widespread discrimination
- Security breach exposing sensitive data
- Regulatory violation with enforcement risk
- Response time: Immediate (within hours)
- Escalation: Executive sponsor + Legal + PR

HIGH (P2)
- Moderate harm to individuals
- Significant bias detected in production
- Performance degradation affecting many users
- Privacy violation
- Response time: 24 hours
- Escalation: Governance committee

MEDIUM (P3)
- Minor harm or inconvenience
- Bias detected but mitigated
- Performance degradation affecting few users
- Response time: 1 week
- Escalation: Governance team

LOW (P4)
- No user harm
- Documentation incomplete
- Minor policy violation
- Response time: 1 month
- Escalation: System owner
```

**Incident Response Process**:

```
AI INCIDENT RESPONSE PLAYBOOK

1. DETECTION & REPORTING
   How incident detected:
   - Automated monitoring alerts
   - User complaints
   - Internal discovery
   - External researcher
   - Regulatory inquiry
   - Media report

   Who can report: Anyone (employees, users, researchers, public)
   How to report: incident@[organization].com or dedicated portal

2. TRIAGE (Within 4 hours)
   Governance lead:
   - Classifies severity (P1-P4)
   - Assembles response team
   - Notifies stakeholders

3. INVESTIGATION (Timeline based on severity)
   Response team:
   - Reproduces issue
   - Identifies root cause
   - Assesses scope (how many affected?)
   - Determines if still ongoing

4. CONTAINMENT (Immediate for P1/P2)
   Options:
   - Pause AI system
   - Implement guardrails
   - Increase human oversight
   - Rollback to previous version
   - Decommission if necessary

5. REMEDIATION (Timeline based on severity)
   - Fix root cause (retrain, adjust thresholds, etc.)
   - Validate fix
   - Test extensively before redeployment

6. COMMUNICATION
   Internal:
   - Incident report to governance committee
   - Post-mortem with team
   - Lessons learned documentation

   External (if required):
   - Affected users notification
   - Regulatory reporting (if required)
   - Public disclosure (if material)
   - Media response (if needed)

7. LEARNING & PREVENTION
   - Update policies/procedures to prevent recurrence
   - Train teams on lessons learned
   - Update monitoring to detect similar issues
   - Add to incident knowledge base

POST-INCIDENT REVIEW (Within 30 days):
â–¡ Root cause identified and fixed
â–¡ Affected users remediated
â–¡ Policy/procedure updates implemented
â–¡ Monitoring enhanced
â–¡ Team trained
â–¡ Documentation complete
```

---

## Phase 4: Continuous Improvement (Ongoing)

### Step 9: Establish Feedback Loops

**Sources of Feedback**:

1. **User Feedback**
   - Complaint channels
   - Surveys on AI experiences
   - Challenge/appeal processes for AI decisions

2. **Internal Feedback**
   - Product teams reporting governance friction
   - Data scientists reporting policy gaps
   - Business units reporting shadow AI

3. **External Feedback**
   - Regulators (audits, inquiries)
   - Researchers (bias studies, security analyses)
   - Civil society (advocacy groups, watchdogs)
   - Media (investigative journalism)

4. **Automated Feedback**
   - Monitoring alerts
   - Performance metrics
   - Incident trends

**Feedback Integration Process**:

```
CONTINUOUS IMPROVEMENT CYCLE

MONTHLY:
â–¡ Review monitoring metrics
â–¡ Review incident reports
â–¡ Identify patterns and trends

QUARTERLY:
â–¡ Governance committee reviews feedback
â–¡ Prioritizes policy updates
â–¡ Allocates resources for improvements

ANNUALLY:
â–¡ Comprehensive governance review
â–¡ Benchmark against industry/peers
â–¡ Update principles and strategy
â–¡ Board-level reporting

AD HOC:
â–¡ Regulatory changes â†’ Policy updates
â–¡ Major incidents â†’ Immediate improvements
â–¡ New AI use cases â†’ Scope expansion
```

---

### Step 10: Measure Governance Maturity

**AI Governance Maturity Model**:

```
LEVEL 1: REACTIVE (Initial State)
- No AI inventory
- No formal policies
- Governance happens only after incidents
- No one responsible for AI governance

LEVEL 2: POLICY-DRIVEN
- AI policies exist
- Inventory started but incomplete
- Governance team established
- Compliance checked at deployment

LEVEL 3: RISK-BASED
- Complete AI inventory
- Risk-based governance (proportional to risk level)
- Monitoring and metrics in place
- Proactive risk assessment

LEVEL 4: INTEGRATED
- AI governance integrated into enterprise risk
- Automated monitoring and alerts
- Culture of responsible AI
- Continuous improvement processes

LEVEL 5: OPTIMIZED
- Industry-leading governance
- Predictive risk management
- External transparency and accountability
- Governance enables (not blocks) innovation

CURRENT LEVEL: _____
TARGET LEVEL (12 months): _____
TARGET LEVEL (24 months): _____
```

---

## Governance Roadmap Template

```
AI GOVERNANCE IMPLEMENTATION ROADMAP

MONTHS 1-3: FOUNDATION
â–¡ Secure executive sponsorship
â–¡ Assemble governance team
â–¡ Define scope and principles
â–¡ Conduct initial AI inventory (quick scan)
MILESTONE: Governance charter approved

MONTHS 4-6: POLICIES
â–¡ Draft core policies (Definition, Risk, Bias, Incidents)
â–¡ Stakeholder review and approval
â–¡ Communicate policies
â–¡ Begin policy implementation
MILESTONE: Core policies published

MONTHS 7-9: INFRASTRUCTURE
â–¡ Complete detailed AI inventory
â–¡ Classify all systems using OECD framework
â–¡ Establish governance committee
â–¡ Implement monitoring tools
â–¡ Launch incident reporting system
MILESTONE: Governance infrastructure operational

MONTHS 10-12: SCALING
â–¡ Expand policy coverage (Procurement, Development, Transparency)
â–¡ Train business units on governance
â–¡ Conduct first bias audits for high-risk systems
â–¡ Begin quarterly governance reporting
MILESTONE: Governance at scale

MONTHS 13-18: OPTIMIZATION
â–¡ Automate compliance checks
â–¡ Enhance monitoring and alerting
â–¡ External transparency reporting
â–¡ Benchmarking against peers
MILESTONE: Mature governance program

MONTHS 19-24: CONTINUOUS IMPROVEMENT
â–¡ Regular policy updates
â–¡ Advanced risk modeling
â–¡ Industry leadership (speaking, publishing)
â–¡ Board-level engagement
MILESTONE: Governance as competitive advantage
```

---

## Resources & Tools

### Templates Provided in This Framework
- Executive Sponsorship Memo
- Team Structure & Roles
- Governance Principles
- Policy Templates (10 core policies)
- AI Inventory Record
- OECD Classification Assessment
- Governance Committee Charter
- Monitoring Dashboard
- Incident Response Playbook
- Maturity Assessment
- Implementation Roadmap

### External Resources
- **OECD AI Principles**: [oecd.ai/en/ai-principles](https://oecd.ai/en/ai-principles)
- **OECD Classification Framework**: [oecd.ai/en/classification](https://oecd.ai/en/classification)
- **ISO/IEC 22989** (AI Concepts): [iso.org/standard/74296.html](https://www.iso.org/standard/74296.html)
- **ISO/IEC 42001** (AI Management System): [iso.org/standard/81230.html](https://www.iso.org/standard/81230.html)
- **EU AI Act**: [digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)
- **NIST AI Risk Management Framework**: [nist.gov/itl/ai-risk-management-framework](https://www.nist.gov/itl/ai-risk-management-framework)

### Tools
- **ML Observability**: Arize, Fiddler, Weights & Biases
- **Bias Testing**: Fairlearn, AI Fairness 360, Aequitas
- **Explainability**: LIME, SHAP, InterpretML
- **Documentation**: Model cards, Datasheets

---

## Common Pitfalls to Avoid

### Pitfall 1: "Governance as Gatekeeping"
**Problem**: Governance seen as blocker to innovation
**Solution**: Design governance that enables responsible innovation, not prevents all innovation
- Streamlined approval for low-risk AI
- Fast-track for well-documented, tested AI
- Governance team as consultants, not just approvers

### Pitfall 2: "Compliance Theater"
**Problem**: Policies exist on paper, not enforced in practice
**Solution**:
- Monitor compliance metrics
- Consequences for policy violations
- Regular audits
- Culture change, not just policy writing

### Pitfall 3: "Perfect is Enemy of Good"
**Problem**: Waiting for perfect governance before starting
**Solution**: Start with core policies, iterate based on feedback
- Launch with Policies 1, 2, 6, 9
- Expand over 12 months
- Continuous improvement

### Pitfall 4: "Governance Without Context"
**Problem**: Copying another org's governance without customization
**Solution**: Adapt to your industry, risk tolerance, regulatory context, culture

### Pitfall 5: "Siloed Governance"
**Problem**: AI governance disconnected from broader risk/compliance
**Solution**: Integrate with enterprise risk management, cybersecurity, privacy, compliance

### Pitfall 6: "No Executive Buy-In"
**Problem**: Governance team lacks authority or resources
**Solution**: Secure executive sponsorship first (Step 1), report to Board regularly

### Pitfall 7: "Shadow AI"
**Problem**: Business units use AI without governance's knowledge
**Solution**:
- Easy registration process (remove friction)
- Partner with business (not police them)
- Ongoing discovery (surveys, procurement reviews)

---

**Used By Articles**:
- AI Governance Frameworks: Building Your Organization's Approach
- [Future articles on AI policy, AI program management, organizational AI governance]

**Reusable For**:
- AI governance articles
- Organizational AI strategy articles
- AI program management articles
- Change management for AI articles
