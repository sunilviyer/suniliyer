{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 article_id: "term-06"\
title: "AI Compute Requirements"\
slug: "ai-compute"\
path: "terminology"\
header_image: "/images/terminology/ai-compute.png"\
navigation:\
  prev:\
    slug: "/articles/multimodal-ai"\
    title: "Multimodal AI"\
  next:\
    slug: "/articles/environmental-cost-ai"\
    title: "Environmental Cost of AI"\
key_learnings:\
  - "AI does not run on magic; it runs on specialized silicon chips capable of trillions of mathematical operations per second."\
  - "GPUs dominate AI because they use parallel processing to handle thousands of simple tasks at once, whereas CPUs are designed for complex serial logic."\
  - "The cost of 'training' a frontier model is a massive upfront investment (often $100M+), while 'inference' is the ongoing cost of using the model."\
  - "A extreme concentration of power exists in the hardware layer, with NVIDIA controlling roughly 80\'9690% of the market."\
  - "Compute governance is an emerging field focusing on cost monitoring, supply chain risks, and access controls for high-performance hardware."\
read_time: "8 min read"\
updated_date: "January 2025"\
tags:\
  - AI Infrastructure\
  - GPUs\
  - Cloud Compute\
  - AI Governance\
seo:\
  description: "Understand the hardware and compute requirements that power modern AI, including the role of GPUs, the difference between training and inference, and the strategic importance of the AI supply chain."\
  keywords:\
    - AI compute\
    - NVIDIA GPU\
    - AI training vs inference\
    - H100 cost\
    - compute governance\
content: |\
  <h2>The Engine Room of Intelligence</h2>\
  <p>Think about the last time you saw a demonstration of a powerful AI like GPT-4. It feels like magic, but here is the key insight: AI does not run on magic; it runs on specialized silicon. To train a model of that scale, it took roughly 25,000 NVIDIA chips running for months.</p>\
  <p>To lead an organization through an AI transformation, you need to understand the hardware layer. It determines what is possible, how fast you can innovate, and most importantly, what it will cost. Think of it this way: if data is the fuel for AI, compute is the engine.</p>\
\
  <h2>Why Your Laptop Brain Isn&apos;t Enough</h2>\
  <p>You might wonder why we need specialized chips at all. Your computer has a CPU (Central Processing Unit), which is a brilliant generalist. It is designed to handle complex logic and branching tasks one after another. However, AI training requires doing the exact same simple math billions of times simultaneously.</p>\
  <p>This is why <card type="concept" id="term-gpu">GPUs (Graphics Processing Units)</card> rule the AI world. Originally built to render video game graphics, they use &quot;parallel processing&quot; to perform thousands of simple calculations at once. This architecture is 10 to 100 times faster for AI than a traditional CPU.</p>\
\
  <h2>The Massive Cost of Learning</h2>\
  <p>In the AI world, we distinguish between two phases: Training and Inference. Training is the one-time process of teaching the model from data&mdash;it is like creating a recipe. Inference is every time someone uses that model to make a prediction&mdash;like cooking from that recipe.</p>\
  <p>Here is the key insight: training is where the massive upfront checks are written. To understand the scale, consider the <card type="insight" id="insight-h100-gpu-cost">cost of a single high-end H100 GPU</card>, which can range from $25,000 to $40,000. For a frontier model, that adds up to over $100 million in compute alone. However, for most of you, the ongoing cost of inference&mdash;every query your employees or customers send&mdash;will eventually exceed the training cost.</p>\
\
  <h2>Geopolitics and Concentration Risk</h2>\
  <p>The supply chain for these chips is incredibly narrow. One company, NVIDIA, controls roughly 80&mdash;90% of the market. Furthermore, the most advanced chips are fabricated almost exclusively in Taiwan. This creates a &quot;concentration risk&quot; for your strategy; a shortage in one region or a change in one company&apos;s export policy can affect the entire AI industry overnight.</p>\
\
  <h2>Governance: Managing the Silicon</h2>\
  <p>Because compute is a physical, measurable resource, it is becoming a major lever for governance. Regulators are looking at &quot;compute thresholds&quot; to determine which models need extra oversight. For your organization, this means you must manage your silicon as carefully as your data.</p>\
  <p>I recommend using an <card type="resource" id="res-ai-compute-governance-checklist">Organizational AI Compute Governance Checklist</card> to track your GPU usage, establish budgets for &quot;exploding&quot; cloud costs, and assess your dependencies on single hardware vendors. By understanding these requirements today, you can build a more resilient and cost-effective AI strategy for tomorrow.</p>}