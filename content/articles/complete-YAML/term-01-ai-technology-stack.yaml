{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 article_id: "term-01"\
title: "The AI Technology Stack"\
slug: "ai-technology-stack"\
path: "terminology"\
header_image: "/images/terminology/ai-technology-stack.png"\
navigation:\
  prev:\
    slug: null\
    title: null\
  next:\
    slug: "/terminology/ai-vs-automation"\
    title: "AI vs Automation"\
key_learnings:\
  - "The AI technology stack consists of five layers: Hardware, Infrastructure, Platforms, Models, and Applications."\
  - "NVIDIA dominates the hardware layer with roughly 80-90% market share, creating significant supply chain concentration risk."\
  - "Foundation models serve as the 'intelligence' layer, where a few major providers influence the entire downstream ecosystem."\
  - "Governance must be applied at every layer, addressing specific risks like data residency, vendor lock-in, and compute costs."\
  - "Understanding the difference between training (one-time cost) and inference (ongoing cost) is essential for AI budgeting."\
read_time: "9 min read"\
updated_date: "January 2025"\
tags:\
  - AI Infrastructure\
  - Cloud Compute\
  - Foundation Models\
  - Technical Foundations\
seo:\
  description: "Explore the five layers of the AI technology stack, from specialized hardware and cloud infrastructure to the models and applications used by your team."\
  keywords:\
    - AI technology stack\
    - GPU vs TPU\
    - AI infrastructure\
    - foundation model layer\
    - AI governance framework\
content: |\
  <h2>Peeking Under the Hood</h2>\
  <p>When you use an AI tool like a chatbot, it can feel a bit like magic. But here is the key insight: that simple interface sits on top of a massive "Jenga tower" of hardware and software dependencies. If any of those lower layers wobble\'97due to a chip shortage, a cloud outage, or a change in a vendor's policy\'97your entire AI strategy can feel the impact.</p>\
  <p>To lead an organization through the AI transformation, you do not need to be a hardware engineer, but you do need to understand how these layers fit together. Think of it this way: understanding the AI stack is like checking the foundation of a building before you decide to add three new floors. Let's walk through the five layers that make modern AI possible.</p>\
\
  <h2>Layer 1: The Hardware Foundation</h2>\
  <p>At the very bottom are the physical chips that do the heavy lifting. Unlike your laptop's brain (the CPU), which is good at doing one complex thing at a time, AI needs chips that can do thousands of tiny, simple math problems simultaneously. This is why GPUs (Graphics Processing Units) are the gold standard for AI today.</p>\
  <p>Currently, NVIDIA dominates this layer, controlling nearly 80-90% of the market. This creates a "concentration risk"\'97nearly every AI system your company uses probably depends on this one supplier. To understand the scale of investment here, consider the <card type="insight" id="insight-h100-gpu-cost">cost of a single high-end H100 GPU</card>, which can range from $25,000 to $40,000.</p>\
\
  <h2>Layer 2 & 3: Infrastructure and Platforms</h2>\
  <p>The next two layers turn that raw hardware into something your team can actually use. Infrastructure is the cloud environment provided by giants like AWS, Azure, or Google Cloud. This is where your data is physically processed, which raises important governance questions about data residency and whether your information stays within specific borders.</p>\
  <p>The Platform layer provides the tools and APIs that let developers build AI apps without managing the hardware themselves. This is where you might face "vendor lock-in." Here is what matters: if you build your entire workflow on one specific provider's platform, moving to a competitor later can be expensive and time-consuming.</p>\
\
  <h2>Layer 4 & 5: Models and Applications</h2>\
  <p>Layer 4 is the "intelligence" of the system\'97the models. Most of the world now uses "foundation models," which are massive systems trained on broad data that can be adapted for many tasks. While some organizations use <card type="resource" id="res-open-source-compliance-checklist">open-weight models</card> that they host themselves, many rely on closed APIs from companies like OpenAI or Anthropic.</p>\
  <p>Finally, at the top is the Application layer. This is the chatbot, the resume screener, or the analytics dashboard your employees see. Many of these applications are actually "thin wrappers"\'97simple interfaces sitting on top of someone else's model and hardware. If the model provider below them changes their rules, your application could change overnight.</p>\
\
  <h2>Managing the Costs and Risks</h2>\
  <p>As you evaluate your AI portfolio, it is helpful to distinguish between training (the one-time cost to teach the model) and inference (the ongoing cost every time someone uses the model). While training GPT-4 cost over $100 million, the inference costs for running a popular service can be 15 times higher over a single year.</p>\
  <p>Because the AI stack is so interconnected, we recommend using a structured <card type="resource" id="res-ai-stack-assessment-framework">stack assessment framework</card> to map out where your data flows and who your critical vendors are. By understanding these dependencies, you can move from just "using AI" to governing it with the clarity your organization needs.</p>}