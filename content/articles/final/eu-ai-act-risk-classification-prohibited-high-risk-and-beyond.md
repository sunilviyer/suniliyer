---
title: EU AI Act Risk Classification - Prohibited, High-Risk, and Beyond
slug: eu-ai-act-risk-classification-prohibited-high-risk-and-beyond
path: risk
publishDate: 2025-08-18
tldr: EU AI Act risk-based pyramid classifies AI into four levels with proportionate regulation - Level 1 Prohibited (banned outright, ‚Ç¨35M or 7% turnover penalties) includes social scoring systems, subliminal manipulation, exploiting vulnerabilities of children/disabled/disadvantaged, real-time public biometric identification for law enforcement (limited exceptions for terrorism/kidnapping/serious crime with judicial authorization), workplace/school emotion recognition, biometric categorization by sensitive characteristics, untargeted facial database scraping effective February 2 2025. Level 2 High-Risk (heavy regulation) defined through two pathways - Annex I safety components (medical devices, vehicles, toys, aviation, machinery, marine equipment) and Annex III sensitive uses (biometrics, critical infrastructure, education affecting outcomes, employment decisions, essential services access, law enforcement, migration/borders, justice, democratic processes) requiring risk management, data quality/governance, technical documentation, logging, transparency/information provision, human oversight, accuracy/robustness/cybersecurity, conformity assessment before market placement with narrow exception for procedural tasks/improving human-completed work/detecting patterns without replacing judgment/preparatory assessments. Level 3 Limited-Risk (transparency only) covers AI interacting with humans (chatbots must disclose), emotion recognition/biometric categorization outside prohibited contexts (must inform), deepfakes/synthetic content (must label, exceptions for art/satire), AI-generated public interest text (must disclose) with ‚Ç¨15M or 3% turnover penalties. Level 4 Minimal-Risk (no AI-specific rules) encompasses vast majority including spam filters, games, inventory management, recommendations not affecting rights, personalization, analytics subject only to general consumer/data protection/product safety/competition law. Classification walkthrough: Step 1 check prohibited list, Step 2 check Annex I safety components, Step 3 check Annex III sensitive uses with exception assessment, Step 4 check transparency obligations, Step 5 default to minimal-risk with common mistakes including assuming internal use exempts (deployer obligations still apply), thinking data aggregation exempts (significant decision influence triggers), ignoring downstream uses for foundation models, over-classifying everything as high-risk wasting resources.
relatedConcepts:
  - eu-ai-act-risk-classification
  - risk-based-pyramid
  - prohibited-ai-practices
  - high-risk-ai-systems
  - limited-risk-ai
  - minimal-risk-ai
  - unacceptable-risk-level
  - social-scoring-ban
  - subliminal-manipulation-prohibition
  - vulnerability-exploitation-ban
  - real-time-biometric-id-restrictions
  - workplace-emotion-recognition-ban
  - sensitive-biometric-categorization-ban
  - facial-scraping-prohibition
  - annex-i-safety-components
  - annex-iii-sensitive-uses
  - biometric-identification-high-risk
  - critical-infrastructure-ai
  - education-ai-high-risk
  - employment-ai-high-risk
  - essential-services-ai
  - law-enforcement-ai
  - migration-border-ai
  - justice-ai
  - democratic-process-ai
  - high-risk-exception-criteria
  - narrow-procedural-task
  - human-activity-improvement
  - pattern-detection
  - preparatory-tasks
  - chatbot-disclosure-requirement
  - emotion-recognition-notification
  - deepfake-labeling
  - ai-generated-content-disclosure
  - spam-filter-minimal-risk
  - recommendation-system-classification
  - classification-walkthrough
  - internal-use-deployer-obligations
  - downstream-use-responsibility
  - foundation-model-classification
  - over-classification-waste
examples:
  - ai-regulatory-compliance-examples
  - ai-governance-use-cases
  - algorithmic-bias-case-studies
templates:
  - ai-regulatory-readiness-assessment
  - ai-risk-assessment-template
  - ai-governance-framework-builder
crossPathRefs:
  - path: responsibility
    articles:
      - the-eu-ai-act-europes-landmark-regulation-explained
      - ai-governance-frameworks-building-your-organizations-approach
      - data-protection-impact-assessments-the-ai-dpia-guide
  - path: risk
    articles:
      - algorithmic-bias-how-ai-discriminates-and-why
      - the-black-box-problem-why-ai-explainability-matters
      - building-trustworthy-ai-the-seven-pillars
tags:
  - eu-ai-act
  - risk-classification
  - prohibited-ai
  - high-risk-ai
  - limited-risk
  - minimal-risk
  - compliance
  - annex-i
  - annex-iii
  - governance
category: AI Risks & Principles
image: eu-ai-act-risk-classification-prohibited-high-risk-and-beyond.jpg
imageAlt: EU AI Act risk pyramid showing four levels from bottom to top - Minimal risk (green, no special rules), Limited risk (amber, transparency required), High-risk (yellow, heavy regulation), Prohibited (red, banned), with examples and penalties for each level
author: Sunil Iyer
readingTime: 17
seoTitle: EU AI Act Risk Levels - Prohibited, High, Limited & Minimal Risk AI
seoDescription: EU AI Act risk classification - prohibited practices (social scoring, manipulation, biometric ID, emotion recognition), high-risk systems (Annex I/III, employment, credit, education), limited-risk transparency (chatbots, deepfakes), minimal-risk exemptions, classification walkthrough, common mistakes.
---

## Summary

EU AI Act risk-based pyramid classifies all AI systems into four proportionate regulatory levels ensuring strictest rules apply only to most dangerous applications while enabling innovation in low-risk contexts. Level 1 Unacceptable Risk practices face absolute prohibition with no exceptions and penalties up to ‚Ç¨35 million or 7% global annual turnover: social scoring systems evaluating trustworthiness from behavior like China's social credit affecting loans/travel/education, subliminal manipulation using imperceptible techniques people can't defend against, exploitation of vulnerabilities targeting children/elderly/disabled to manipulate harmful behavior, real-time biometric identification in public spaces for law enforcement creating mass surveillance (narrow exceptions for finding kidnapping victims/preventing imminent terrorism/locating serious crime suspects with judicial authorization), emotion recognition in workplaces and schools from facial expressions/voice creating coercive monitoring environments, biometric categorization by sensitive characteristics (race, politics, sexual orientation, religion) enabling discrimination, untargeted scraping of faces from internet/CCTV for recognition databases violating privacy - prohibitions effective February 2 2025 applying immediately.

Level 2 High-Risk AI systems permitted with extensive requirements before and after market placement defined through two pathways: Annex I covering AI as safety component of products under existing EU product safety legislation (medical devices, vehicles, toys, aviation equipment, machinery, marine equipment) like autonomous vehicle braking systems, Annex III covering AI in eight sensitive use areas regardless of product - biometrics (facial recognition, fingerprint matching where allowed), critical infrastructure (electricity grids, water systems, traffic management), education (exam grading, student admission, cheating detection), employment (resume screening, interview analysis, performance monitoring, task allocation), essential services (credit scoring, loan approval, insurance pricing/underwriting), law enforcement (risk assessment, evidence analysis, crime prediction), migration/border control (visa assessment, document verification), justice (sentencing assistance, case research), democratic processes (influencing voting behavior). Important exception: even if in Annex III category, not high-risk if performing narrow procedural task, improving previously completed human activity, detecting patterns without replacing judgment, or only preparatory assessment tasks - example: AI formatting resumes for readability not high-risk while AI scoring candidates is. High-risk providers must: establish risk management system, meet data quality/governance requirements, create/maintain technical documentation, enable logging/record-keeping, provide transparency/information to deployers, allow human oversight, ensure accuracy/robustness/cybersecurity, conduct conformity assessment before market placement with ongoing performance monitoring, serious incident reporting, documentation updates when changed.

Level 3 Limited-Risk AI faces specific transparency obligations only with penalties up to ‚Ç¨15 million or 3% global turnover applying principle people should know when interacting with AI or viewing AI-generated content: chatbots and systems interacting with humans must disclose "You are chatting with AI assistant" or similar (exception if obvious from context like game character), emotion recognition and biometric categorization systems must inform individuals they're being analyzed (applies outside prohibited workplace/school contexts like retail), deepfakes and AI-generated/manipulated content must be labeled as artificially created (exceptions for artistic/satirical work where artificial nature part of creative intent), AI-generated text on public interest matters must disclose AI authorship. Level 4 Minimal-Risk AI encompasses vast majority of systems with no AI-specific EU AI Act requirements: spam filters, video game AI, inventory management, music recommendations, photo filters, predictive text, general search engines, analytics tools - these pose minimal fundamental rights risk where consequences of errors minor, still subject to existing consumer protection/data protection (GDPR)/product safety/competition law but no special AI obligations.

Classification walkthrough provides systematic assessment: Step 1 check prohibited list for social scoring/subliminal techniques/vulnerability exploitation/real-time biometric ID/workplace-school emotion recognition/sensitive trait categorization/facial scraping - if yes STOP system prohibited, Step 2 check if AI is safety component of Annex I product (medical/vehicle/toy/aviation/machinery/marine) - if yes high-risk, Step 3 check if used in Annex III sensitive area (biometrics/infrastructure/education/employment/services/law enforcement/migration/justice/democracy) and whether narrow exception applies for procedural/improvement/pattern-detection/preparatory tasks - if yes and no exception high-risk, Step 4 check transparency obligations for human interaction/emotion recognition/content generation - if yes limited-risk, Step 5 default to minimal-risk if none above apply. Real-world examples: resume screening high-risk (employment Annex III), customer service chatbot limited-risk (human interaction disclosure), credit scoring high-risk (essential services Annex III), email spam filter minimal-risk (none apply), student exam proctoring high-risk (education Annex III), facial recognition door access high-risk (biometrics Annex III), AI marketing image limited-risk (must label), self-driving car high-risk (safety component Annex I).

Common classification mistakes undermine compliance: Mistake 1 assuming internal use means no regulation (deployer obligations still apply for high-risk AI used internally not sold), Mistake 2 thinking "just aggregating data" exempts (making or significantly influencing decisions about people triggers classification regardless of data processing label), Mistake 3 ignoring downstream uses (foundation model/GPAI providers have obligations even without controlling ultimate use), Mistake 4 over-classifying everything as high-risk "to be safe" (wastes resources creating unnecessary compliance burdens where accurate classification matters for proportionate response). Risk classification forms foundation of all EU AI Act compliance with documentation/conformity assessment/monitoring flowing from classification - system logical targeting AI significantly affecting lives (jobs, education, services access, freedom) while leaving most AI in minimal-risk with no special requirements, challenge requires honest assessment against criteria where wishful thinking doesn't change classification and regulators will eventually verify.

## Key Learning Objectives

After reading this article, you will be able to:

1. **Understand risk pyramid structure** - Four-level framework (prohibited/high/limited/minimal) with proportionate regulation and penalty tiers
2. **Identify prohibited AI practices** - Seven banned categories (social scoring, manipulation, vulnerability exploitation, biometric ID, emotion recognition, categorization, scraping)
3. **Recognize high-risk AI pathways** - Annex I safety components vs Annex III sensitive uses with eight use areas
4. **Apply high-risk exceptions** - Narrow procedural task, human activity improvement, pattern detection, preparatory assessment criteria
5. **Implement limited-risk transparency** - Chatbot disclosure, emotion recognition notification, deepfake labeling, AI-generated content marking
6. **Classify minimal-risk AI** - Vast majority exemption with examples and continued general law applicability
7. **Execute classification walkthrough** - Five-step systematic assessment from prohibited through minimal-risk default
8. **Navigate real-world examples** - Resume screening, chatbots, credit scoring, spam filters, exam proctoring, facial recognition, deepfakes, autonomous vehicles
9. **Avoid classification mistakes** - Internal use misconception, data aggregation exemption, downstream use ignorance, over-classification waste
10. **Connect classification to compliance** - How risk level determines documentation, conformity assessment, monitoring, reporting obligations

---

## The Risk Pyramid: Four Levels

Think of AI regulation under the EU AI Act as a pyramid:

```
          /\
         /  \
        / üö´ \     PROHIBITED (Banned entirely)
       /------\
      /   ‚ö†Ô∏è   \    HIGH-RISK (Heavy regulation)
     /----------\
    /    ‚ìò      \   LIMITED RISK (Transparency rules)
   /--------------\
  /     ‚úì         \  MINIMAL RISK (No AI-specific rules)
 /------------------\
```

The higher you go on the pyramid, the fewer AI systems there are‚Äîbut the stricter the rules.

---

## Level 1: Prohibited AI Practices (The Red Zone)

Some AI applications are considered so dangerous to fundamental rights and democratic values that the EU banned them outright. These aren't just heavily regulated‚Äîthey're **illegal**.

### What's Banned

**1. Social Scoring Systems**
AI that evaluates people's "trustworthiness" based on social behavior and uses that score to treat them differently.

*Real-world example*: China's social credit system, where citizens lose points for things like jaywalking or posting "fake news," and those points affect whether they can get loans, buy plane tickets, or enroll their kids in good schools.

*Why it's banned*: It creates a surveillance society where every action is judged and recorded, chilling free speech and behavior.

**2. Subliminal Manipulation**
AI that manipulates people through techniques they can't consciously perceive.

*Real-world example*: Imagine an AI that uses subliminal audio cues or micro-visual signals to make you buy products or vote for certain candidates without you realizing you're being influenced.

*Why it's banned*: People can't protect themselves from manipulation they don't know is happening.

**3. Exploitation of Vulnerabilities**
AI that specifically targets children, elderly people, or people with disabilities to manipulate their behavior in harmful ways.

*Real-world example*: An AI toy that encourages children to engage in dangerous behavior, or an AI assistant that exploits a confused elderly person to make purchases.

*Why it's banned*: It preys on those least able to protect themselves.

**4. Real-Time Biometric Identification in Public Spaces (for law enforcement)**
Facial recognition cameras scanning everyone in a public square to identify specific individuals.

*Limited exceptions*: Finding kidnapping victims, preventing imminent terrorist attacks, or locating suspects in serious crimes‚Äîbut only with judicial authorization and strict safeguards.

*Why it's largely banned*: Mass surveillance threatens privacy and freedom of assembly for everyone, not just criminals.

**5. Emotion Recognition in Workplaces and Schools**
AI that infers emotions from biometric data (facial expressions, voice analysis) in employment and educational contexts.

*Real-world example*: Software that monitors whether call center employees are smiling enough, or analyzes student facial expressions to gauge engagement.

*Why it's banned*: It creates coercive environments where people feel constantly watched and judged for their emotional states.

**6. Biometric Categorization Based on Sensitive Characteristics**
AI that categorizes people by race, political opinions, sexual orientation, or religious beliefs using biometric data.

*Real-world example*: AI that claims to identify sexual orientation from facial features, or systems that categorize people by ethnicity from photos.

*Why it's banned*: It enables discrimination and violates human dignity.

**7. Untargeted Scraping for Facial Recognition Databases**
Creating databases of faces by scraping images from the internet or CCTV without consent.

*Real-world example*: Clearview AI scraped billions of photos from social media to create a facial recognition database sold to law enforcement.

*Why it's banned*: It violates privacy rights and enables mass surveillance without consent.

### Timeline

These prohibitions took effect on **February 2, 2025**.

---

## Level 2: High-Risk AI Systems (The Yellow Zone)

This is where most of the regulatory action happens. High-risk AI systems aren't banned, but they face **extensive requirements** before and after being placed on the market.

### What Makes AI "High-Risk"?

The EU AI Act uses two pathways to classify AI as high-risk:

**Pathway 1: Safety Components (Annex I)**
AI that serves as a safety component of products already covered by EU product safety legislation:
- Medical devices
- Vehicles
- Toys
- Aviation equipment
- Machinery
- Marine equipment

*Example*: The AI system that helps an autonomous vehicle decide when to brake.

**Pathway 2: Specific Use Cases (Annex III)**
AI used in these sensitive areas, regardless of what product it's part of:

| Area | Examples |
|------|----------|
| **Biometrics** | Facial recognition, fingerprint matching (where allowed) |
| **Critical Infrastructure** | AI managing electricity grids, water systems, traffic |
| **Education** | AI that grades exams, admits students, detects cheating |
| **Employment** | Resume screening, interview analysis, performance monitoring |
| **Essential Services** | Credit scoring, loan approval, insurance pricing |
| **Law Enforcement** | Risk assessment tools, evidence analysis, crime prediction |
| **Migration/Border Control** | Visa application assessment, document verification |
| **Justice** | AI assisting judges with sentencing or case research |
| **Democratic Processes** | AI used to influence voting behavior |

### Important Exception

Even if an AI system falls into an Annex III category, it's NOT considered high-risk if it:
- Performs a narrow procedural task
- Improves the result of a previously completed human activity
- Detects decision-making patterns without replacing human judgment
- Only performs preparatory tasks for assessments

*Example*: AI that formats a resume for easier reading isn't high-risk, but AI that scores candidates is.

### What High-Risk Providers Must Do

If your AI is high-risk, you must comply with extensive requirements including:

1. Establish a risk management system
2. Meet data quality and governance requirements
3. Create and maintain technical documentation
4. Enable record-keeping (logging)
5. Provide transparency and information to users
6. Allow for human oversight
7. Ensure accuracy, robustness, and cybersecurity
8. Conduct conformity assessment before market placement

---

## Level 3: Limited Risk AI (The Amber Zone)

Some AI systems don't face the full weight of high-risk requirements but still need to meet **specific transparency obligations**.

### The Transparency Requirement

The principle is simple: **people should know when they're interacting with AI or when content was created by AI**.

### What Systems Are Covered

**1. AI Systems Interacting with Humans (Chatbots)**
If someone is talking to an AI, they must be told they're talking to an AI.

*Example*: A customer service chatbot must clearly state "You are chatting with an AI assistant" or similar disclosure.

*Exception*: If it's obvious from the context (like a clearly-labeled AI character in a video game).

**2. Emotion Recognition and Biometric Categorization Systems**
If an AI is analyzing someone's emotions or categorizing them biometrically, that person must be informed.

*Example*: If a store uses AI to analyze customer emotions, signage must inform customers.

*Note*: Remember, emotion recognition in workplaces and schools is banned entirely‚Äîthis applies to other contexts where it's permitted.

**3. Deepfakes and AI-Generated Content**
Content that's artificially generated or manipulated must be labeled as such.

*Example*: An AI-generated image of a public figure must be marked as artificially created.

*Exception*: Artistic or satirical work where the artificial nature is part of the creative intent.

**4. AI-Generated Text for Public Information**
Text generated by AI on matters of public interest must be disclosed as AI-generated.

*Example*: An AI-written news article must disclose that it was generated by artificial intelligence.

---

## Level 4: Minimal Risk AI (The Green Zone)

The vast majority of AI systems fall here and face **no AI-specific regulations** under the EU AI Act.

### Examples of Minimal Risk AI

- Spam filters
- Video game AI
- Inventory management systems
- Music recommendation algorithms
- Photo filters that add bunny ears
- Predictive text on your phone
- AI-powered search engines (for general search)

### Why No Special Rules?

These systems pose minimal risk to fundamental rights. If something goes wrong‚Äîyour spam filter misses an email, or your game's AI opponent does something weird‚Äîthe consequences are minor.

That doesn't mean these systems are completely unregulated. They still must comply with:
- Existing consumer protection laws
- Data protection laws (GDPR)
- Product safety regulations
- Competition law

They just don't have AI-specific obligations under the EU AI Act.

---

## How to Classify Your AI System: A Practical Walkthrough

Let's say you've developed an AI system and need to figure out its classification. Here's the step-by-step process:

### Step 1: Check the Prohibited List

Go through the list of prohibited practices. Does your AI:
- Assign social scores?
- Use subliminal techniques?
- Exploit vulnerable groups?
- Conduct real-time biometric ID in public (without qualifying exceptions)?
- Do emotion recognition in workplaces/schools?
- Categorize people by sensitive traits using biometrics?
- Scrape faces for databases?

If yes to any: **STOP. Your AI is prohibited.**

### Step 2: Check Annex I (Safety Components)

Is your AI a safety component of a product covered by EU product safety legislation?

If yes: **Your AI is high-risk.**

### Step 3: Check Annex III (Sensitive Uses)

Is your AI used in any of the eight sensitive areas listed in Annex III?

If yes, consider: Does it qualify for an exception (narrow procedural task, etc.)?

If no exception applies: **Your AI is high-risk.**

### Step 4: Check Transparency Obligations

Is your AI:
- Interacting with humans (chatbot-style)?
- Recognizing emotions or categorizing biometrics?
- Generating or manipulating content (deepfakes)?

If yes: **Your AI has limited-risk transparency obligations.**

### Step 5: Default to Minimal Risk

If none of the above apply: **Your AI is minimal risk.**

---

## Real-World Classification Examples

| AI System | Classification | Reasoning |
|-----------|---------------|-----------|
| Resume screening software | High-Risk | Employment decision-making (Annex III) |
| Customer service chatbot | Limited Risk | Interacts with humans; must disclose |
| Credit scoring algorithm | High-Risk | Access to essential services (Annex III) |
| Email spam filter | Minimal Risk | None of the above categories apply |
| Social media content recommendation | Minimal Risk* | General recommendation; not targeting vulnerabilities |
| Student exam proctoring | High-Risk | Educational context (Annex III) |
| Facial recognition door access | High-Risk | Biometric identification (Annex III) |
| AI-generated marketing image | Limited Risk | Must be labeled as AI-generated |
| Self-driving car AI | High-Risk | Safety component (Annex I) |
| Emotion monitoring for drivers | Potentially Prohibited | Depends on context; workplace = banned |

*Note: Large platforms with recommender systems have separate obligations under the EU Digital Services Act.

---

## Common Classification Mistakes

### Mistake 1: Assuming "Internal Use" Means No Regulation

If your company uses AI only internally (not sold to others), you're a "deployer," not a "provider." But high-risk AI used internally still triggers deployer obligations.

### Mistake 2: Thinking "We Just Aggregate Data"

If your AI makes or significantly influences decisions about people, the fact that it's "just" processing data doesn't exempt it.

### Mistake 3: Ignoring Downstream Uses

If you build a foundation model or general-purpose AI, you may have obligations even though you don't control how it's ultimately used.

### Mistake 4: Over-Classifying to Be Safe

Some organizations assume everything is high-risk "just to be safe." This wastes resources and creates unnecessary compliance burdens. Accurate classification matters.

---

## Conclusion

Risk classification is the foundation of EU AI Act compliance. Everything else‚Äîdocumentation, conformity assessment, monitoring‚Äîflows from how your AI system is classified.

The good news: the system is logical. High-risk categories target AI that can significantly affect people's lives: their jobs, their education, their access to services, their freedom. Most AI systems will fall into minimal risk with no special requirements.

The challenge: you need to honestly assess your AI systems against these criteria. Wishful thinking doesn't change your classification‚Äîand regulators will eventually check.

Get the classification right first. Everything else follows from there.

---

## Sources

1. European Union. "Regulation (EU) 2024/1689 of the European Parliament and of the Council (EU AI Act)." Official Journal of the European Union, Articles 5-7 and Annexes I-III, 2024.

2. European Commission. "AI Act: Different Rules for Different Risk Levels." 2024.

3. Future of Life Institute. "EU Artificial Intelligence Act: Risk-Based Approach." 2024.

4. IAPP. "EU AI Act Risk Classification Analysis." 2024.

5. European Parliament. "Artificial Intelligence Act: Deal on Comprehensive Rules for Trustworthy AI." Press Release, December 2023.

6. Stanford HAI. "Analyzing the EU AI Act: What You Need to Know." Human-Centered Artificial Intelligence, 2024.

7. Veale, Michael and Zuiderveen Borgesius, Frederik. "Demystifying the Draft EU Artificial Intelligence Act." Computer Law Review International, 2021.

---

*Next: High-Risk AI Systems - Compliance Requirements in Detail*
