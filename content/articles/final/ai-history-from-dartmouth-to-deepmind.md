---
title: AI History — From Dartmouth to DeepMind
slug: ai-history-from-dartmouth-to-deepmind
path: history
tldr: From the optimistic summer of 1956 to ChatGPT's 100 million users in two months, AI's 70-year journey reveals cycles of hype, disappointment, and breakthrough.
contentSections:
  - Summary
  - The Summer That Started It All
  - The Dartmouth Conference Where AI Got Its Name
  - The Winters When Reality Bit Back
  - The Milestones Machines Start Winning
  - The Transformer Revolution Attention Changes Everything
  - From Dartmouth to Today What Have We Learned
relatedConcepts:
  - ai-winters
  - dartmouth-conference
  - deep-blue
  - alphago
  - transformer-architecture
  - chatgpt
crossPathRefs:
  terminology:
    - artificial-intelligence-definition
    - deep-learning-explained
  responsibility:
    - ai-governance-frameworks-building-your-organizations-approac
  future:
    - the-future-of-ai-regulation-whats-coming-next
tags:
  - ai history
  - dartmouth conference
  - ai winters
  - deep learning
  - transformers
category: AI Fundamentals
image: ai-history-from-dartmouth-to-deepmind.jpg
imageAlt: Timeline visualization from 1956 Dartmouth Conference to modern AI systems including Deep Blue, AlphaGo, and ChatGPT
author: Sunil Iyer
publishDate: 2025-02-14
readingTime: 6
seoTitle: AI History: From Dartmouth (1956) to DeepMind & ChatGPT
seoDescription: Explore 70 years of AI evolution—from the optimistic Dartmouth Conference through AI Winters to breakthrough moments like Deep Blue, AlphaGo, and ChatGPT's explosive growth.
---

## Summary

The field of artificial intelligence has traveled a remarkable journey from its birth at the 1956 Dartmouth Conference to today's powerful language models and autonomous systems. Understanding this history reveals patterns of overpromising, disappointment, unexpected breakthroughs, and governance challenges that remain relevant today.

**Key Takeaways**:

- AI research began with spectacular overconfidence—organizers expected to solve intelligence in 8 weeks
- Two "AI Winters" (1970s and 1980s) taught hard lessons about overpromising and underfunding
- Breakthrough moments (Deep Blue, AlphaGo, AlexNet, ChatGPT) often came suddenly after long quiet periods
- Modern AI governance must account for unpredictable timelines and rapid public adoption
- The gap between laboratory demos and mainstream deployment can now be measured in weeks, not years

**Reading Time**: 6 minutes

---

## Key Learning Objectives

After reading this article, you will be able to:

1. **Trace the timeline** of AI development from the 1956 Dartmouth Conference to modern large language models
2. **Understand AI Winters** and why funding dried up twice in AI's history despite promising early results
3. **Identify breakthrough moments** that changed public perception of AI capabilities (Deep Blue, AlphaGo, AlexNet, ChatGPT)
4. **Recognize historical patterns** of hype, disappointment, and unexpected progress in AI development
5. **Apply historical lessons** to current AI governance challenges, especially around timeline prediction and rapid adoption
6. **Explain the Transformer revolution** and why "Attention Is All You Need" changed everything starting in 2017

---

## The Summer That Started It All

![AI History — From Dartmouth to DeepMind]({{IMAGE_PLACEHOLDER_ai-history-from-dartmouth-to-deepmind}})

In the summer of 1956, ten scientists gathered at Dartmouth College with a rather ambitious plan: solve artificial intelligence in about eight weeks. Their proposal stated with remarkable confidence that "every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it." They requested funding for a two-month workshop, believing that a "significant advance" could be made if a carefully selected group worked on the problem together.

This might be the most spectacularly wrong time estimate in the history of science. Nearly seven decades later, we're still working on those original goals. But what emerged from that New Hampshire summer wasn't a solution—it was the birth of an entirely new field.

**[See detailed example: The Dartmouth Conference →]({{LINK_EXAMPLE_dartmouth-conference}})**

---

## The Dartmouth Conference: Where AI Got Its Name

The Dartmouth Summer Research Project on Artificial Intelligence ran from June 18 to August 17, 1956. The organizers—John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon—were intellectual heavyweights. McCarthy would later invent the programming language LISP. Shannon had essentially created information theory. These weren't dreamers; they were serious scientists who happened to be wildly optimistic.

The conference didn't produce the breakthrough its organizers envisioned, but it accomplished something arguably more important: it unified a scattered research community under a single banner. Before Dartmouth, researchers working on machine intelligence called their field by various names—cybernetics, automata theory, complex information processing. McCarthy's term "artificial intelligence" stuck, giving the field an identity and a mission.

The early years following Dartmouth buzzed with genuine achievements. Arthur Samuel's checkers program learned to beat its creator. Newell and Simon's Logic Theorist proved mathematical theorems. ELIZA, an early chatbot, convinced some users they were talking to a human therapist. Optimism ran high. Herbert Simon predicted in 1965 that "machines will be capable, within twenty years, of doing any work a man can do."

---

## The Winters: When Reality Bit Back

Then reality intervened, as it tends to do with overpromised technology.

The first AI Winter arrived in the early 1970s. The Lighthill Report of 1973, commissioned by the British government, delivered a devastating assessment: AI research had failed to deliver on its promises. DARPA funding plummeted from approximately $30 million annually to almost nothing by 1974. The Stanford AI Lab, once a flagship research center, closed in 1979. Researchers found that problems which seemed simple to humans—recognizing faces, understanding natural language, navigating a room—were extraordinarily difficult to program.

A brief revival came in the 1980s with expert systems—programs that captured human expertise in narrow domains. Companies invested billions. Japan launched its Fifth Generation Computer project with $850 million in funding. The United States and Europe scrambled to compete. For a moment, it seemed AI's time had finally arrived.

It hadn't. Expert systems proved brittle and expensive to maintain. Knowledge changed faster than it could be encoded. Markets turned out to be smaller than projected. By 1987, the specialized LISP machine market collapsed almost overnight as cheaper general-purpose computers caught up. The second AI Winter had begun, and it would last nearly a decade.

The term "artificial intelligence" became toxic in funding proposals. Researchers started calling their work "informatics" or "computational intelligence"—anything to avoid the stigma.

---

## The Milestones: Machines Start Winning

The thaw began with games—specifically, with a chess match that captivated the world.

On May 11, 1997, IBM's Deep Blue defeated world chess champion Garry Kasparov. It was the first time a computer had beaten a reigning world champion under standard tournament conditions. Deep Blue could evaluate 200 million chess positions per second through sheer computational force. Kasparov, stunned by his loss, accused IBM of cheating. He wasn't entirely wrong to be suspicious—there had never been anything like this before.

**[See detailed example: Deep Blue vs. Kasparov →]({{LINK_EXAMPLE_deep-blue}})**

But Deep Blue was a specialist, and chess, while complex, has clear rules. The ancient Chinese game of Go, with more possible board configurations than atoms in the universe, remained safely beyond computer reach. Experts predicted it would take another decade or more for AI to challenge top human players.

Those experts were wrong by about fifteen years.

In March 2016, Google DeepMind's AlphaGo defeated Lee Sedol, one of the greatest Go players in history, four games to one. Over 200 million people watched online. Unlike Deep Blue's brute-force approach, AlphaGo used neural networks and reinforcement learning—it had taught itself by playing millions of games against itself. In Game Two, AlphaGo played Move 37, a move so unconventional it had a 1 in 10,000 chance of being chosen by a human player. Commentators initially thought it was a mistake. It wasn't. It was creative.

**[See detailed example: AlphaGo vs. Lee Sedol and Move 37 →]({{LINK_EXAMPLE_alphago}})**

Meanwhile, a quieter revolution had begun four years earlier. In September 2012, a neural network called AlexNet won the ImageNet image recognition challenge by a staggering margin—its error rate was 10.8 percentage points better than the runner-up. AlexNet proved that deep learning, long dismissed as impractical, could achieve breakthrough results when combined with large datasets and powerful GPUs. The researchers—Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton—had trained it on two gaming graphics cards in a bedroom.

**[See detailed example: AlexNet and the Deep Learning Revolution →]({{LINK_EXAMPLE_alexnet}})**

---

## The Transformer Revolution: Attention Changes Everything

The modern AI explosion traces to a 2017 paper with a clever title: "Attention Is All You Need." Eight Google researchers introduced the Transformer architecture, a new way for neural networks to process sequences of data. Previous approaches required processing information step by step; Transformers could examine entire sequences simultaneously, dramatically speeding up training and improving results.

**[Use the Transformer Timeline Template to understand this evolution →]({{LINK_TEMPLATE_transformer-timeline}})**

The paper's authors couldn't have fully anticipated what they'd unleashed. Within a year, OpenAI released GPT-1, a language model built on the Transformer architecture. GPT-2 followed in 2019, then GPT-3 in 2020 with 175 billion parameters—over a thousand times larger than its predecessor.

On November 30, 2022, OpenAI released ChatGPT to the public. Within five days, it had a million users. Within two months, it had 100 million. For the first time, ordinary people could hold conversations with AI systems that felt genuinely intelligent. The AI boom that had been building for a decade exploded into public consciousness.

**[See detailed example: ChatGPT's Explosive Launch →]({{LINK_EXAMPLE_chatgpt}})**

---

## From Dartmouth to Today: What Have We Learned?

The scientists at Dartmouth in 1956 weren't wrong about AI's potential—they were wrong about the timeline by roughly six decades. The field's history is a cautionary tale about the gap between vision and implementation, between laboratory demonstrations and real-world deployment.

Each AI Winter taught the same lesson: overpromising leads to underfunding. Each breakthrough—from Deep Blue to AlphaGo to ChatGPT—reminded us that progress often comes suddenly, after long periods of apparent stagnation.

For AI governance professionals, this history matters. We're not dealing with a technology that emerged fully formed; we're dealing with one that has cycled through hype and disappointment for nearly seventy years. The current capabilities are real, but so is the tendency toward overconfidence. Understanding where AI came from helps us navigate where it's going—and perhaps avoid promising we'll solve everything in eight weeks.

---

## Sources and Further Reading

1. **McCarthy, J., Minsky, M., Rochester, N., & Shannon, C.** (1955). "A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence." Available at: jmc.stanford.edu/articles/dartmouth/dartmouth.pdf

2. **Lighthill, J.** (1973). "Artificial Intelligence: A General Survey." Science Research Council Report.

3. **Russell, S., & Norvig, P.** (2020). *Artificial Intelligence: A Modern Approach* (4th ed.). Pearson. Chapter 1: History of AI.

4. **Campbell, M., Hoane, A. J., & Hsu, F.** (2002). "Deep Blue." *Artificial Intelligence*, 134(1-2), 57-83.

5. **Silver, D., et al.** (2016). "Mastering the Game of Go with Deep Neural Networks and Tree Search." *Nature*, 529, 484-489.

6. **Krizhevsky, A., Sutskever, I., & Hinton, G.** (2012). "ImageNet Classification with Deep Convolutional Neural Networks." *NeurIPS 2012*.

7. **Vaswani, A., et al.** (2017). "Attention Is All You Need." *NeurIPS 2017*. Available at: arxiv.org/abs/1706.03762

8. **Brown, T., et al.** (2020). "Language Models are Few-Shot Learners." *NeurIPS 2020*. (GPT-3 paper)

9. **OpenAI** (2023). "GPT-4 Technical Report." Available at: openai.com/research/gpt-4

10. **Crevier, D.** (1993). *AI: The Tumultuous History of the Search for Artificial Intelligence*. Basic Books.

---

## Related Articles

**In This Path (History)**:
- [The Evolution of Machine Learning: From Perceptrons to Deep Learning]
- [A Timeline of AI Regulation: From Science Fiction to Legislative Reality]

**Cross-Path Connections**:

**Terminology**:
- [Artificial Intelligence: Defining an Elusive Concept]({{LINK_terminology_ai-definition}})
- [Deep Learning Decoded: Neural Networks for Non-Engineers]({{LINK_terminology_deep-learning}})
- [Understanding the Transformer Architecture]({{LINK_terminology_transformers}})

**Responsibility & Governance**:
- [AI Governance Frameworks: Building Your Organization's Approach]({{LINK_responsibility_governance-frameworks}})
- [Preparing for AI Regulation: A Compliance Roadmap]({{LINK_responsibility_compliance-roadmap}})

**Risk**:
- [AI Hype Cycles: Separating Signal from Noise]({{LINK_risk_hype-cycles}})
- [When AI Goes Wrong: A Taxonomy of AI Harms]({{LINK_risk_ai-harms}})

**Future**:
- [The Future of AI Regulation: What's Coming Next]({{LINK_future_regulation}})
- [Artificial General Intelligence: Hype, Hope, and Governance]({{LINK_future_agi}})

---

## Component Assets

**Examples**:
- [AI History Examples: Deep Blue, AlphaGo, AlexNet, ChatGPT, and Dartmouth]({{LINK_EXAMPLES_ai-history-from-dartmouth-to-deepmind-examples}})

**Templates**:
- [The Transformer Revolution Timeline Template]({{LINK_TEMPLATE_ai-history-transformer-timeline}})

**Images Required**:
1. `ai-history-from-dartmouth-to-deepmind.jpg` - Hero image: Timeline visualization
2. `ai-history-dartmouth-conference.jpg` - 1956 conference participants
3. `ai-history-deep-blue-kasparov.jpg` - Chess match moment
4. `ai-history-alphago-move37.jpg` - Go board showing famous Move 37
5. `ai-history-chatgpt-growth.jpg` - User growth chart
6. `ai-history-timeline-full.jpg` - Complete 1956-2025 timeline infographic

**Flowcharts**: None

**Lists**: Timeline summary table (included in examples file)

**Tables**: Milestone comparison table (included in examples file)

---

*This article is part of the AI Governance Learning Platform. Visit [AIDefence](https://suniliyer.ca) or the [AIDefence YouTube channel](https://youtube.com/@aidefence) for the complete curriculum.*

---

**Metadata for Platform**:
- **Learning Path**: History
- **Difficulty Level**: Beginner
- **Prerequisites**: None (foundational article)
- **Next Recommended**: "AI Terminology: Understanding the AI Family Tree"
- **Est. Completion Time**: 6 minutes reading + 10 minutes examples
