---
title: Untitled
slug: ai-hallucinations-when-machines-confidently-lie
path: responsibility
tldr: Explore key concepts and practical applications in AI governance.
contentSections:
  - The Case for Liability Reform
  - Reform Proposals
  - Allocating Responsibility in the AI Chain
  - Sector-Specific Considerations
  - International Approaches
  - Policy Considerations
  - The Future of AI Liability
relatedConcepts: []
crossPathRefs:
tags:
  - ai
  - governance
  - responsibility
category: AI & Technology
image: ai-hallucinations-when-machines-confidently-lie.jpg
imageAlt: Untitled
author: Sunil Iyer
publishDate: 2025-12-23
readingTime: 11
seoTitle: Untitled
seoDescription: Explore key concepts and practical applications in AI governance.
---



## The Case for Liability Reform


### Why Reform Is Needed

**Compensation:** Victims of AI harm deserve compensation. If existing law doesn't provide it, reform is needed.

**Deterrence:** Liability incentivizes safety. If AI developers face no liability, they have less incentive to make AI safe.

**Fairness:** Those who profit from AI should bear its risks. Shifting risk to victims is unfair.

**Innovation:** Contrary to intuition, clear liability rules may help innovation by providing predictability. Uncertainty about liability chills investment.

**Trust:** Public trust in AI depends on accountability. Liability rules that fail to hold anyone responsible undermine trust.


### Arguments Against Major Reform

**Existing law is adequate:** Courts have adapted liability rules to new technologies before. They can do so for AI.

**Reform is premature:** We don't yet understand AI's risks well enough to design appropriate rules. Premature reform may get it wrong.

**Innovation concerns:** Liability rules that are too strict could chill beneficial AI development.

**Sector-specific solutions:** Different AI applications have different risks. Horizontal liability rules may not fit all contexts.

**Alternative mechanisms:** Insurance, regulation, and compensation funds may address harm without traditional liability.

---


## Reform Proposals


### Strict Liability for AI

**Concept:** Hold AI developers or deployers strictly liable for AI harm, without requiring proof of negligence or defect.

**Arguments for:**
- Simplifies victim's burden of proof
- Places risk on parties best positioned to manage it
- Creates strong safety incentives
- Addresses the black box problem

**Arguments against:**
- May be excessive for low-risk AI
- Could chill innovation
- Doesn't distinguish careful from careless developers
- May be difficult to define scope

**Variants:**
- Strict liability only for "high-risk" AI
- Strict liability with caps on damages
- Strict liability with regulatory safe harbors


### Presumption of Causation

**Concept:** If AI was involved in harm, presume the AI caused it unless the defendant proves otherwise.

**Arguments for:**
- Addresses the proof problem without full strict liability
- Maintains fault basis while shifting burden
- Incentivizes transparency

**Arguments against:**
- May lead to liability for AI that wasn't actually causal
- Still requires defining who the defendant is
- Defendants may not be able to disprove causation for black-box AI

**The EU AI Liability Directive takes this approach (discussed in Article 50).**


### Mandatory Disclosure

**Concept:** Require AI developers to disclose information that would enable liability claims—training data, model details, testing results.

**Arguments for:**
- Addresses information asymmetry
- Enables traditional liability to function
- Respects existing legal frameworks

**Arguments against:**
- Trade secret concerns
- Disclosure may not enable understanding of complex AI
- Creates compliance burden


### Mandatory Insurance

**Concept:** Require AI developers or deployers to maintain insurance against AI harm.

**Arguments for:**
- Ensures compensation is available
- Insurance companies become AI safety monitors
- Doesn't require proving specific liability

**Arguments against:**
- Insurance market for AI risk is immature
- Premiums may be unpredictable
- Doesn't directly create accountability


### Compensation Funds

**Concept:** Create funds to compensate AI harm victims, financed by AI developers or through taxes.

**Arguments for:**
- Guarantees compensation
- Avoids case-by-case litigation
- Can cover harms that don't fit liability frameworks

**Arguments against:**
- Reduces individual accountability
- Political challenges in establishing funds
- Difficult to calibrate contributions


### Algorithmic Accountability

**Concept:** Require AI systems to be auditable and explainable as a precondition for deployment.

**Arguments for:**
- Addresses black box problem at the source
- Enables oversight and accountability
- Creates records for liability claims

**Arguments against:**
- May not be technically feasible for all AI
- Could exclude beneficial but unexplainable AI
- Creates compliance burden

---


## Allocating Responsibility in the AI Chain


### Developer vs. Deployer

A fundamental question: should liability fall on AI developers (who create the technology) or deployers (who put it into use)?

**Arguments for developer liability:**
- Developers have the most knowledge about AI
- Developers make design choices that affect safety
- Developers are best positioned to improve AI

**Arguments against developer liability:**
- Developers can't control how AI is used
- Deployers choose to use AI and how
- Deployers interact with users/victims

**Arguments for deployer liability:**
- Deployers choose to deploy AI
- Deployers control the use context
- Deployers have relationships with users/victims

**Arguments against deployer liability:**
- Deployers may not understand AI
- Deployers can't fix developer problems
- Smaller deployers may be judgment-proof

**Possible approaches:**
- Joint and several liability (either can be sued)
- Primary liability on deployer with contribution from developer
- Allocation based on who caused the specific harm


### The Role of Users

Users may also bear some responsibility:

**User assumption of risk:** If users knowingly use risky AI, should they bear some responsibility?

**User misuse:** If users misuse AI (against instructions, for unintended purposes), does liability shift?

**User modification:** If users modify or fine-tune AI, do they become liable?

**Consumer vs. business users:** Should liability rules differ for consumer products vs. B2B AI?

---


## Sector-Specific Considerations


### Autonomous Vehicles

**Current framework:** Vehicle manufacturers face product liability. Many states have AV-specific laws allocating responsibility.

**Key issues:**
- Who is the "driver" when there is no driver?
- How to handle mixed human/AI control?
- Insurance requirements for AVs

**Proposals:**
- Strict manufacturer liability for AV accidents
- No-fault insurance schemes for AVs
- Federal preemption of state liability rules


### Medical AI

**Current framework:** Medical device regulations (FDA), medical malpractice law, product liability.

**Key issues:**
- Physician liability when relying on AI
- Device manufacturer liability for AI diagnostics
- Learned intermediary doctrine (does warning the doctor suffice?)

**Proposals:**
- Clear allocation between AI manufacturer and physician
- Updated informed consent for AI-assisted care
- Safe harbors for reasonable AI reliance


### Financial Services AI

**Current framework:** Financial regulation, contract law, fiduciary duties.

**Key issues:**
- Liability for discriminatory AI lending
- Liability for robo-adviser recommendations
- Liability for algorithmic trading failures

**Proposals:**
- Algorithmic impact assessments as liability factor
- Extended duties of care for financial AI
- Collective redress for AI discrimination


### Content Recommendation AI

**Current framework:** Section 230 immunity in US; varying approaches internationally.

**Key issues:**
- Liability for radicalization, self-harm, misinformation
- Platform vs. publisher distinction
- Amplification vs. creation

**Proposals:**
- Modify Section 230 for algorithmic amplification
- Duty of care for recommendation systems
- Transparency requirements as condition of immunity

---


## International Approaches


### European Union

The EU is pursuing comprehensive AI liability reform:

**Revised Product Liability Directive (2022 proposal):**
- Explicitly includes software as "product"
- Addresses AI components in products
- Modernizes defect concepts for AI

**AI Liability Directive (2022 proposal):**
- Presumption of causation for AI harm
- Disclosure requirements
- Applies to harm from high-risk AI under AI Act

(Detailed in Article 50)


### United Kingdom

Post-Brexit, the UK is developing its own approach:

**Law Commission recommendations:** The Law Commission has examined AI and liability, recommending incremental reform rather than wholesale change.

**Sector-specific focus:** The UK is pursuing sector-specific regulation rather than horizontal AI liability rules.


### United States

The US has no comprehensive AI liability reform, but:

**NIST AI RMF:** The AI Risk Management Framework provides voluntary standards that could influence liability.

**Sector regulation:** FDA, NHTSA, and financial regulators are developing AI-specific approaches.

**State action:** Some states are enacting AI disclosure and anti-discrimination requirements that could affect liability.


### China

China has enacted AI regulations but liability frameworks are less developed:

**Civil Code provisions:** General tort liability applies to AI.

**Specific regulations:** Regulations on recommendation algorithms, deep synthesis (deepfakes), and generative AI include provisions that could affect liability.

---


## Policy Considerations


### Balancing Innovation and Safety

The central policy challenge is balancing:

**Innovation:** AI offers enormous benefits. Excessive liability could deter beneficial development.

**Safety:** AI poses real risks. Insufficient liability could lead to inadequate safety investment.

**Different stakeholders weight these differently:**
- Industry emphasizes innovation
- Consumer advocates emphasize safety
- Academics often seek balance

**Approaches to balance:**
- Risk-based regulation (more liability for riskier AI)
- Safe harbors for compliant developers
- Tiered liability based on harm severity
- Innovation-friendly procedures (quick resolution, limited discovery)


### Distributional Concerns

Liability rules have distributional effects:

**Who bears costs?**
- Developers (reducing returns)
- Consumers (through higher prices)
- Insurers (through premiums)
- Taxpayers (through compensation funds)
- Victims (through uncompensated harm)

**Who benefits?**
- Large companies (able to self-insure, influence rules)
- Lawyers (through litigation)
- Safety industry (compliance consulting)
- Consumers (through safer products)


### Harmonization vs. Diversity

Should AI liability rules be:

**Harmonized globally?**
- Provides certainty for global companies
- Prevents race to the bottom
- Simplifies compliance

**Or diverse by jurisdiction?**
- Allows experimentation
- Reflects local values
- Enables competition between approaches

---


## The Future of AI Liability


### Likely Developments

**More sector-specific rules:** Different AI applications will likely get different liability treatment.

**EU leadership:** The EU's AI Liability Directive will influence global thinking.

**Insurance evolution:** AI liability insurance will develop, shaping practical risk allocation.

**Standards development:** Technical standards for AI safety will influence liability by defining "reasonable" practices.

**Case law evolution:** As AI cases are litigated, courts will develop AI-specific doctrine.


### Key Uncertainties

**Will strict liability prevail?** Or will fault-based approaches adapt?

**How will causation be proven?** Will presumptions become standard?

**Who will bear ultimate responsibility?** Developers, deployers, or users?

**How will damages be measured?** Especially for novel AI harms.

**Will compensation funds emerge?** For harms that don't fit traditional liability.


### What Organizations Should Do Now

**Monitor developments:** Track evolving liability rules in relevant jurisdictions.

**Document carefully:** Maintain records that could demonstrate reasonable practices.

**Consider insurance:** Explore AI liability coverage as it develops.

**Allocate risk contractually:** Define liability allocation in vendor and customer contracts.

**Invest in safety:** Regardless of liability rules, safe AI is good AI.

---


## Conclusion

When AI fails, traditional liability law often fails too. Product liability's categories don't fit software that changes over time. Negligence law's reasonable person standard doesn't translate to reasonable AI development. The black box nature of AI defeats the proof requirements both frameworks demand.

The result is a liability gap that threatens both victims (who may go uncompensated) and society (which loses the safety incentives liability creates).

Reform is coming. The EU is leading with proposals that shift burdens to AI developers and create presumptions of causation. Other jurisdictions are watching and considering their own approaches. The debate continues over how to balance innovation against safety, how to allocate responsibility across the AI value chain, and how to adapt centuries-old legal concepts to 21st-century technology.

For organizations deploying AI, the path forward requires:

1. **Understanding current law:** Know what liability you already face.
2. **Anticipating reform:** Prepare for stricter rules that are likely coming.
3. **Documenting practices:** Create records that could demonstrate reasonable care.
4. **Investing in safety:** The best liability strategy is avoiding harm in the first place.
5. **Allocating risk thoughtfully:** Use contracts and insurance to manage uncertainty.

The question "who pays when AI fails?" doesn't have a clear answer today. But the answer is being written—by legislators, regulators, courts, and the companies building AI. Everyone has a stake in getting it right.

---


## Sources

1. European Commission. "Proposal for a Directive on adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive)." COM(2022) 496 final, September 28, 2022.

2. European Commission. "Proposal for a Directive on liability for defective products (revised Product Liability Directive)." COM(2022) 495 final, September 28, 2022.

3. Vladeck, David C. "Machines Without Principals: Liability Rules and Artificial Intelligence." Washington Law Review, Vol. 89, 2014.

4. Selbst, Andrew D. "Negligence and AI's Human Users." Boston University Law Review, Vol. 100, 2020.

5. Chagal-Feferkorn, Karni. "The Reasonable Algorithm." University of Illinois Journal of Law, Technology & Policy, Vol. 2018, 2018.

6. Buiten, Miriam C. "Towards Intelligent Regulation of Artificial Intelligence." European Journal of Risk Regulation, Vol. 10, Issue 1, 2019.

7. Hubbard, F. Patrick. "Sophisticated Robots: Balancing Liability, Regulation, and Innovation." Florida Law Review, Vol. 66, 2014.

8. Scherer, Matthew U. "Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies." Harvard Journal of Law & Technology, Vol. 29, Issue 2, 2016.

9. Abbott, Ryan. "The Reasonable Computer: Disrupting the Paradigm of Tort Liability." George Washington Law Review, Vol. 86, 2018.

10. Geistfeld, Mark A. "A Roadmap for Autonomous Vehicles: State Tort Liability, Automobile Insurance, and Federal Safety Regulation." California Law Review, Vol. 105, 2017.

11. European Parliament. "Civil liability regime for artificial intelligence." European Parliamentary Research Service, 2020.

12. Law Commission of England and Wales. "Automated Vehicles: Joint Report." Law Com No. 404, 2022.

13. National Highway Traffic Safety Administration. "Federal Automated Vehicles Policy." NHTSA, 2016 (and subsequent updates).

14. Price, W. Nicholson II. "Artificial Intelligence in Health Care: Applications and Legal Implications." SciTech Lawyer, Vol. 14, Issue 1, 2017.
