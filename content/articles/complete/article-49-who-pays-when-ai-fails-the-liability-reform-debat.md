---
title: Article 49: Who Pays When AI Fails? – The Liability Reform Debate
slug: article-49-who-pays-when-ai-fails-the-liability-reform-debat
path: responsibility
tldr: Explore key concepts and practical applications in AI governance.
contentSections:
  - Why AI Vendor Evaluation Matters
  - AI Vendor Evaluation Framework Overview
  - PHASE 1: PREPARATION
  - PHASE 2: INITIAL SCREENING
  - PHASE 3: DETAILED EVALUATION
  - PHASE 4: VALIDATION
  - PHASE 5: SELECTION AND CONTRACTING
  - PHASE 6: ONGOING MANAGEMENT
  - Quick Reference: Vendor Evaluation Checklist
relatedConcepts: []
crossPathRefs:
tags:
  - article
  - pays
  - artificial intelligence
  - ai risk management
  - who pays when
category: Legal Frameworks
image: article-49-who-pays-when-ai-fails-the-liability-reform-debat.jpg
imageAlt: Article 49: Who Pays When AI Fails? – The Liability Reform Debate
author: Sunil Iyer
publishDate: 2025-12-23
readingTime: 27
seoTitle: Article 49: Who Pays When AI Fails? – The Liability Reform D
seoDescription: Explore key concepts and practical applications in AI governance.
---



## Why AI Vendor Evaluation Matters


### The Stakes Are Higher with AI

| Traditional Software | AI/ML Systems |
|---------------------|---------------|
| Deterministic behavior | Probabilistic/uncertain outputs |
| Bugs are reproducible | Errors may be inconsistent |
| Clear logic to audit | "Black box" decision-making |
| Static after deployment | May drift or degrade over time |
| Standard security risks | Novel attack vectors (adversarial, poisoning) |
| Data as input | Data shapes behavior |
| Compliance is binary | Fairness is nuanced |


### Regulatory Requirements

```
REGULATORY DRIVERS FOR AI VENDOR DUE DILIGENCE

EU AI Act:
- Importers and distributors have compliance obligations
- Must verify provider has conducted conformity assessment
- Must ensure proper documentation available
- Subject to penalties for non-compliant AI

GDPR:
- Controllers responsible for processor compliance
- Must conduct due diligence on data processing
- Required contractual provisions (Article 28)
- Joint liability for violations

NYC Local Law 144:
- Employers responsible for bias audits
- Even when using third-party tools
- Must obtain audit from vendor or conduct own

Industry Regulations:
- Financial services: Model risk management (SR 11-7)
- Healthcare: FDA AI/ML guidance
- Insurance: State AI regulations
- Employment: EEOC guidance on AI
```


### Risks of Inadequate Vendor Evaluation

| Risk Category | Examples |
|---------------|----------|
| **Legal/Regulatory** | Non-compliance fines, enforcement actions, lawsuits |
| **Operational** | System failures, poor performance, integration issues |
| **Reputational** | Bias incidents, privacy breaches, public backlash |
| **Financial** | Unexpected costs, liability, remediation expenses |
| **Strategic** | Vendor lock-in, capability gaps, competitive disadvantage |
| **Ethical** | Harm to individuals, discrimination, trust erosion |

---


## AI Vendor Evaluation Framework Overview

```
AI VENDOR EVALUATION FRAMEWORK

┌─────────────────────────────────────────────────────────────┐
│  PHASE 1: PREPARATION                                       │
│  Define requirements, evaluation criteria, team             │
├─────────────────────────────────────────────────────────────┤
│  PHASE 2: INITIAL SCREENING                                 │
│  Market scan, shortlist candidates, preliminary review      │
├─────────────────────────────────────────────────────────────┤
│  PHASE 3: DETAILED EVALUATION                               │
│  Technical, governance, security, privacy, legal            │
├─────────────────────────────────────────────────────────────┤
│  PHASE 4: VALIDATION                                        │
│  Proof of concept, reference checks, testing                │
├─────────────────────────────────────────────────────────────┤
│  PHASE 5: SELECTION AND CONTRACTING                         │
│  Final selection, negotiation, contract execution           │
├─────────────────────────────────────────────────────────────┤
│  PHASE 6: ONGOING MANAGEMENT                                │
│  Monitoring, reviews, relationship management               │
└─────────────────────────────────────────────────────────────┘
```

---


## PHASE 1: PREPARATION


### Step 1: Define Your Requirements


#### 1.1 Business Requirements

```
BUSINESS REQUIREMENTS DEFINITION

Project Overview:
- Project name: _______________________
- Business sponsor: _______________________
- Target implementation date: _______________________
- Budget range: _______________________

Business Problem:
What problem are you trying to solve?
_____________________________________________________________
_____________________________________________________________

Why AI/ML:
Why is an AI solution appropriate (vs. alternatives)?
_____________________________________________________________
_____________________________________________________________

Success Criteria:
How will you measure success?

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
|        |        |                   |
|        |        |                   |
|        |        |                   |

Use Case Details:
- Primary use case: _______________________
- Secondary use cases: _______________________
- Users: _______________________
- Volume/scale: _______________________
- Integration requirements: _______________________

Constraints:
□ Budget: _______________________
□ Timeline: _______________________
□ Technical: _______________________
□ Regulatory: _______________________
□ Other: _______________________
```


#### 1.2 Technical Requirements

```
TECHNICAL REQUIREMENTS

Functional Requirements:
| Requirement | Priority | Notes |
|-------------|----------|-------|
| | Must have | |
| | Must have | |
| | Should have | |
| | Nice to have | |

AI/ML Specific:
- Model type needed: _______________________
- Input data types: _______________________
- Output format: _______________________
- Accuracy requirements: _______________________
- Latency requirements: _______________________
- Explainability requirements: _______________________

Integration Requirements:
- Systems to integrate: _______________________
- APIs required: _______________________
- Data formats: _______________________
- Authentication: _______________________

Infrastructure:
□ Cloud-hosted (vendor)
□ Cloud-hosted (your cloud)
□ On-premises
□ Hybrid
□ Edge deployment

Scalability:
- Current volume: _______________________
- Expected growth: _______________________
- Peak load requirements: _______________________

Availability:
- Uptime requirement: ________%
- Disaster recovery: _______________________
- Maintenance windows: _______________________
```


#### 1.3 Governance Requirements

```
GOVERNANCE REQUIREMENTS

Regulatory Compliance:
□ EU AI Act (specify risk level): _______________________
□ GDPR
□ CCPA/CPRA
□ HIPAA
□ Financial services regulations: _______________________
□ Industry-specific: _______________________
□ Other: _______________________

Internal Policy Compliance:
□ AI/ML Policy
□ Data Governance Policy
□ Information Security Policy
□ Vendor Management Policy
□ Ethics Policy
□ Other: _______________________

Risk Classification:
Based on your AI policy, this procurement is:
□ High risk
□ Medium risk
□ Low risk

Required Approvals:
| Approver | Role | Required For |
|----------|------|--------------|
|          |      |              |
|          |      |              |

Documentation Requirements:
□ Model card / documentation
□ Bias testing results
□ Security assessment
□ Privacy impact assessment
□ Technical specifications
□ Training data information
□ Other: _______________________
```

---


### Step 2: Establish Evaluation Criteria


#### 2.1 Evaluation Dimensions

```
EVALUATION DIMENSIONS AND WEIGHTING

Dimension weights should reflect your priorities.
Total must equal 100%.

| Dimension | Weight | Rationale |
|-----------|--------|-----------|
| Technical Capability | ___% | |
| AI Governance & Ethics | ___% | |
| Security | ___% | |
| Privacy & Data Protection | ___% | |
| Vendor Stability & Support | ___% | |
| Cost & Commercial Terms | ___% | |
| TOTAL | 100% | |

Suggested Weights by Risk Level:

High-Risk AI (employment, credit, healthcare):
- Technical: 20%
- Governance/Ethics: 25%
- Security: 15%
- Privacy: 20%
- Vendor Stability: 10%
- Cost: 10%

Medium-Risk AI (customer-facing, internal decisions):
- Technical: 25%
- Governance/Ethics: 20%
- Security: 15%
- Privacy: 15%
- Vendor Stability: 15%
- Cost: 10%

Lower-Risk AI (analytics, productivity):
- Technical: 30%
- Governance/Ethics: 15%
- Security: 15%
- Privacy: 10%
- Vendor Stability: 15%
- Cost: 15%
```


#### 2.2 Scoring Methodology

```
SCORING METHODOLOGY

Rating Scale:
| Score | Meaning | Criteria |
|-------|---------|----------|
| 5 | Excellent | Exceeds requirements, industry-leading |
| 4 | Good | Fully meets requirements |
| 3 | Acceptable | Meets minimum requirements |
| 2 | Below expectations | Partial gaps, needs improvement |
| 1 | Poor | Significant gaps |
| 0 | Unacceptable | Does not meet requirements / disqualifying |

Minimum Thresholds:
- Overall score: Must be ≥ ___
- Each dimension: Must be ≥ ___
- Critical items: Must score ≥ ___

Disqualifying Factors:
□ Cannot meet regulatory requirements
□ Unacceptable security posture
□ No bias testing / fairness assessment
□ Unwilling to provide required documentation
□ Financial instability
□ Adverse legal/regulatory history
□ Other: _______________________
```

---


### Step 3: Assemble Evaluation Team

```
EVALUATION TEAM

Core Team:

| Role | Name | Responsibilities |
|------|------|------------------|
| Evaluation Lead | | Overall coordination, final recommendation |
| Technical Evaluator | | Technical assessment, integration |
| Procurement Lead | | Commercial terms, negotiation |
| Legal/Compliance | | Contract review, regulatory |
| Security | | Security assessment |
| Privacy | | Data protection review |
| Business Owner | | Requirements, acceptance |

Extended Team (as needed):

| Role | Name | Involvement |
|------|------|-------------|
| AI/ML Expert | | Technical deep-dive |
| Ethics/Responsible AI | | Governance assessment |
| Data Engineering | | Data integration |
| Finance | | Cost analysis |
| End Users | | Usability feedback |

Decision Authority:
- Recommendation: Evaluation team
- Approval: _______________________
- Contract signing: _______________________

Conflict of Interest:
All team members must disclose any relationships with vendors.
□ Disclosures collected and documented
```

---


## PHASE 2: INITIAL SCREENING


### Step 4: Market Scan and Shortlisting


#### 4.1 Identify Potential Vendors

```
VENDOR IDENTIFICATION

Sources for Vendor Discovery:
□ Industry analyst reports (Gartner, Forrester, IDC)
□ Peer recommendations
□ Industry conferences/events
□ Online research
□ RFI responses
□ Existing vendor relationships
□ Startup databases (Crunchbase, etc.)
□ AI vendor directories

Initial Vendor List:
| Vendor | Product | Source | Initial Fit |
|--------|---------|--------|-------------|
|        |         |        | High/Med/Low |
|        |         |        |             |
|        |         |        |             |
|        |         |        |             |
|        |         |        |             |
```


#### 4.2 Preliminary Screening

```
PRELIMINARY SCREENING CHECKLIST

For each vendor, quickly assess:

BASIC FIT
□ Does the product address our use case?
□ Does the vendor serve our industry/region?
□ Is the solution within budget range?
□ Does the vendor's size/stage fit our needs?
□ Are there any obvious disqualifiers?

QUICK RESEARCH
□ Company website review
□ Product documentation available
□ Customer case studies
□ Recent news/press
□ Glassdoor/employee reviews
□ Any red flags (lawsuits, breaches, controversies)?

INITIAL OUTREACH
□ Request initial meeting/demo
□ Request basic documentation
□ Confirm ability to meet key requirements
□ Understand pricing model

SCREENING DECISION
□ Advance to detailed evaluation
□ Keep on watch list
□ Eliminate (reason: _______________)
```


#### 4.3 RFI/RFP Process (if applicable)

```
REQUEST FOR INFORMATION (RFI) / REQUEST FOR PROPOSAL (RFP)

When to Use:
- RFI: Early stage, understanding market capabilities
- RFP: Formal procurement, comparing specific proposals

AI-SPECIFIC RFI/RFP QUESTIONS

Include standard procurement questions PLUS:

Model and Algorithm:
1. Describe the AI/ML techniques used in your solution.
2. What type of model(s) power the solution?
3. How was the model trained? On what data?
4. How do you ensure model accuracy and reliability?
5. What is your approach to model explainability?

Bias and Fairness:
6. How do you test for bias in your AI systems?
7. Can you provide bias audit results or fairness metrics?
8. What demographic groups are tested?
9. How do you mitigate identified biases?
10. Do you comply with NYC Local Law 144 (if applicable)?

Data Practices:
11. What data do you require from us?
12. How is our data used? (Training, inference, other?)
13. Where is data stored and processed?
14. What is your data retention policy?
15. Can we delete our data upon contract termination?

Transparency and Documentation:
16. Do you provide model cards or similar documentation?
17. Can you explain individual predictions/decisions?
18. What documentation do you provide for compliance purposes?
19. How do you communicate model updates or changes?

Security:
20. What security certifications do you hold?
21. Describe your approach to AI-specific security threats.
22. How do you protect against adversarial attacks?
23. What is your incident response process?

Governance:
24. Do you have a responsible AI program? Describe it.
25. Do you have an AI ethics board or review process?
26. How do you ensure ongoing compliance with AI regulations?
27. What is your approach to human oversight?

Support and Transparency:
28. What ongoing support do you provide?
29. How do you notify customers of model changes?
30. Can we audit your AI systems?
31. What is your approach to AI incidents?
```

---


## PHASE 3: DETAILED EVALUATION


### Step 5: Technical Evaluation

```
TECHNICAL EVALUATION

═══════════════════════════════════════════════════════════════
5.1 FUNCTIONALITY AND PERFORMANCE
═══════════════════════════════════════════════════════════════

Functional Capability:
| Requirement | Met? | Score | Evidence/Notes |
|-------------|------|-------|----------------|
|             | Y/N/P | /5   |                |
|             |      |       |                |
|             |      |       |                |

Performance Metrics:
| Metric | Requirement | Vendor Claim | Validated? |
|--------|-------------|--------------|------------|
| Accuracy | | | |
| Precision | | | |
| Recall | | | |
| Latency | | | |
| Throughput | | | |
| Availability | | | |

Model Information:
- Model type/architecture: _______________________
- Model size/complexity: _______________________
- Training data description: _______________________
- Last training date: _______________________
- Update frequency: _______________________

Explainability:
□ Global explanations available
□ Local (individual) explanations available
□ Feature importance provided
□ Confidence scores provided
□ Explanation quality: _______________________

Technical Score: _____ / 5

═══════════════════════════════════════════════════════════════
5.2 INTEGRATION AND ARCHITECTURE
═══════════════════════════════════════════════════════════════

Integration Approach:
□ API-based
□ SDK/library
□ Embedded
□ Data pipeline
□ Other: _______________________

API Assessment:
□ RESTful API
□ Documentation quality: _______________________
□ Authentication methods: _______________________
□ Rate limits: _______________________
□ Versioning approach: _______________________
□ Backwards compatibility: _______________________

Data Integration:
□ Supported data formats: _______________________
□ Batch processing: _______________________
□ Real-time/streaming: _______________________
□ Data transformation requirements: _______________________

Infrastructure:
□ Deployment model: _______________________
□ Cloud providers supported: _______________________
□ On-prem option: _______________________
□ Containerization: _______________________
□ Scalability approach: _______________________

Integration Score: _____ / 5

═══════════════════════════════════════════════════════════════
5.3 RELIABILITY AND SUPPORT
═══════════════════════════════════════════════════════════════

Reliability:
- Uptime SLA: _______________________
- Historical uptime: _______________________
- Disaster recovery: _______________________
- Redundancy: _______________________
- Maintenance approach: _______________________

Monitoring:
□ Performance dashboards provided
□ Alerting capabilities
□ Usage analytics
□ Model monitoring (drift, degradation)
□ Audit logging

Support:
- Support tiers available: _______________________
- Response time SLAs: _______________________
- Support channels: _______________________
- Documentation quality: _______________________
- Training available: _______________________
- Customer success resources: _______________________

Reliability Score: _____ / 5

TECHNICAL DIMENSION TOTAL: _____ / 15 (Average: _____ / 5)
```

---


### Step 6: AI Governance and Ethics Evaluation

```
AI GOVERNANCE AND ETHICS EVALUATION

═══════════════════════════════════════════════════════════════
6.1 RESPONSIBLE AI PROGRAM
═══════════════════════════════════════════════════════════════

Governance Structure:
□ Has documented AI ethics principles
□ Has responsible AI team/function
□ Has AI ethics board or review process
□ Has AI governance policies
□ Executive accountability for AI ethics

Evidence:
- Published AI principles: □ Yes □ No
- Responsible AI report: □ Yes □ No
- Ethics board composition: _______________________
- Review process description: _______________________

Responsible AI Score: _____ / 5

═══════════════════════════════════════════════════════════════
6.2 FAIRNESS AND BIAS
═══════════════════════════════════════════════════════════════

Bias Testing:
□ Conducts bias testing
□ Tests across multiple demographic groups
□ Provides bias audit results to customers
□ Has bias mitigation processes
□ Conducts ongoing bias monitoring

Specific Information:
- Groups tested: _______________________
- Metrics used: _______________________
- Testing frequency: _______________________
- Most recent results: _______________________
- Mitigation approaches: _______________________

Regulatory Compliance:
□ NYC Local Law 144 compliant (if applicable)
□ EU AI Act compliant (if applicable)
□ Industry-specific requirements met

Fairness Score: _____ / 5

═══════════════════════════════════════════════════════════════
6.3 TRANSPARENCY AND DOCUMENTATION
═══════════════════════════════════════════════════════════════

Documentation Provided:
□ Model card or equivalent
□ Data documentation
□ Performance documentation
□ Limitations documentation
□ Intended use documentation
□ Change/update logs

Documentation Quality:
| Document | Available? | Quality (1-5) | Notes |
|----------|------------|---------------|-------|
| Model card | | | |
| Data sheet | | | |
| API docs | | | |
| User guide | | | |
| Compliance docs | | | |

Transparency:
□ Willing to explain model decisions
□ Provides confidence/uncertainty information
□ Discloses training data sources
□ Notifies of model changes
□ Transparent about limitations

Transparency Score: _____ / 5

═══════════════════════════════════════════════════════════════
6.4 HUMAN OVERSIGHT
═══════════════════════════════════════════════════════════════

Human Oversight Features:
□ Human-in-the-loop option available
□ Review queues for uncertain predictions
□ Override capabilities
□ Audit trail of decisions
□ Escalation mechanisms
□ Adjustable automation levels

Oversight Assessment:
- Default automation level: _______________________
- Customization available: _______________________
- Recommended oversight: _______________________

Human Oversight Score: _____ / 5

GOVERNANCE DIMENSION TOTAL: _____ / 20 (Average: _____ / 5)
```

---


### Step 7: Security Evaluation

```
SECURITY EVALUATION

═══════════════════════════════════════════════════════════════
7.1 SECURITY CERTIFICATIONS AND COMPLIANCE
═══════════════════════════════════════════════════════════════

Certifications:
| Certification | Held? | Expiration | Scope |
|---------------|-------|------------|-------|
| SOC 2 Type II | | | |
| ISO 27001 | | | |
| ISO 27701 | | | |
| FedRAMP | | | |
| HIPAA | | | |
| PCI DSS | | | |
| Other: | | | |

Certification Score: _____ / 5

═══════════════════════════════════════════════════════════════
7.2 INFRASTRUCTURE SECURITY
═══════════════════════════════════════════════════════════════

Infrastructure:
□ Cloud provider(s): _______________________
□ Data center locations: _______________________
□ Network security (firewalls, segmentation)
□ Encryption at rest
□ Encryption in transit
□ Key management approach
□ Physical security

Access Controls:
□ Multi-factor authentication
□ Role-based access control
□ Single sign-on support
□ Privileged access management
□ Access logging and monitoring

Infrastructure Score: _____ / 5

═══════════════════════════════════════════════════════════════
7.3 AI-SPECIFIC SECURITY
═══════════════════════════════════════════════════════════════

AI Security Measures:
□ Adversarial attack protection
□ Data poisoning prevention
□ Model extraction protection
□ Prompt injection prevention (if generative)
□ Input validation
□ Output filtering
□ Model integrity verification

AI Security Practices:
- Adversarial testing conducted: □ Yes □ No
- Red teaming: □ Yes □ No
- AI security assessments: □ Yes □ No
- Security update process: _______________________

AI Security Score: _____ / 5

═══════════════════════════════════════════════════════════════
7.4 INCIDENT RESPONSE
═══════════════════════════════════════════════════════════════

Security Incident Response:
□ Documented incident response plan
□ 24/7 security monitoring
□ Incident notification commitments
□ Customer notification timeline: _______
□ Breach history: _______________________

Security Assessment:
□ Penetration testing (frequency: _______)
□ Vulnerability scanning
□ Bug bounty program
□ Third-party security audits

Incident Response Score: _____ / 5

SECURITY DIMENSION TOTAL: _____ / 20 (Average: _____ / 5)
```

---


### Step 8: Privacy and Data Protection Evaluation

```
PRIVACY AND DATA PROTECTION EVALUATION

═══════════════════════════════════════════════════════════════
8.1 DATA HANDLING PRACTICES
═══════════════════════════════════════════════════════════════

Data Collection:
- What data is required: _______________________
- What data is optional: _______________________
- Minimum data needed: _______________________

Data Use:
□ Data used only for contracted services
□ Data NOT used for model training (or opt-out available)
□ Data NOT shared with third parties
□ Data use limitations documented
□ Secondary use restrictions

Data Storage:
- Storage locations: _______________________
- Data residency options: _______________________
- Multi-tenancy approach: _______________________
- Data isolation: _______________________

Data Retention:
- Retention period: _______________________
- Deletion upon termination: □ Yes □ No
- Deletion verification: □ Yes □ No
- Data portability: □ Yes □ No

Data Handling Score: _____ / 5

═══════════════════════════════════════════════════════════════
8.2 PRIVACY COMPLIANCE
═══════════════════════════════════════════════════════════════

Regulatory Compliance:
| Regulation | Compliant? | Evidence |
|------------|------------|----------|
| GDPR | | |
| CCPA/CPRA | | |
| HIPAA | | |
| Other: | | |

Privacy Documentation:
□ Privacy policy reviewed
□ Data processing agreement available
□ Sub-processor list provided
□ Privacy impact assessment available
□ Records of processing activities

Data Subject Rights:
□ Supports access requests
□ Supports deletion requests
□ Supports portability
□ Supports objection/opt-out
□ Process documented

Privacy Compliance Score: _____ / 5

═══════════════════════════════════════════════════════════════
8.3 AI-SPECIFIC PRIVACY
═══════════════════════════════════════════════════════════════

Training Data:
- Is customer data used for training? □ Yes □ No □ Opt-out
- Can this be restricted contractually? □ Yes □ No
- Training data sources disclosed: □ Yes □ No

Inference Privacy:
□ Input data minimization
□ Output data minimization
□ No persistent storage of queries (or limited)
□ Anonymization/pseudonymization options

Privacy-Enhancing Technologies:
□ Differential privacy
□ Federated learning
□ Homomorphic encryption
□ Secure multi-party computation
□ On-device processing option

AI Privacy Score: _____ / 5

PRIVACY DIMENSION TOTAL: _____ / 15 (Average: _____ / 5)
```

---


### Step 9: Vendor Stability and Support Evaluation

```
VENDOR STABILITY AND SUPPORT EVALUATION

═══════════════════════════════════════════════════════════════
9.1 COMPANY STABILITY
═══════════════════════════════════════════════════════════════

Company Information:
- Founded: _______________________
- Headquarters: _______________________
- Employees: _______________________
- Funding/ownership: _______________________
- Revenue (if available): _______________________

Financial Health:
□ Profitable / clear path to profitability
□ Well-funded (runway > 18 months)
□ Stable funding sources
□ No concerning financial signals
□ Financial statements reviewed (if available)

Market Position:
- Market share/position: _______________________
- Key competitors: _______________________
- Analyst coverage: _______________________
- Industry recognition: _______________________

Red Flags:
□ Recent layoffs
□ Leadership turnover
□ Funding difficulties
□ Customer losses
□ Negative press
□ Legal/regulatory issues

Stability Score: _____ / 5

═══════════════════════════════════════════════════════════════
9.2 CUSTOMER BASE AND REFERENCES
═══════════════════════════════════════════════════════════════

Customer Base:
- Total customers: _______________________
- Customers in our industry: _______________________
- Customers of similar size: _______________________
- Customer logos provided: _______________________

References:
| Company | Industry | Use Case | Contact | Spoke? |
|---------|----------|----------|---------|--------|
|         |          |          |         |        |
|         |          |          |         |        |
|         |          |          |         |        |

Reference Questions:
□ Implementation experience
□ Ongoing support quality
□ Performance vs. expectations
□ Any issues encountered
□ Would they recommend?

Case Studies:
□ Relevant case studies reviewed
□ Results validated with references

References Score: _____ / 5

═══════════════════════════════════════════════════════════════
9.3 SUPPORT AND PARTNERSHIP
═══════════════════════════════════════════════════════════════

Support Model:
- Support tiers offered: _______________________
- Recommended tier: _______________________
- Support hours: _______________________
- Response time SLAs: _______________________
- Escalation process: _______________________

Support Channels:
□ Email
□ Phone
□ Chat
□ Portal/ticketing
□ Dedicated support contact
□ On-site support available

Customer Success:
□ Dedicated CSM
□ Implementation support
□ Training provided
□ Documentation/knowledge base
□ User community
□ Regular business reviews

Support Score: _____ / 5

═══════════════════════════════════════════════════════════════
9.4 PRODUCT ROADMAP AND INNOVATION
═══════════════════════════════════════════════════════════════

Product Direction:
- Roadmap shared: □ Yes □ No
- Alignment with our needs: _______________________
- Update/release frequency: _______________________
- Customer input into roadmap: _______________________

Innovation:
□ Continued R&D investment
□ Patent portfolio / publications
□ Industry thought leadership
□ New feature velocity

Long-term Viability:
□ Sustainable business model
□ Diversified customer base
□ Competitive differentiation
□ Acquirable (exit risk vs. opportunity)

Roadmap Score: _____ / 5

VENDOR DIMENSION TOTAL: _____ / 20 (Average: _____ / 5)
```

---


### Step 10: Commercial Evaluation

```
COMMERCIAL EVALUATION

═══════════════════════════════════════════════════════════════
10.1 PRICING MODEL
═══════════════════════════════════════════════════════════════

Pricing Structure:
□ Subscription (per seat)
□ Subscription (per organization)
□ Usage-based (per API call)
□ Usage-based (per prediction)
□ Usage-based (per data volume)
□ Tiered pricing
□ Custom/enterprise pricing
□ Other: _______________________

Pricing Details:
| Component | Unit | Price | Notes |
|-----------|------|-------|-------|
|           |      |       |       |
|           |      |       |       |
|           |      |       |       |

Total Cost Estimate:
| Period | Volume | Cost | Notes |
|--------|--------|------|-------|
| Year 1 |        |      | Including implementation |
| Year 2 |        |      |       |
| Year 3 |        |      |       |
| 3-Year Total |  |      |       |

Cost Predictability:
□ Costs are predictable
□ Usage-based costs have caps/limits
□ Price protection / escalation caps
□ Volume discounts available

Pricing Score: _____ / 5

═══════════════════════════════════════════════════════════════
10.2 CONTRACT TERMS
═══════════════════════════════════════════════════════════════

Standard Terms Review:
| Term | Vendor Position | Our Requirement | Gap? |
|------|-----------------|-----------------|------|
| Contract length | | | |
| Termination rights | | | |
| Data ownership | | | |
| Liability caps | | | |
| Indemnification | | | |
| SLA penalties | | | |
| IP ownership | | | |
| Insurance | | | |

Contract Flexibility:
□ Willing to negotiate key terms
□ Accepts customer paper (or hybrid)
□ Reasonable data protection terms
□ Adequate termination rights
□ Appropriate liability allocation

Contract Score: _____ / 5

═══════════════════════════════════════════════════════════════
10.3 TOTAL COST OF OWNERSHIP
═══════════════════════════════════════════════════════════════

One-Time Costs:
| Item | Cost |
|------|------|
| License/subscription (initial) | |
| Implementation services | |
| Training | |
| Integration development | |
| Data migration | |
| Other: | |
| TOTAL ONE-TIME | |

Ongoing Annual Costs:
| Item | Cost |
|------|------|
| Subscription/license | |
| Support | |
| Usage/consumption | |
| Internal administration | |
| Ongoing integration maintenance | |
| Other: | |
| TOTAL ANNUAL | |

Hidden Costs to Consider:
□ Overage charges
□ Required add-ons
□ Training for new users
□ Integration updates
□ Compliance maintenance
□ Exit/migration costs

TCO Score: _____ / 5

COMMERCIAL DIMENSION TOTAL: _____ / 15 (Average: _____ / 5)
```

---


## PHASE 4: VALIDATION


### Step 11: Proof of Concept

```
PROOF OF CONCEPT (POC)

POC Planning:

Objectives:
□ Validate technical capabilities
□ Test integration feasibility
□ Assess performance with our data
□ Evaluate user experience
□ Confirm bias/fairness for our population
□ Other: _______________________

Scope:
- Duration: _______________________
- Use case(s) to test: _______________________
- Data to use: _______________________
- Users involved: _______________________
- Success criteria: _______________________

POC Agreement:
□ POC terms agreed
□ Data protection terms confirmed
□ Cost (if any): _______________________
□ Support during POC: _______________________

POC Execution:

| Test | Objective | Result | Pass/Fail |
|------|-----------|--------|-----------|
|      |           |        |           |
|      |           |        |           |
|      |           |        |           |

Performance Validation:
| Metric | Target | Actual | Assessment |
|--------|--------|--------|------------|
|        |        |        |            |
|        |        |        |            |

Bias Testing (with our data):
| Group | Metric | Result | Acceptable? |
|-------|--------|--------|-------------|
|       |        |        |             |
|       |        |        |             |

User Feedback:
| User | Role | Feedback | Rating |
|------|------|----------|--------|
|      |      |          |        |
|      |      |          |        |

POC Conclusion:
□ Successful - proceed to selection
□ Partially successful - issues to address
□ Unsuccessful - do not proceed
```


### Step 12: Reference Checks

```
REFERENCE CHECK TEMPLATE

Reference: _______________________
Company: _______________________
Role: _______________________
Date: _______________________
Interviewer: _______________________

Questions:

1. How long have you been using [Vendor/Product]?
   _____________________________________________________________

2. What use case(s) do you use it for?
   _____________________________________________________________

3. How was the implementation experience?
   _____________________________________________________________

4. How would you rate the product performance?
   □ Excellent  □ Good  □ Acceptable  □ Below expectations  □ Poor

5. How would you rate the vendor's support?
   □ Excellent  □ Good  □ Acceptable  □ Below expectations  □ Poor

6. Have you experienced any issues? How were they handled?
   _____________________________________________________________

7. How does the vendor handle updates and changes?
   _____________________________________________________________

8. Have you assessed the AI for bias/fairness? What were results?
   _____________________________________________________________

9. What do you wish you had known before purchasing?
   _____________________________________________________________

10. Would you recommend this vendor? Would you buy again?
    _____________________________________________________________

Overall Reference Assessment:
□ Strongly positive
□ Positive
□ Mixed
□ Negative
□ Concerning
```

---


## PHASE 5: SELECTION AND CONTRACTING


### Step 13: Final Selection

```
VENDOR COMPARISON MATRIX

| Dimension | Weight | Vendor A | Vendor B | Vendor C |
|-----------|--------|----------|----------|----------|
| Technical | ___% | /5 = | /5 = | /5 = |
| Governance/Ethics | ___% | /5 = | /5 = | /5 = |
| Security | ___% | /5 = | /5 = | /5 = |
| Privacy | ___% | /5 = | /5 = | /5 = |
| Vendor Stability | ___% | /5 = | /5 = | /5 = |
| Commercial | ___% | /5 = | /5 = | /5 = |
| WEIGHTED TOTAL | 100% | | | |

Qualitative Factors:
| Factor | Vendor A | Vendor B | Vendor C |
|--------|----------|----------|----------|
| Cultural fit | | | |
| Strategic alignment | | | |
| Innovation | | | |
| Reference quality | | | |
| POC results | | | |

Risk Assessment:
| Risk | Vendor A | Vendor B | Vendor C |
|------|----------|----------|----------|
| Vendor viability | | | |
| Lock-in risk | | | |
| Compliance risk | | | |
| Implementation risk | | | |

RECOMMENDATION:

Recommended Vendor: _______________________

Rationale:
_____________________________________________________________
_____________________________________________________________
_____________________________________________________________

Risks and Mitigations:
_____________________________________________________________
_____________________________________________________________

Approval:
| Approver | Decision | Date |
|----------|----------|------|
|          |          |      |
|          |          |      |
```


### Step 14: Contract Negotiation

```
AI VENDOR CONTRACT CHECKLIST

Essential AI-Specific Contract Terms:

DATA AND IP:
□ Customer owns its data
□ Customer data not used for training without consent
□ Clear IP ownership for outputs
□ Data return/deletion upon termination
□ Data processing agreement included

TRANSPARENCY AND DOCUMENTATION:
□ Model documentation provided
□ Notification of material model changes
□ Bias testing results available
□ Audit rights included
□ Cooperation with regulatory inquiries

PERFORMANCE AND SLAs:
□ Performance SLAs defined
□ Accuracy guarantees (if applicable)
□ Availability SLAs
□ Remedies for SLA breaches

COMPLIANCE:
□ Regulatory compliance representations
□ Compliance with AI-specific laws
□ Bias audit cooperation
□ Indemnification for AI-specific claims

LIABILITY AND INSURANCE:
□ Appropriate liability caps
□ AI-specific indemnification
□ Insurance requirements met
□ Limitation carve-outs appropriate

SECURITY:
□ Security requirements documented
□ Incident notification obligations
□ Breach liability addressed
□ Ongoing security certifications

CHANGE MANAGEMENT:
□ Change notification process
□ Approval rights for material changes
□ Version compatibility commitments
□ Deprecation notice period

EXIT:
□ Termination rights adequate
□ Transition assistance
□ Data portability
□ No unreasonable termination penalties

Contract Review Sign-off:
□ Legal review complete
□ Security review complete
□ Privacy review complete
□ Procurement review complete
□ Business owner approval
```

---


## PHASE 6: ONGOING MANAGEMENT


### Step 15: Ongoing Vendor Management

```
ONGOING AI VENDOR MANAGEMENT

═══════════════════════════════════════════════════════════════
PERFORMANCE MONITORING
═══════════════════════════════════════════════════════════════

Regular Monitoring:
| Metric | Frequency | Target | Owner |
|--------|-----------|--------|-------|
| Model accuracy | | | |
| System availability | | | |
| Response time | | | |
| Fairness metrics | | | |
| Support response | | | |
| User satisfaction | | | |

Monitoring Dashboard:
□ Vendor-provided monitoring
□ Internal monitoring
□ Third-party monitoring
□ Combined approach

Issue Tracking:
□ Issue logging process established
□ Escalation path defined
□ SLA tracking in place
□ Regular issue review

═══════════════════════════════════════════════════════════════
PERIODIC REVIEWS
═══════════════════════════════════════════════════════════════

Quarterly Business Review:
□ Performance against SLAs
□ Issue summary and resolution
□ Usage and cost review
□ Upcoming changes/roadmap
□ Support quality assessment
□ Relationship health

Annual Assessment:
□ Full performance evaluation
□ Bias/fairness reassessment
□ Security posture review
□ Privacy compliance check
□ Contract terms review
□ Market alternatives review
□ Continue/renew decision

Annual Assessment Criteria:
| Criterion | Rating | Notes |
|-----------|--------|-------|
| Performance | /5 | |
| Support | /5 | |
| Compliance | /5 | |
| Value | /5 | |
| Relationship | /5 | |
| OVERALL | /5 | |

═══════════════════════════════════════════════════════════════
CHANGE MANAGEMENT
═══════════════════════════════════════════════════════════════

Vendor Change Notification:
□ Process for receiving vendor updates
□ Assessment process for changes
□ Testing requirements for updates
□ Approval process for changes
□ Rollback capabilities

Types of Changes to Track:
□ Model updates/retraining
□ Algorithm changes
□ Data source changes
□ Feature changes
□ Security updates
□ Compliance changes
□ Pricing changes

═══════════════════════════════════════════════════════════════
COMPLIANCE MAINTENANCE
═══════════════════════════════════════════════════════════════

Ongoing Compliance:
□ Annual bias audit (if required)
□ Privacy compliance verification
□ Security certification renewal
□ Regulatory change monitoring
□ Policy alignment check

Documentation Maintenance:
□ Model card updates obtained
□ Risk assessment updated
□ Vendor file maintained
□ Contract amendments tracked

═══════════════════════════════════════════════════════════════
EXIT PLANNING
═══════════════════════════════════════════════════════════════

Exit Readiness:
□ Alternative vendors identified
□ Data portability tested
□ Migration plan documented
□ Contractual exit rights understood
□ Transition timeline estimated

Exit Triggers:
□ Material SLA breaches
□ Security incident
□ Compliance failure
□ Financial instability
□ Strategic change
□ Better alternative available
```

---


## Quick Reference: Vendor Evaluation Checklist

```
AI VENDOR EVALUATION QUICK CHECKLIST

MUST HAVE (Disqualifying if not met):
□ Can demonstrate product meets functional requirements
□ Provides model documentation
□ Has conducted bias testing
□ Meets security certification requirements
□ Complies with applicable regulations
□ Acceptable data handling practices
□ Financially stable / viable
□ Acceptable contract terms

SHOULD HAVE (Strong preference):
□ Responsible AI program
□ Regular bias audits
□ Explainability features
□ Human oversight options
□ Strong customer references
□ Adequate support model
□ Clear product roadmap
□ Competitive pricing

NICE TO HAVE (Differentiators):
□ Industry-specific experience
□ Advanced fairness features
□ Privacy-enhancing technologies
□ Thought leadership
□ Partnership approach
□ Innovation track record

RED FLAGS (Investigate further):
□ Unwilling to provide documentation
□ No bias testing
□ Vague about data practices
□ Lacks security certifications
□ Poor references
□ Financial concerns
□ Regulatory issues
□ High employee turnover
□ Recent negative press
```

---


## Summary

Evaluating AI vendors requires going beyond traditional software procurement to assess:

1. **Technical Capability** — Not just features, but model quality, explainability, and reliability
2. **AI Governance** — Responsible AI practices, bias testing, transparency
3. **Security** — Traditional plus AI-specific threats
4. **Privacy** — Data handling, especially training data use
5. **Vendor Stability** — Financial health, customer base, support
6. **Commercial Terms** — Pricing, contract terms, total cost

Key differences from traditional vendor evaluation:
- Greater emphasis on transparency and documentation
- Specific attention to bias and fairness
- AI-specific security considerations
- Training data practices scrutiny
- Regulatory compliance for AI-specific laws
- Ongoing monitoring requirements

Take the time to conduct thorough due diligence — the risks of getting it wrong with AI are significant, but so are the rewards of finding the right partner.

---

**Word Count:** Approximately 6,500 words
**Estimated Reading Time:** 30-35 minutes
**Checklists Included:** 15+
**Templates Included:** 20+

---

*End of Guide*
