---
title: 'Article 134: AI and Elections – Protecting Democratic Processes'
tldr: ''
category: Industry-Specific Insights
learning_objectives:
- Understand the key concepts and principles of regulatory requirements
- Implement incident response protocols in real-world scenarios
- Evaluate transparency measures for organizational compliance
seo_keywords:
- article
- elections
- AI governance
- deepfakes
- synthetic
components:
- type: image_prompt
  label: Article Hero Image
  section: Header
  id: image-prompt-hero
  prompt: industry-specific icons, sector applications, business context, professional
    illustration, modern flat design style, clean and authoritative, high quality,
    blue and gray color scheme with accent colors, suitable for professional article
    header
- type: table
  label: Country vs Approach Table
  section: Other Jurisdictions
  id: table-other-jurisdictions
- type: flowchart
  label: Conclusion Process
  section: Conclusion
  id: flowchart-conclusion
- type: flowchart
  label: Sources and Further Reading Process
  section: Sources and Further Reading
  id: flowchart-sources-and-further-reading
- type: template
  label: 'Threat 2: Scaled Disinformation'
  section: 'Threat 2: Scaled Disinformation'
  id: template-threat-2-scaled-disinformation
  template_link: /templates/threat-2-scaled-disinformation.md
- type: template
  label: Information Monitoring
  section: Information Monitoring
  id: template-information-monitoring
  template_link: /templates/information-monitoring.md
- type: template
  label: Platform Responses
  section: Platform Responses
  id: template-platform-responses
  template_link: /templates/platform-responses.md
- type: template
  label: Major Platform Policies
  section: Major Platform Policies
  id: template-major-platform-policies
  template_link: /templates/major-platform-policies.md
- type: template
  label: Platform Limitations
  section: Platform Limitations
  id: template-platform-limitations
  template_link: /templates/platform-limitations.md
- type: template
  label: 'Pillar 2: Platform Responsibility'
  section: 'Pillar 2: Platform Responsibility'
  id: template-pillar-2-platform-responsibility
  template_link: /templates/pillar-2-platform-responsibility.md
topic_fingerprint:
- transparency
- accountability
- audit
named_examples:
- deepfakes
- defense
- digital services act
- eu ai act
- european parliament
- facebook
- fcc
- google
- meta
- mit
- stanford
- tiktok
- twitter
word_count: 2068
processed_date: '2025-12-18T20:05:29.166Z'
---


## How AI Threatens Elections


### Threat 1: Deepfakes and Synthetic Media

**What it is:** AI-generated audio, video, or images of candidates or officials saying or doing things they never said or did.

**Examples:**
- Fake video of a candidate making offensive statements
- Synthetic audio of an official announcing policy changes
- Fabricated images of candidates in compromising situations
- Fake endorsements from celebrities or officials

**Why it's dangerous:**
- Realistic enough to fool many viewers
- Can spread rapidly on social media
- Difficult to debunk once viral
- Can be timed for maximum impact (just before election)
- Erodes trust even in authentic content

**Real incidents:**
- Biden New Hampshire robocalls (2024)
- Fake audio of UK Labour leader leaked before election (2024)
- Synthetic videos of political figures in various countries
- AI-generated "endorsements" from celebrities

<!-- component:template:template-threat-2-scaled-disinformation -->

### Threat 2: Scaled Disinformation

**What it is:** Using AI to generate and spread false information about candidates, policies, or the electoral process itself.

**Capabilities:**
- Generate thousands of unique disinformation articles
- Create fake social media accounts with synthetic personas
- Produce fake local news content
- Automatically respond to and amplify narratives

**Why it's different from before:**
- Scale: AI can produce vastly more content than human operations
- Personalization: Content can be tailored to different audiences
- Speed: Rapid response to events
- Cost: Much cheaper than human-operated campaigns


### Threat 3: Microtargeting and Manipulation

**What it is:** Using AI to identify and target voters with personalized persuasion.

**Techniques:**
- Analyzing voter data to identify persuadable individuals
- Generating customized messages for different segments
- Predicting psychological vulnerabilities
- Timing interventions for maximum impact

**Concerns:**
- Voters may not realize they're being targeted
- Different voters receive different (potentially contradictory) messages
- Exploits emotional vulnerabilities
- Difficult to monitor or counter


### Threat 4: Undermining Trust in Authentic Content

**The "liar's dividend":** When anything could be fake, people may dismiss real content as fabricated.

**How this manifests:**
- Candidates deny real statements by claiming "deepfake"
- Voters become cynical about all political content
- Legitimate journalism dismissed as AI-generated
- Difficult to establish shared facts

**This cuts both ways:**
- Real misdeeds can be denied as fakes
- Real evidence loses credibility
- Democracy requires some shared basis of fact


### Threat 5: Attacks on Election Infrastructure

**AI-enabled cyber threats:**
- AI-assisted attacks on voter registration systems
- Automated vulnerability discovery in election systems
- AI-generated phishing targeting election officials
- Synthetic voice attacks on poll workers

---


## AI as a Defensive Tool


### Detection and Verification

**Deepfake detection:**
- AI systems trained to identify synthetic content
- Analyzing inconsistencies humans might miss
- Real-time screening of content

**Limitations:**
- Detection accuracy is imperfect
- Generators improve faster than detectors
- May not catch sophisticated fakes

<!-- component:template:template-information-monitoring -->

### Information Monitoring

**AI-powered monitoring:**
- Tracking disinformation narratives across platforms
- Identifying coordinated inauthentic behavior
- Early warning of emerging false claims
- Volume and spread analysis

**Organizations doing this:**
- Media organizations
- Academic researchers
- Government agencies
- Platform trust and safety teams


### Rapid Response

**AI-assisted fact-checking:**
- Automating initial claim verification
- Identifying sources and context
- Scaling fact-checking operations
- Real-time prebunking


### Voter Education

**AI for outreach:**
- Personalized voter education content
- Chatbots answering election questions
- Accessibility tools for voter information
- Countering misinformation with accurate information

---


## Regulatory Responses


### United States

**Federal level:**
- FCC: Banned AI-generated robocalls (February 2024)
- FEC: Considering rules on AI in campaign ads
- No comprehensive federal AI election law yet

**State level:**
- California: Disclosure requirements for AI in political ads
- Texas: Criminal penalties for deepfake election interference
- Minnesota: Disclosure requirements for synthetic media
- Many other states with pending or enacted legislation

**Gaps:**
- Patchwork of state laws
- Enforcement challenges
- First Amendment considerations
- Rapidly evolving technology


### European Union

**EU AI Act provisions:**
- Transparency requirements for AI-generated content
- Specific provisions for synthetic media
- Applies to political advertising

**Digital Services Act:**
- Platform obligations for election integrity
- Disinformation codes of practice
- Transparency in political advertising

**National laws:**
- Various EU countries with additional requirements
- France: Restrictions on AI-generated campaign content
- Germany: Platform accountability measures


### Other Jurisdictions

| Country | Approach |
<!-- component:table:table-other-jurisdictions -->
|---------|----------|
| India | Guidelines on AI and elections; platform cooperation |
| Brazil | Electoral court rules on synthetic content |
| Canada | Guidelines; proposed legislation |
| Australia | Electoral authority guidance; proposed rules |
| South Korea | Restrictions on AI-generated campaign content |


### The Regulatory Challenge

**Timing problem:** Technology moves faster than legislation.

**Jurisdiction problem:** Content created in one country affects elections in another.

**Enforcement problem:** Difficult to identify and prosecute bad actors.

**Free speech problem:** Balancing restrictions with political expression.

---

<!-- component:template:template-platform-responses -->

## Platform Responses

<!-- component:template:template-major-platform-policies -->

### Major Platform Policies

**Meta (Facebook/Instagram):**
- Labels for AI-generated content
- Removes manipulated media likely to mislead
- Disclosure requirements for political ads
- War rooms for election monitoring

**Google/YouTube:**
- Disclosure requirements for synthetic content in ads
- Removal policies for manipulated content
- Prebunks and information panels
- Collaboration with election authorities

**TikTok:**
- Labels for AI-generated content
- Policies against synthetic media of public figures
- Election integrity operations center
- Fact-checking partnerships

**X (Twitter):**
- Community notes for context
- Policies against manipulated media
- Enforcement has been inconsistent post-acquisition

<!-- component:template:template-platform-limitations -->

### Platform Limitations

**Scale:** Billions of pieces of content; can't review everything.

**Detection:** Perfect detection isn't possible.

**Speed:** Content spreads before it can be reviewed.

**Consistency:** Policies applied inconsistently.

**Business incentives:** Engagement vs. accuracy tensions.

---


## Case Studies


### Case Study 1: New Hampshire Biden Robocalls (2024)

**What happened:**
- AI-generated voice clone of President Biden
- Robocalls to New Hampshire voters before primary
- Message discouraged voting in primary
- Estimated 5,000-25,000 calls made

**Response:**
- FCC investigation launched
- Voice cloning company identified
- Political consultant charged
- FCC banned AI robocalls

**Lessons:**
- Relatively simple attack with significant impact
- Detection and attribution possible
- Existing laws (robocall rules) could apply
- Swift regulatory response possible


### Case Study 2: Slovakia Election Audio (2023)

**What happened:**
- Fake audio clip of candidate discussing election rigging
- Released days before election
- Spread during media blackout period
- Unclear impact on results

**Response:**
- Fact-checkers identified as fake
- But spread during period when corrections difficult
- Election proceeded; debated impact

**Lessons:**
- Timing of release can maximize impact
- Media blackout periods create vulnerability
- Even debunked content can have effect


### Case Study 3: Various 2024 Election Deepfakes

**Incidents:**
- Fake Biden videos in various countries
- AI-generated campaign ads (disclosed and undisclosed)
- Synthetic content targeting down-ballot races
- AI-generated "endorsements"

**Pattern:**
- Increasing sophistication
- Mixed detection success
- Rapid platform response when detected
- Growing public awareness

---


## Defending Democracy: A Framework


### Pillar 1: Legal and Regulatory

**Key elements:**
- Clear laws against election-related synthetic media fraud
- Disclosure requirements for AI in political advertising
- Platform accountability measures
- Enforcement mechanisms and penalties

**Challenges:**
- Balancing free speech
- Keeping pace with technology
- International coordination
- Enforcement capacity

<!-- component:template:template-pillar-2-platform-responsibility -->

### Pillar 2: Platform Responsibility

**Key elements:**
- Policies against election manipulation
- Detection and labeling of synthetic content
- Rapid response capabilities
- Transparency in enforcement

**What platforms should do:**
- Invest in detection technology
- Clear and consistent enforcement
- Collaboration with authorities and researchers
- Preparation for rapid response


### Pillar 3: Technical Defenses

**Key elements:**
- Detection tools for synthetic content
- Content provenance standards (C2PA)
- Watermarking of AI-generated content
- Authentication infrastructure

**Current state:**
- Tools exist but imperfect
- Standards emerging but not universal
- Adoption growing but incomplete


### Pillar 4: Media Literacy

**Key elements:**
- Public education on AI-generated content
- Critical thinking about digital media
- Understanding of how manipulation works
- Tools and techniques for verification

**Approaches:**
- School curricula updates
- Public awareness campaigns
- Media organization initiatives
- Civil society efforts


### Pillar 5: Election System Resilience

**Key elements:**
- Security of election infrastructure
- Paper trails and audit capabilities
- Election official training
- Incident response plans

**Why it matters:**
- AI can enable new attack vectors
- Resilient systems can withstand manipulation
- Recovery possible if fundamentals secure

---


## What Different Actors Should Do


### For Election Officials

**Preparation:**
- Assess AI-related threats
- Train staff on deepfakes and synthetic content
- Establish verification protocols
- Develop communication plans for incidents

**Operations:**
- Monitor for emerging disinformation
- Coordinate with platforms and law enforcement
- Rapid response to false claims about elections
- Clear communication with voters


### For Campaigns and Parties

**Responsible use:**
- Clear policies on AI use in campaigning
- Disclosure of AI-generated content
- Avoid techniques that undermine democratic norms

**Defense:**
- Monitor for deepfakes of candidates
- Rapid response capabilities
- Verification infrastructure
- Supporter education


### For Media Organizations

**Verification:**
- Enhanced verification protocols
- Deepfake detection tools
- Skepticism of too-good-to-be-true content
- Clear labeling of AI-related stories

**Coverage:**
- Responsible reporting on AI and elections
- Avoid amplifying disinformation
- Context about AI capabilities and limitations
- Prebunking known threats


### For Technology Companies

**AI developers:**
- Watermarking of outputs
- Policies against election interference use
- Cooperation with researchers
- Responsible disclosure

**Platforms:**
- Investment in detection
- Clear enforcement of policies
- Transparency in actions
- Preparation for election periods


### For Voters

**Digital literacy:**
- Skepticism about shocking content
- Verification before sharing
- Understanding of AI capabilities
- Attention to authoritative sources

**Practical steps:**
- Check sources before believing/sharing
- Look for verification from multiple outlets
- Be aware of emotional manipulation
- Report suspected synthetic content

---


## The 2024 Global Election Stress Test


### What We Learned

**AI election interference is real:** No longer theoretical; documented incidents in multiple countries.

**Detection is possible but imperfect:** Many fakes identified, but not all, and not always quickly.

**Rapid response matters:** Speed of response affects impact.

**Regulation is catching up:** New rules emerging but gaps remain.

**Public awareness growing:** More skepticism about digital content.


### What We Still Don't Know

**Full extent of operations:** How much AI-generated content affected elections that we didn't detect?

**Impact on outcomes:** Did AI interference change any election results?

**State actor involvement:** What are nation-states doing?

**Future evolution:** How will these techniques develop?

---


## Conclusion

AI represents both the greatest threat and potentially greatest defense for democratic elections in the digital age. The technology to generate convincing fake content exists and is being used. The defenses—technical, legal, and social—are developing but incomplete.

Key takeaways:

<!-- component:flowchart:flowchart-conclusion -->
1. **The threat is real and present:** AI-generated election interference is happening now, not in some future scenario

2. **Multiple defense layers needed:** No single solution; requires legal, technical, platform, and educational responses

3. **Speed is critical:** Disinformation spreads fast; response must be faster

4. **Trust is the ultimate target:** The goal is often to undermine faith in democratic processes, not just specific elections

5. **Everyone has a role:** Election officials, platforms, media, campaigns, and voters all contribute to defense

The health of democracy depends on shared access to facts, ability to verify information, and trust in electoral processes. AI threatens all three. Protecting elections requires understanding these threats and building robust defenses.

The 2024 elections were a stress test. We learned a lot. The challenge now is applying those lessons before the next cycle.

---


## Sources and Further Reading

1. **FCC AI Robocall Ban:** Federal Communications Commission. (2024). Declaratory Ruling on AI-generated calls.

2. **New Hampshire Robocall Investigation:** New Hampshire Attorney General. (2024). Investigation materials.

3. **EU AI Act Election Provisions:** European Parliament and Council. (2024). Regulation (EU) 2024/1689.

4. **EU Digital Services Act:** European Parliament and Council. (2022). Regulation (EU) 2022/2065.

5. **California AB 730:** California Legislature. (2019). Requirements for disclosure of synthetic media in elections.

6. **Texas Election Deepfake Law:** Texas Legislature. SB 751. Criminal penalties for deepfake election interference.

7. **Meta Election Integrity:** Meta. Election integrity policies and operations.

8. **Stanford Internet Observatory:** Stanford University. Research on online election interference.

9. **Partnership on AI:** Partnership on AI. Synthetic media framework for elections.

10. **Brennan Center for Justice:** Brennan Center. Resources on AI and elections.

11. **Election Assistance Commission:** EAC. Guidance on AI and election security.

12. **MIT Media Lab:** MIT. Detect Fakes project and research. https://detectfakes.media.mit.edu/

---

*This article is part of the AI Governance Mastery Program by AIDefence (suniliyer.ca). For more resources on AI governance, visit the complete article series.*
