metadata:
  total_articles: 158
  total_concept_cards: 120
  total_example_cards: 28
  total_resource_cards: 10
  learning_paths: 5
  generated_date: '2025-12-18'
learning_paths:
  - id: history
    title: History
    slug: /learn/history
    tagline: From Dartmouth to DeepMind
    description: Trace AI's evolution from 1950s academic conferences to today's foundation models. Understand what AI is, how it works, and why GPUs power the AI revolution.
    card_count: 15
    estimated_reading_time: 45-60 minutes
    primary_keywords:
      - AI history timeline
      - evolution of AI
      - what is artificial intelligence
      - machine learning explained
    concept_cards:
      - history-1
      - history-2
      - history-3
      - history-4
      - history-5
      - history-6
      - history-7
      - history-8
      - history-9
      - history-10
      - history-11
      - history-12
      - history-13
      - history-14
      - history-15
    example_cards: []
    resource_cards: []
  - id: terminology
    title: Terminology
    slug: /learn/terminology
    tagline: How AI Actually Works
    description: Master AI terminology from machine learning to neural networks. Understand supervised vs. unsupervised learning, deep learning architectures, and foundation models.
    card_count: 24
    estimated_reading_time: 60-90 minutes
    primary_keywords:
      - what is machine learning
      - AI concepts explained
      - supervised vs unsupervised learning
      - neural networks explained
      - deep learning tutorial
    concept_cards:
      - term-1
      - term-2
      - term-3
      - term-4
      - term-5
      - term-6
      - term-7
      - term-8
      - term-9
      - term-10
      - term-11
      - term-12
      - term-13
      - term-14
    example_cards:
      - ex-netflix
      - ex-google-search
      - ex-chatgpt
      - ex-siri-alexa
      - ex-google-maps
    resource_cards: []
  - id: risk
    title: Risk
    slug: /learn/risk
    tagline: When AI Goes Wrong
    description: Explore AI harms from algorithmic bias to deepfakes. Learn how bias enters systems, why explainability matters, and how to build trustworthy AI.
    card_count: 28
    estimated_reading_time: 90-120 minutes
    primary_keywords:
      - algorithmic bias examples
      - AI bias explained
      - AI safety failures
      - deepfake risks
      - AI discrimination
      - AI hallucinations
    concept_cards:
      - risk-1
      - risk-2
      - risk-3
      - risk-4
      - risk-5
      - risk-6
      - risk-7
      - risk-8
      - risk-9
      - risk-10
      - risk-11
      - risk-12
      - risk-13
      - risk-14
      - risk-15
      - risk-16
      - risk-17
      - risk-18
      - risk-19
      - risk-20
    example_cards:
      - ex-amazon-hiring
      - ex-compas
      - ex-apple-card
      - ex-gender-shades
      - ex-uk-grading
      - ex-healthcare-algo
      - ex-predictive-policing
    resource_cards:
      - res-risk-assessment
      - res-bias-audit
  - id: responsibility
    title: Responsibility
    slug: /learn/responsibility
    tagline: AI Governance Frameworks
    description: Navigate AI laws from GDPR to the EU AI Act. Implement governance frameworks, develop policies, and build ethical AI practices.
    card_count: 75
    estimated_reading_time: 180-240 minutes
    primary_keywords:
      - EU AI Act explained
      - AI governance frameworks
      - GDPR and AI
      - AI compliance requirements
      - responsible AI practices
      - AI policy templates
    concept_cards:
      - resp-1
      - resp-2
      - resp-3
      - resp-4
      - resp-5
      - resp-6
      - resp-7
      - resp-8
      - resp-9
      - resp-10
      - resp-11
      - resp-12
      - resp-13
      - resp-14
      - resp-15
      - resp-16
      - resp-17
      - resp-18
      - resp-19
      - resp-20
      - resp-21
      - resp-22
      - resp-23
      - resp-24
      - resp-25
      - resp-26
      - resp-27
      - resp-28
      - resp-29
      - resp-30
      - resp-31
      - resp-32
      - resp-33
      - resp-34
      - resp-35
      - resp-36
      - resp-37
      - resp-38
      - resp-39
      - resp-40
      - resp-41
      - resp-42
      - resp-43
      - resp-44
      - resp-45
      - resp-46
      - resp-47
      - resp-48
      - resp-49
      - resp-50
      - resp-51
      - resp-52
      - resp-53
      - resp-54
      - resp-55
      - resp-56
      - resp-57
      - resp-58
      - resp-59
      - resp-60
      - resp-61
      - resp-62
      - resp-63
      - resp-64
      - resp-65
      - resp-66
      - resp-67
      - resp-68
      - resp-69
      - resp-70
      - resp-71
      - resp-72
      - resp-73
      - resp-74
      - resp-75
    example_cards:
      - ex-amazon-legal
      - ex-gdpr-enforcement
      - ex-compas-legal
      - ex-eu-ai-act-cases
    resource_cards:
      - res-policy-templates
      - res-impact-assessment
      - res-model-card
      - res-ethics-review
      - res-incident-response
      - res-vendor-eval
      - res-ai-roadmap
      - res-aigp-study
  - id: future
    title: Future
    slug: /learn/future
    tagline: What's Next for AI
    description: Explore AGI, existential risk, and emerging trends. Navigate AI careers, industry-specific governance, and the future of regulation.
    card_count: 20
    estimated_reading_time: 60-90 minutes
    primary_keywords:
      - future of AI regulation
      - AGI timeline
      - AI existential risk
      - AI governance careers
      - AI policy trends
    concept_cards:
      - future-1
      - future-2
      - future-3
      - future-4
      - future-5
      - future-6
      - future-7
      - future-8
      - future-9
      - future-10
      - future-11
      - future-12
      - future-13
      - future-14
      - future-15
      - future-16
      - future-17
      - future-18
      - future-19
      - future-20
    example_cards:
      - ex-gpt4-claude-gemini
      - ex-deepfake-elections
      - ex-openai-anthropic
    resource_cards:
      - res-career-roadmap
      - res-aigp-study
concept_cards_history:
  - id: history-1
    title: The Building Blocks – What AI Actually Is
    slug: what-ai-actually-is
    path: history
    source_file: content/articles/final/strong-vs-weak-ai-why-the-difference-matters-for-governance.md
    source_phase: 'Phase 1: AI Fundamentals'
    tldr: AI is machine-based systems that infer from inputs to generate outputs influencing physical or virtual environments. OECD definition Nov 2023.
    content_sections:
      - OECD AI Definition (Nov 2023)
      - ISO/IEC 22989:2022 Terminology
      - What Makes AI Different from Traditional Software
      - Why Definitions Matter for Governance
    related_concepts:
      - history-2
      - history-5
      - term-1
    cross_path_refs:
      terminology:
        - term-1
    tags:
      - fundamentals
      - definitions
      - OECD
      - ISO
  - id: history-2
    title: The AI Family Tree – Types of AI Systems Explained
    slug: ai-family-tree
    path: history
    source_file: content/articles/final/the-ai-family-tree-understanding-ai-intelligence-levels.md
    tldr: AI systems span narrow AI (single task), general AI (multiple tasks), and AGI (hypothetical human-level). Classification frameworks guide governance.
    content_sections:
      - Narrow vs. General vs. AGI
      - OECD Classification Framework (5 dimensions)
      - Use Case Categories
    related_concepts:
      - history-1
      - history-5
    cross_path_refs:
      terminology:
        - term-1
        - term-9
    tags:
      - classification
      - frameworks
      - AGI
      - narrow-AI
  - id: history-3
    title: The AI Technology Stack – From Chips to ChatGPT
    slug: ai-technology-stack
    path: history
    source_file: content/articles/final/the-ai-technology-stack-from-chips-to-applications.md
    tldr: 'AI stack: Hardware (GPUs), Infrastructure (data centers), Models (neural networks), Applications (ChatGPT). Each layer has governance implications.'
    content_sections:
      - 'Hardware Layer: GPUs, TPUs, Custom Chips'
      - 'Infrastructure Layer: Cloud, Data Centers'
      - 'Model Layer: Training, Fine-tuning'
      - 'Application Layer: User-facing AI'
    related_concepts:
      - history-14
      - term-12
    cross_path_refs:
      terminology:
        - term-7
        - term-9
        - term-12
      risk:
        - risk-15
    tags:
      - technology
      - infrastructure
      - hardware
      - cloud
  - id: history-4
    title: AI History – From Dartmouth to DeepMind
    slug: ai-history-timeline
    path: history
    source_file: content/articles/final/ai-history-from-dartmouth-to-deepmind.md
    tldr: 'AI''s 70-year journey: 1956 Dartmouth Conference, 1980s/90s AI Winters, 2012 deep learning breakthrough (AlexNet), 2022 ChatGPT moment.'
    content_sections:
      - '1956: Dartmouth Conference Birth of AI'
      - '1980s-90s: AI Winters'
      - '2012: AlexNet Deep Learning Renaissance'
      - '2016: AlphaGo Defeats World Champion'
      - '2022: ChatGPT Mainstream Moment'
    related_concepts:
      - history-7
      - term-6
      - term-7
    cross_path_refs:
      terminology:
        - term-6
        - term-7
    tags:
      - history
      - timeline
      - Dartmouth
      - AI-winter
      - breakthrough
  - id: history-5
    title: Strong vs. Weak AI – Why the Difference Matters for Governance
    slug: strong-vs-weak-ai
    path: history
    source_file: content/articles/final/strong-vs-weak-ai-why-the-difference-matters-for-governance.md
    tldr: Weak AI (narrow, task-specific, exists today) vs. Strong AI/AGI (general intelligence, hypothetical). Governance must address current weak AI risks.
    content_sections:
      - 'Weak AI: Siri, Spam Filters, Recommendation Engines'
      - 'Strong AI/AGI: Science Fiction vs. Research'
      - Why Governance Focuses on Weak AI
      - Preparing for AGI Without Ignoring Current Harms
    related_concepts:
      - history-1
      - history-2
    cross_path_refs:
      risk:
        - risk-1
    tags:
      - AGI
      - narrow-AI
      - strong-AI
      - weak-AI
      - definitions
  - id: history-6
    title: Machine Learning Demystified – How Machines Actually Learn
    slug: machine-learning-explained
    path: history
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'Machine learning: computers learn patterns from data instead of explicit programming. Three types: supervised, unsupervised, reinforcement learning.'
    content_sections:
      - What Is Machine Learning?
      - Supervised Learning (labeled data)
      - Unsupervised Learning (pattern discovery)
      - Reinforcement Learning (reward-based)
      - Real-World Examples
    related_concepts:
      - history-11
      - term-1
      - term-6
    cross_path_refs:
      terminology:
        - term-1
        - term-2
        - term-6
      risk:
        - risk-2
    example_refs:
      - ex-netflix
    tags:
      - machine-learning
      - supervised
      - unsupervised
      - reinforcement
  - id: history-7
    title: Deep Learning Decoded – Neural Networks for Non-Engineers
    slug: deep-learning-explained
    path: history
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: Deep learning uses multi-layer neural networks to find complex patterns. 2012 AlexNet breakthrough enabled modern AI (image recognition, NLP, etc.).
    content_sections:
      - What Are Neural Networks?
      - Layers, Weights, Backpropagation
      - Why 'Deep'? (Multiple Hidden Layers)
      - 2012 AlexNet Breakthrough
      - Convolutional Neural Networks (CNNs)
      - Recurrent Neural Networks (RNNs)
    related_concepts:
      - history-4
      - history-6
      - term-7
    cross_path_refs:
      terminology:
        - term-7
    tags:
      - deep-learning
      - neural-networks
      - AlexNet
      - CNN
      - RNN
  - id: history-8
    title: Generative AI Explained – How ChatGPT, DALL-E, and Claude Work
    slug: generative-ai-explained
    path: history
    source_file: content/articles/final/generative-ai-explained-how-chatgpt-dall-e-and-claude-work.md
    tldr: Generative AI creates new content (text, images, code) by learning patterns from training data. Transformers architecture (2017) enabled GPT, Claude, DALL-E.
    content_sections:
      - What Is Generative AI?
      - Transformers Architecture (Attention Mechanism)
      - Large Language Models (GPT, Claude, Gemini)
      - Diffusion Models (DALL-E, Stable Diffusion)
      - 'Use Cases: Content Creation, Code Generation'
    related_concepts:
      - history-9
      - history-12
      - term-8
      - term-9
    cross_path_refs:
      terminology:
        - term-8
        - term-9
      risk:
        - risk-3
        - risk-6
    example_refs:
      - ex-chatgpt
    tags:
      - generative-AI
      - transformers
      - GPT
      - DALL-E
      - Claude
  - id: history-9
    title: Large Language Models – The Technology Behind the Hype
    slug: large-language-models
    path: history
    source_file: content/articles/final/large-language-models-the-technology-behind-the-hype.md
    tldr: LLMs (GPT-4, Claude, Gemini) are trained on vast text corpora to predict next words. Scale (parameters, data, compute) drives emergent capabilities.
    content_sections:
      - What Is a Large Language Model?
      - 'Training Process: Pre-training + Fine-tuning'
      - 'Scale: Parameters, Data, Compute (FLOPs)'
      - Emergent Capabilities
      - 'Limitations: Hallucinations, Bias, Knowledge Cutoff'
    related_concepts:
      - history-8
      - history-12
      - term-9
    cross_path_refs:
      terminology:
        - term-9
      risk:
        - risk-3
    example_refs:
      - ex-gpt4-claude-gemini
    tags:
      - LLM
      - GPT-4
      - Claude
      - Gemini
      - transformers
  - id: history-10
    title: AI vs. Automation – Understanding the Distinction
    slug: ai-vs-automation
    path: history
    source_file: content/articles/final/ai-vs-automation-understanding-the-distinction.md
    tldr: Automation follows explicit rules. AI learns patterns from data. AI can handle unstructured inputs (text, images) that traditional automation can't.
    content_sections:
      - 'Automation: Rules-Based Systems'
      - 'AI: Pattern Recognition from Data'
      - When to Use Automation vs. AI
      - Hybrid Approaches
    related_concepts:
      - history-1
      - history-6
    cross_path_refs:
      terminology:
        - term-1
    tags:
      - automation
      - AI-definition
      - rules-based
  - id: history-11
    title: The Data Behind AI – Why Training Data Determines Everything
    slug: data-behind-ai
    path: history
    source_file: content/articles/final/the-data-behind-ai-why-training-data-determines-everything.md
    tldr: AI quality depends on training data quality. Garbage in, garbage out. Bias in data = bias in AI. Data governance is AI governance.
    content_sections:
      - Why Data Matters More Than Algorithms
      - Training Data vs. Inference Data
      - Data Quality Dimensions (Accuracy, Completeness, Representativeness)
      - Historical Bias in Data
      - Data Governance for AI
    related_concepts:
      - history-6
      - term-6
    cross_path_refs:
      terminology:
        - term-6
      risk:
        - risk-2
    tags:
      - training-data
      - data-quality
      - bias
      - data-governance
  - id: history-12
    title: Foundation Models – The New Building Blocks of AI
    slug: foundation-models
    path: history
    source_file: content/articles/final/foundation-models-the-new-building-blocks-of-ai.md
    tldr: Foundation models (GPT-4, DALL-E) are pre-trained on massive datasets, then adapted for specific tasks. Centralized models raise governance challenges.
    content_sections:
      - What Are Foundation Models?
      - Pre-training + Fine-tuning Paradigm
      - 'Examples: GPT-4, DALL-E, CLIP, Whisper'
      - 'Benefits: Efficiency, Transfer Learning'
      - 'Risks: Centralization, Bias Amplification, Misuse'
    related_concepts:
      - history-8
      - history-9
      - term-12
    cross_path_refs:
      terminology:
        - term-12
    example_refs:
      - ex-gpt4-claude-gemini
    tags:
      - foundation-models
      - pre-training
      - fine-tuning
      - GPT-4
  - id: history-13
    title: Multimodal AI – When Machines See, Hear, and Speak
    slug: multimodal-ai
    path: history
    source_file: content/articles/final/multimodal-ai-when-machines-see-hear-and-speak.md
    tldr: Multimodal AI processes multiple input types (text, images, audio, video). GPT-4V, Gemini, Claude 3 combine vision + language understanding.
    content_sections:
      - What Is Multimodal AI?
      - Text + Vision (GPT-4V, Gemini)
      - Text + Audio (Whisper, Speech Recognition)
      - Text + Video (Video Understanding Models)
      - 'Use Cases: Accessibility, Content Moderation, Medical Diagnosis'
    related_concepts:
      - history-8
      - history-9
      - term-13
    cross_path_refs:
      terminology:
        - term-13
    example_refs:
      - ex-gpt4-claude-gemini
    tags:
      - multimodal
      - vision
      - audio
      - GPT-4V
      - Gemini
  - id: history-14
    title: AI Compute – Why GPUs Rule the AI World
    slug: ai-compute-gpus
    path: history
    source_file: content/articles/final/ai-compute-why-gpus-rule-the-ai-world.md
    tldr: GPUs (Graphics Processing Units) parallelize matrix operations, 100x faster than CPUs for AI training. NVIDIA dominance, custom chips (TPU, Trainium).
    content_sections:
      - Why GPUs for AI?
      - Parallel Processing vs. Sequential (CPU)
      - NVIDIA H100, A100 Dominance
      - 'Custom AI Chips: Google TPU, AWS Trainium'
      - Compute Cost = Barrier to Entry
    related_concepts:
      - history-3
      - history-9
    cross_path_refs:
      terminology:
        - term-14
      risk:
        - risk-15
    tags:
      - GPU
      - compute
      - NVIDIA
      - TPU
      - hardware
  - id: history-15
    title: The Environmental Cost of AI – Data Centers, Energy, and Sustainability
    slug: environmental-cost-ai
    path: history
    source_file: content/articles/final/the-environmental-cost-of-ai-data-centers-energy-and-sustainability.md
    tldr: Training GPT-3 consumed 1,287 MWh (equivalent to 120 US homes/year). Data centers use 1-2% global electricity. Sustainability is AI governance issue.
    content_sections:
      - Energy Consumption of AI Training
      - GPT-3 Carbon Footprint
      - Data Center Water Usage (Cooling)
      - E-waste from AI Hardware
      - 'Green AI: Efficiency, Renewable Energy'
    related_concepts:
      - history-14
      - term-9
    cross_path_refs:
      risk:
        - risk-15
    tags:
      - sustainability
      - energy
      - carbon-footprint
      - data-centers
      - green-AI
concept_cards_terminology:
  - id: term-1
    title: What Is Machine Learning? (Terminology Deep Dive)
    slug: machine-learning-terminology
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'Machine learning: algorithms that improve performance through experience (data) without explicit programming. Three paradigms: supervised, unsupervised, reinforcement.'
    content_sections:
      - Tom Mitchell's Definition
      - Supervised Learning Explained
      - Unsupervised Learning Explained
      - Reinforcement Learning Explained
      - When to Use Each Paradigm
    related_concepts:
      - history-6
      - term-2
      - term-6
    cross_path_refs:
      history:
        - history-6
      risk:
        - risk-2
    example_refs:
      - ex-netflix
    tags:
      - machine-learning
      - supervised
      - unsupervised
      - reinforcement
      - definitions
  - id: term-2
    title: Supervised vs. Unsupervised Learning
    slug: supervised-unsupervised
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'Supervised: labeled data (email=spam/not spam). Unsupervised: find hidden patterns (customer segmentation). Different use cases, different risks.'
    content_sections:
      - 'Supervised Learning: Classification, Regression'
      - 'Unsupervised Learning: Clustering, Dimensionality Reduction'
      - Semi-Supervised Learning
      - When to Use Each
    related_concepts:
      - term-1
      - term-6
    cross_path_refs:
      risk:
        - risk-2
    example_refs:
      - ex-netflix
    tags:
      - supervised
      - unsupervised
      - classification
      - clustering
  - id: term-3
    title: Neural Networks Architecture Basics
    slug: neural-networks-basics
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'Neural networks: layers of interconnected nodes (neurons). Input layer → hidden layers → output layer. Weights adjusted via backpropagation.'
    content_sections:
      - Neurons, Weights, Biases
      - Activation Functions (ReLU, Sigmoid, Tanh)
      - Feedforward vs. Recurrent Networks
      - Backpropagation (How Networks Learn)
    related_concepts:
      - history-7
      - term-7
    cross_path_refs:
      history:
        - history-7
    tags:
      - neural-networks
      - architecture
      - backpropagation
      - weights
  - id: term-4
    title: Training Data, Validation Data, Test Data
    slug: training-validation-test
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: Training data (learn patterns), validation data (tune hyperparameters), test data (final evaluation). Never test on training data (overfitting).
    content_sections:
      - 'Training Set: Where AI Learns'
      - 'Validation Set: Hyperparameter Tuning'
      - 'Test Set: Unbiased Evaluation'
      - Overfitting vs. Underfitting
      - Cross-Validation Techniques
    related_concepts:
      - term-1
      - term-6
      - history-11
    cross_path_refs:
      risk:
        - risk-2
    tags:
      - training-data
      - validation
      - testing
      - overfitting
  - id: term-5
    title: Bias vs. Variance Tradeoff
    slug: bias-variance-tradeoff
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'Bias: model too simple (underfitting). Variance: model too complex (overfitting). Goal: sweet spot with low bias + low variance.'
    content_sections:
      - What Is Bias (Underfitting)?
      - What Is Variance (Overfitting)?
      - The Tradeoff Curve
      - Regularization Techniques
    related_concepts:
      - term-4
    tags:
      - bias-variance
      - overfitting
      - underfitting
      - regularization
  - id: term-6
    title: Feature Engineering – The Art of Data Preparation
    slug: feature-engineering
    path: terminology
    source_file: content/articles/final/the-data-behind-ai-why-training-data-determines-everything.md
    tldr: 'Feature engineering: selecting, transforming, creating variables AI uses. Good features = better AI. Domain expertise critical.'
    content_sections:
      - What Are Features?
      - Feature Selection (Which Variables Matter?)
      - Feature Transformation (Normalization, Encoding)
      - Feature Creation (Domain Knowledge)
      - Automated Feature Engineering
    related_concepts:
      - term-1
      - term-4
      - history-11
    cross_path_refs:
      risk:
        - risk-2
    tags:
      - feature-engineering
      - data-preparation
      - domain-knowledge
  - id: term-7
    title: Convolutional Neural Networks (CNNs) for Images
    slug: cnns-image-recognition
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'CNNs: specialized neural networks for images. Convolutional layers detect edges, textures, objects. Used in facial recognition, medical imaging.'
    content_sections:
      - How CNNs Work (Convolution, Pooling)
      - AlexNet (2012 Breakthrough)
      - ResNet, VGG Architectures
      - 'Use Cases: Image Classification, Object Detection'
    related_concepts:
      - history-7
      - term-3
    cross_path_refs:
      history:
        - history-7
      risk:
        - risk-2
    example_refs:
      - ex-gender-shades
    tags:
      - CNN
      - image-recognition
      - computer-vision
      - AlexNet
  - id: term-8
    title: Recurrent Neural Networks (RNNs) for Sequences
    slug: rnns-sequence-data
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'RNNs: neural networks for sequential data (text, time series, audio). LSTM, GRU variants solve vanishing gradient problem.'
    content_sections:
      - How RNNs Work (Memory, Hidden State)
      - Vanishing Gradient Problem
      - LSTM (Long Short-Term Memory)
      - GRU (Gated Recurrent Units)
      - Replaced by Transformers for NLP
    related_concepts:
      - term-3
      - term-9
    cross_path_refs:
      history:
        - history-8
    tags:
      - RNN
      - LSTM
      - GRU
      - sequence-data
      - NLP
  - id: term-9
    title: Transformers – The Architecture Behind ChatGPT
    slug: transformers-architecture
    path: terminology
    source_file: content/articles/final/generative-ai-explained-how-chatgpt-dall-e-and-claude-work.md
    tldr: 'Transformers (2017): attention mechanism processes entire sequence in parallel. Enabled GPT, BERT, Claude. Replaced RNNs for NLP.'
    content_sections:
      - Attention Mechanism (Query, Key, Value)
      - Self-Attention
      - Multi-Head Attention
      - Positional Encoding
      - Encoder-Decoder Architecture
      - GPT (Decoder-Only), BERT (Encoder-Only)
    related_concepts:
      - history-8
      - history-9
      - term-8
    cross_path_refs:
      history:
        - history-8
        - history-9
    example_refs:
      - ex-chatgpt
      - ex-gpt4-claude-gemini
    tags:
      - transformers
      - attention
      - GPT
      - BERT
      - NLP
  - id: term-10
    title: Pre-training and Fine-tuning
    slug: pre-training-fine-tuning
    path: terminology
    source_file: content/articles/final/foundation-models-the-new-building-blocks-of-ai.md
    tldr: 'Pre-training: learn general patterns from massive datasets. Fine-tuning: adapt to specific tasks. Transfer learning enables efficiency.'
    content_sections:
      - Pre-training Phase (Unlabeled Data, Self-Supervised)
      - Fine-tuning Phase (Task-Specific, Labeled Data)
      - Transfer Learning Benefits
      - Few-Shot, Zero-Shot Learning
    related_concepts:
      - history-12
      - term-9
    cross_path_refs:
      history:
        - history-12
    tags:
      - pre-training
      - fine-tuning
      - transfer-learning
      - foundation-models
  - id: term-11
    title: Hyperparameters vs. Parameters
    slug: hyperparameters-vs-parameters
    path: terminology
    source_file: content/articles/final/deep-learning-decoded-neural-networks-for-non-engineers.md
    tldr: 'Parameters: learned from data (weights, biases). Hyperparameters: set by humans before training (learning rate, batch size). Tuning is art+science.'
    content_sections:
      - What Are Parameters? (Model Weights)
      - What Are Hyperparameters? (Learning Rate, etc.)
      - Hyperparameter Tuning Strategies
      - Grid Search, Random Search, Bayesian Optimization
    related_concepts:
      - term-1
      - term-4
    tags:
      - hyperparameters
      - parameters
      - tuning
      - learning-rate
  - id: term-12
    title: Model Size – Parameters, FLOPs, and Scale
    slug: model-size-scale
    path: terminology
    source_file: content/articles/final/large-language-models-the-technology-behind-the-hype.md
    tldr: 'Model size measured in parameters (GPT-3: 175B, GPT-4: ~1.76T rumored). Training compute measured in FLOPs. Scale drives capability.'
    content_sections:
      - What Are Parameters?
      - GPT-3 (175B), GPT-4, PaLM (540B)
      - 'FLOPs: Measuring Training Compute'
      - EU AI Act Systemic Risk Threshold (10^25 FLOPs)
      - Scaling Laws (Emergent Capabilities)
    related_concepts:
      - history-9
      - history-12
      - term-9
    cross_path_refs:
      history:
        - history-9
    example_refs:
      - ex-gpt4-claude-gemini
    tags:
      - model-size
      - parameters
      - FLOPs
      - scaling-laws
  - id: term-13
    title: Embeddings and Vector Representations
    slug: embeddings-vectors
    path: terminology
    source_file: content/articles/final/large-language-models-the-technology-behind-the-hype.md
    tldr: 'Embeddings: words/images/concepts mapped to high-dimensional vectors. Similar meanings = close vectors. Foundation of semantic search, RAG.'
    content_sections:
      - What Are Embeddings?
      - Word2Vec, GloVe (Word Embeddings)
      - Sentence and Document Embeddings
      - Image Embeddings (CLIP)
      - 'Use Cases: Semantic Search, Recommendation'
    related_concepts:
      - term-9
      - history-13
    cross_path_refs:
      history:
        - history-13
    tags:
      - embeddings
      - vectors
      - Word2Vec
      - semantic-search
  - id: term-14
    title: Inference vs. Training
    slug: inference-vs-training
    path: terminology
    source_file: content/articles/final/foundation-models-the-new-building-blocks-of-ai.md
    tldr: 'Training: AI learns patterns (expensive, one-time). Inference: AI makes predictions (cheap, repeated). Different hardware, cost profiles.'
    content_sections:
      - 'Training Phase: Learning from Data'
      - 'Inference Phase: Applying Learned Patterns'
      - Computational Costs (Training >> Inference)
      - Latency Requirements (Inference Critical)
      - Inference Optimization Techniques
    related_concepts:
      - history-14
      - term-1
    cross_path_refs:
      history:
        - history-14
    tags:
      - inference
      - training
      - deployment
      - latency
concept_cards_risk:
  - id: risk-1
    title: When AI Goes Wrong – A Taxonomy of AI Harms
    slug: taxonomy-ai-harms
    path: risk
    source_file: content/articles/final/when-ai-goes-wrong-a-taxonomy-of-ai-harms.md
    tldr: 'AI harms: individual (bias, privacy), societal (misinformation, job loss), systemic (power concentration). Classification guides governance.'
    content_sections:
      - Individual Harms (Discrimination, Privacy Violations)
      - Societal Harms (Misinformation, Manipulation)
      - Systemic Harms (Power Concentration, Environmental)
      - Severity vs. Likelihood Matrix
    related_concepts:
      - risk-2
      - risk-4
      - risk-6
      - risk-7
      - risk-8
    tags:
      - AI-harms
      - taxonomy
      - risk-classification
  - id: risk-2
    title: Algorithmic Bias – How AI Discriminates and Why
    slug: algorithmic-bias
    path: risk
    source_file: content/articles/final/algorithmic-bias-how-ai-discriminates-and-why.md
    tldr: Systematic errors in AI that unfairly disadvantage specific groups through biased data, design choices, or deployment.
    content_sections:
      - What Is Algorithmic Bias?
      - 'Source 1: Biased Training Data'
      - 'Source 2: Biased Design Choices'
      - 'Source 3: Biased Deployment'
      - How to Detect Bias
      - Mitigation Strategies
    related_concepts:
      - risk-18
      - risk-19
    cross_path_refs:
      terminology:
        - term-1
        - term-6
        - history-11
    example_refs:
      - ex-amazon-hiring
      - ex-compas
      - ex-apple-card
      - ex-gender-shades
      - ex-uk-grading
      - ex-healthcare-algo
    resource_refs:
      - res-bias-audit
    tags:
      - algorithmic-bias
      - discrimination
      - fairness
      - bias-detection
    word_count: 2833
  - id: risk-3
    title: AI Hallucinations – When Machines Confidently Lie
    slug: ai-hallucinations
    path: risk
    source_file: content/articles/final/ai-hallucinations-when-machines-confidently-lie.md
    tldr: LLMs generate plausible but false information (hallucinations). Caused by training data gaps, pattern overgeneralization, lack of grounding.
    content_sections:
      - What Are AI Hallucinations?
      - Why LLMs Hallucinate (Probabilistic Prediction)
      - 'Types: Fabricated Facts, Fake Citations, Confabulation'
      - Detection Methods
      - 'Mitigation: RAG, Grounding, Human Verification'
    related_concepts:
      - history-9
      - term-9
    cross_path_refs:
      terminology:
        - term-9
        - history-9
    example_refs:
      - ex-chatgpt
    tags:
      - hallucinations
      - LLM
      - misinformation
      - reliability
  - id: risk-4
    title: The Black Box Problem – Why AI Explainability Matters
    slug: black-box-explainability
    path: risk
    source_file: content/articles/final/the-black-box-problem-why-ai-explainability-matters.md
    tldr: 'Complex AI models (deep learning) are black boxes: inputs → outputs, but internal logic unclear. Explainability critical for trust, debugging, compliance.'
    content_sections:
      - What Is the Black Box Problem?
      - Why Explainability Matters (Trust, Legal, Debugging)
      - Interpretable Models vs. Post-hoc Explanations
      - LIME, SHAP, Attention Visualization
      - Accuracy vs. Interpretability Tradeoff
    related_concepts:
      - risk-19
    cross_path_refs:
      terminology:
        - term-7
    tags:
      - explainability
      - black-box
      - interpretability
      - LIME
      - SHAP
  - id: risk-5
    title: AI and Privacy – The Data Collection Dilemma
    slug: ai-privacy-dilemma
    path: risk
    source_file: content/articles/final/ai-and-privacy-the-data-collection-dilemma.md
    tldr: 'AI requires vast data, often personal. Risks: surveillance, re-identification, secondary use without consent. Privacy-enhancing tech (PETs) offers solutions.'
    content_sections:
      - AI's Insatiable Data Appetite
      - 'Privacy Risks: Surveillance, Re-identification, Profiling'
      - Training Data Privacy (Membership Inference Attacks)
      - Inference Privacy (Model Inversion)
      - Privacy-Enhancing Technologies (Differential Privacy, Federated Learning)
    related_concepts:
      - risk-20
    tags:
      - privacy
      - surveillance
      - GDPR
      - PETs
      - differential-privacy
  - id: risk-6
    title: Deepfakes and Synthetic Media – The Trust Crisis
    slug: deepfakes-trust-crisis
    path: risk
    source_file: content/articles/final/deepfakes-and-synthetic-media-the-trust-crisis.md
    tldr: AI-generated fake videos, audio, images (deepfakes) erode trust in media. Election interference, fraud, harassment risks. Detection + regulation needed.
    content_sections:
      - What Are Deepfakes? (GANs, Diffusion Models)
      - 'Use Cases: Entertainment vs. Malicious'
      - 'Harms: Misinformation, Fraud, Non-Consensual Porn'
      - Detection Technologies
      - 'Regulation: EU AI Act Transparency Requirements'
    related_concepts:
      - risk-7
    cross_path_refs:
      terminology:
        - history-8
    example_refs:
      - ex-deepfake-elections
    tags:
      - deepfakes
      - synthetic-media
      - GANs
      - misinformation
  - id: risk-7
    title: AI-Powered Misinformation – Democracy at Risk
    slug: ai-misinformation-democracy
    path: risk
    source_file: content/articles/final/ai-powered-misinformation-democracy-at-risk.md
    tldr: 'LLMs generate convincing fake news at scale. Recommendation algorithms amplify divisive content. Risks: election interference, polarization, radicalization.'
    content_sections:
      - AI-Generated Misinformation (Text, Images, Video)
      - Algorithmic Amplification (Facebook, YouTube, TikTok)
      - Election Interference Scenarios
      - Combating AI Misinformation (Detection, Labeling, Literacy)
    related_concepts:
      - risk-6
    example_refs:
      - ex-deepfake-elections
    tags:
      - misinformation
      - democracy
      - elections
      - social-media
  - id: risk-8
    title: Job Displacement – AI and the Future of Work
    slug: job-displacement
    path: risk
    source_file: content/articles/final/when-ai-goes-wrong-a-taxonomy-of-ai-harms.md
    tldr: AI automates cognitive tasks (writing, coding, analysis). 300M jobs affected per Goldman Sachs. Reskilling, safety nets, governance needed.
    content_sections:
      - Which Jobs Are at Risk? (Cognitive Automation)
      - Historical Automation vs. AI (Scope, Speed)
      - Economic Impact Estimates
      - 'Mitigation: Reskilling, UBI, Labor Protections'
    tags:
      - job-displacement
      - automation
      - future-of-work
      - reskilling
  - id: risk-9
    title: Autonomous Weapons – The AI Arms Race
    slug: autonomous-weapons
    path: risk
    source_file: content/articles/final/when-ai-goes-wrong-a-taxonomy-of-ai-harms.md
    tldr: 'Lethal autonomous weapons (LAWS): AI selects and engages targets without human intervention. International ban efforts ongoing. Proliferation risk.'
    content_sections:
      - What Are Autonomous Weapons?
      - Current Capabilities (Drones, Missile Defense)
      - 'Risks: Accidents, Proliferation, Lowered Threshold for War'
      - International Governance Efforts (UN CCW)
    tags:
      - autonomous-weapons
      - LAWS
      - arms-race
      - security
  - id: risk-10
    title: AI Safety – Preventing Catastrophic Failures
    slug: ai-safety-failures
    path: risk
    source_file: content/articles/final/ai-safety-preventing-catastrophic-failures.md
    tldr: 'AI safety: preventing systems from causing harm (accidents, misuse, misalignment). Red teaming, robustness testing, safety by design.'
    content_sections:
      - What Is AI Safety?
      - Accident Risk (Bugs, Edge Cases, Distributional Shift)
      - Misuse Risk (Dual-Use Technologies)
      - Misalignment Risk (AGI Alignment Problem)
      - 'Safety Techniques: Robustness Testing, Red Teaming'
    related_concepts:
      - risk-9
    tags:
      - AI-safety
      - robustness
      - red-teaming
      - alignment
  - id: risk-11
    title: Building Trustworthy AI – The Seven Pillars
    slug: trustworthy-ai-pillars
    path: risk
    source_file: content/articles/final/building-trustworthy-ai-the-seven-pillars.md
    tldr: 'Trustworthy AI framework: (1) Human agency, (2) Robustness, (3) Privacy, (4) Transparency, (5) Fairness, (6) Accountability, (7) Societal wellbeing.'
    content_sections:
      - 'Pillar 1: Human Agency and Oversight'
      - 'Pillar 2: Technical Robustness and Safety'
      - 'Pillar 3: Privacy and Data Governance'
      - 'Pillar 4: Transparency'
      - 'Pillar 5: Diversity, Non-discrimination, Fairness'
      - 'Pillar 6: Societal and Environmental Wellbeing'
      - 'Pillar 7: Accountability'
    related_concepts:
      - risk-18
      - risk-19
      - risk-20
    tags:
      - trustworthy-AI
      - AI-principles
      - ethics
      - governance
  - id: risk-12
    title: Human-Centered AI Design – Keeping People in the Loop
    slug: human-centered-ai
    path: risk
    source_file: content/articles/final/human-centered-ai-design-keeping-people-in-the-loop.md
    tldr: 'Human-in-the-loop (HITL): humans review AI decisions before implementation. Critical for high-stakes use cases. Prevents automation bias.'
    content_sections:
      - What Is Human-in-the-Loop?
      - When HITL Is Required (High-Stakes Decisions)
      - Effective HITL Design (Avoid Rubber-Stamping)
      - Human-on-the-Loop (Monitoring)
      - Automation Bias Risk
    related_concepts:
      - risk-11
    tags:
      - HITL
      - human-in-loop
      - oversight
      - automation-bias
  - id: risk-13
    title: Fairness in AI – Definitions, Metrics, and Trade-offs
    slug: fairness-definitions
    path: risk
    source_file: content/articles/final/building-trustworthy-ai-the-seven-pillars.md
    tldr: Multiple fairness definitions (demographic parity, equal opportunity, predictive parity). Often mutually exclusive. Context determines which matters.
    content_sections:
      - Demographic Parity
      - Equal Opportunity (True Positive Parity)
      - Predictive Parity (Precision Parity)
      - Individual Fairness
      - Impossibility Theorems (Can't Satisfy All)
      - Choosing the Right Metric
    related_concepts:
      - risk-2
      - risk-18
    resource_refs:
      - res-bias-audit
    tags:
      - fairness
      - metrics
      - demographic-parity
      - equal-opportunity
  - id: risk-14
    title: AI Transparency – What Users Deserve to Know
    slug: ai-transparency
    path: risk
    source_file: content/articles/final/ai-transparency-what-users-deserve-to-know.md
    tldr: 'Transparency obligations: disclose AI use, explain logic, provide recourse. Required by GDPR Article 22, EU AI Act, NYC Law 144.'
    content_sections:
      - Why Transparency Matters
      - Levels of Transparency (System, Logic, Decision)
      - GDPR Article 22 (Right to Explanation)
      - EU AI Act Transparency Requirements
      - Model Cards, Datasheets, Nutrition Labels
    related_concepts:
      - risk-4
    tags:
      - transparency
      - disclosure
      - GDPR
      - model-cards
  - id: risk-15
    title: AI Accountability – Who's Responsible When AI Fails?
    slug: ai-accountability
    path: risk
    source_file: content/articles/final/ai-accountability-who-is-responsible-when-ai-causes-harm.md
    tldr: 'Accountability gap: who''s liable when AI harms? Developer, deployer, user? Product liability, negligence, strict liability frameworks.'
    content_sections:
      - The Accountability Problem (Many Actors)
      - 'Legal Frameworks: Product Liability, Negligence'
      - EU AI Liability Directive
      - Organizational Accountability (Governance Roles)
    related_concepts:
      - risk-11
    tags:
      - accountability
      - liability
      - product-liability
      - responsibility
  - id: risk-16
    title: The Ethics Landscape – AI Principles Worldwide
    slug: ethics-landscape
    path: risk
    source_file: content/articles/final/global-ai-law-tracker-whos-regulating-what.md
    tldr: '170+ AI ethics frameworks globally (OECD, UNESCO, IEEE, company policies). Convergence on core principles: fairness, transparency, accountability.'
    content_sections:
      - OECD AI Principles (2019, Updated 2024)
      - UNESCO AI Ethics Recommendation
      - IEEE Ethically Aligned Design
      - Corporate AI Principles (Google, Microsoft, Amazon)
      - Convergence vs. Divergence
    related_concepts:
      - risk-11
      - risk-17
      - risk-18
      - risk-19
    tags:
      - AI-ethics
      - principles
      - OECD
      - UNESCO
      - IEEE
  - id: risk-17
    title: OECD AI Principles – The Global Standard
    slug: oecd-ai-principles
    path: risk
    source_file: content/articles/final/global-ai-law-tracker-whos-regulating-what.md
    tldr: '5 OECD AI Principles (2019): Inclusive growth, Human-centered values, Transparency, Robustness, Accountability. 47 countries adopted. Updated May 2024.'
    content_sections:
      - 'Principle 1: Inclusive Growth, Sustainable Development, Wellbeing'
      - 'Principle 2: Human-Centered Values and Fairness'
      - 'Principle 3: Transparency and Explainability'
      - 'Principle 4: Robustness, Security, and Safety'
      - 'Principle 5: Accountability'
      - 2024 Updates (Foundation Models, Climate)
    related_concepts:
      - risk-16
      - risk-11
    tags:
      - OECD
      - AI-principles
      - international-standards
  - id: risk-18
    title: The White House AI Bill of Rights – America's Framework
    slug: ai-bill-of-rights
    path: risk
    source_file: content/articles/final/global-ai-law-tracker-whos-regulating-what.md
    tldr: '5 protections (2022): Safe systems, Algorithmic discrimination protections, Data privacy, Notice & explanation, Human alternatives. Non-binding guidance.'
    content_sections:
      - Safe and Effective Systems
      - Algorithmic Discrimination Protections
      - Data Privacy
      - Notice and Explanation
      - Human Alternatives, Consideration, and Fallback
      - Implementation Guidance
    related_concepts:
      - risk-16
      - risk-11
    tags:
      - AI-Bill-of-Rights
      - White-House
      - US-policy
  - id: risk-19
    title: UNESCO AI Ethics – A Global Perspective
    slug: unesco-ai-ethics
    path: risk
    source_file: content/articles/final/global-ai-law-tracker-whos-regulating-what.md
    tldr: 'UNESCO Recommendation on AI Ethics (2021): 10 principles, 11 policy areas. Adopted by 193 countries. Emphasizes human rights, equity, sustainability.'
    content_sections:
      - 10 Principles (Human Rights, Wellbeing, Diversity, etc.)
      - 11 Policy Action Areas
      - Global South Perspective
      - Implementation Challenges
    related_concepts:
      - risk-16
      - risk-11
    tags:
      - UNESCO
      - AI-ethics
      - global-perspective
      - human-rights
  - id: risk-20
    title: Responsible AI in Practice – From Principles to Implementation
    slug: responsible-ai-practice
    path: risk
    source_file: content/articles/final/responsibility-of-responsible-ai-for-organizations.md
    tldr: 'Bridging principles-to-practice gap: embed ethics in SDLC, governance structures, tooling. Microsoft, Google responsible AI programs.'
    content_sections:
      - The Operationalization Challenge
      - Embedding Ethics in Development Lifecycle
      - Governance Structures (Ethics Boards, Review Processes)
      - 'Tools: Fairlearn, AI Fairness 360, Model Cards'
      - 'Case Studies: Microsoft, Google RAI Programs'
    related_concepts:
      - risk-11
      - risk-16
    tags:
      - responsible-AI
      - operationalization
      - governance
      - tools
example_cards:
  - id: ex-amazon-hiring
    title: Amazon's Biased Hiring Algorithm
    slug: amazon-hiring-bias
    summary: Amazon scrapped AI recruiting tool that penalized resumes containing 'women's', learning bias from historical hiring data (2018).
    category: bias
    industry: hr
    year: 2018
    source: Reuters (Dastin, 2018)
    referenced_by_concepts:
      - risk-2
      - resp-38
      - resp-39
      - resp-51
    related_examples:
      - ex-compas
      - ex-apple-card
    tags:
      - discrimination
      - gender-bias
      - historical-bias
      - employment
      - machine-learning
    markdown_file: content/examples/amazon-hiring.md
  - id: ex-compas
    title: ProPublica COMPAS Investigation
    slug: compas-bias
    summary: ProPublica found COMPAS algorithm incorrectly flagged Black defendants as high-risk at 2x rate of white defendants (2016).
    category: bias
    industry: criminal-justice
    year: 2016
    source: ProPublica (Angwin et al., 2016)
    referenced_by_concepts:
      - risk-2
      - risk-13
      - resp-145
    related_examples:
      - ex-amazon-hiring
      - ex-predictive-policing
    tags:
      - bias
      - criminal-justice
      - fairness-metrics
      - recidivism
    markdown_file: content/examples/compas.md
  - id: ex-apple-card
    title: Apple Card Gender Bias Controversy
    slug: apple-card-bias
    summary: Apple Card algorithm gave women lower credit limits than men with same finances. NY regulator investigated (2019).
    category: bias
    industry: finance
    year: 2019
    source: New York Times (Vigdor, 2019)
    referenced_by_concepts:
      - risk-2
      - resp-37
      - resp-47
    related_examples:
      - ex-amazon-hiring
    tags:
      - bias
      - credit-scoring
      - gender-discrimination
      - finance
    markdown_file: content/examples/apple-card.md
  - id: ex-gender-shades
    title: Gender Shades Study (MIT)
    slug: gender-shades
    summary: Joy Buolamwini found facial recognition error rates 34% for dark-skinned women vs 0.8% for light-skinned men (2018).
    category: bias
    industry: computer-vision
    year: 2018
    source: Buolamwini & Gebru (2018)
    referenced_by_concepts:
      - risk-2
      - term-7
    related_examples:
      - ex-healthcare-algo
    tags:
      - bias
      - facial-recognition
      - representation-bias
      - intersectionality
    markdown_file: content/examples/gender-shades.md
  - id: ex-uk-grading
    title: UK A-Level Grading Algorithm (COVID)
    slug: uk-grading-algorithm
    summary: Algorithm downgraded students at historically lower-performing schools, encoding socioeconomic bias. Public outcry forced reversal (2020).
    category: bias
    industry: education
    year: 2020
    source: BBC News (Adams, 2020)
    referenced_by_concepts:
      - risk-2
    related_examples:
      - ex-amazon-hiring
    tags:
      - bias
      - education
      - design-choices
      - socioeconomic
    markdown_file: content/examples/uk-grading.md
  - id: ex-healthcare-algo
    title: Healthcare Algorithm Underestimating Black Patients' Needs
    slug: healthcare-bias
    summary: Algorithm used cost as proxy for need, systematically underestimating Black patients' healthcare needs (Obermeyer et al., 2019).
    category: bias
    industry: healthcare
    year: 2019
    source: Science (Obermeyer et al., 2019)
    referenced_by_concepts:
      - risk-2
      - resp-139
    related_examples:
      - ex-gender-shades
    tags:
      - bias
      - healthcare
      - deployment-bias
      - proxy-variables
    markdown_file: content/examples/healthcare-algo.md
  - id: ex-predictive-policing
    title: Predictive Policing Feedback Loops
    slug: predictive-policing
    summary: Algorithms predict crime in neighborhoods, police sent there, more arrests recorded, confirming algorithm's 'prediction'. Runaway feedback loop.
    category: bias
    industry: criminal-justice
    year: '2018'
    source: Ensign et al. (2018)
    referenced_by_concepts:
      - risk-2
      - resp-145
    related_examples:
      - ex-compas
    tags:
      - bias
      - feedback-loops
      - predictive-policing
      - criminal-justice
    markdown_file: content/examples/predictive-policing.md
  - id: ex-netflix
    title: Netflix Recommendation Algorithm
    slug: netflix-recommendations
    summary: Netflix personalization AI saves ~$1B annually in customer retention. Uses collaborative filtering + deep learning.
    category: optimization
    industry: entertainment
    year: 2006-present
    source: Netflix Tech Blog
    referenced_by_concepts:
      - history-6
      - term-1
      - term-2
    related_examples:
      - ex-google-search
    tags:
      - recommendation
      - personalization
      - collaborative-filtering
      - optimization
    markdown_file: content/examples/netflix.md
  - id: ex-google-search
    title: Google Search AI (RankBrain)
    slug: google-search
    summary: Google RankBrain (2015) uses machine learning to understand search queries and rank results. Third-most important ranking factor.
    category: search
    industry: tech
    year: 2015-present
    source: Google Blog
    referenced_by_concepts:
      - term-1
    related_examples:
      - ex-netflix
    tags:
      - search
      - ranking
      - RankBrain
      - NLP
    markdown_file: content/examples/google-search.md
  - id: ex-chatgpt
    title: ChatGPT – Mainstream LLM Breakthrough
    slug: chatgpt
    summary: ChatGPT (Nov 2022) reached 100M users in 2 months, fastest-growing consumer app. Transformer-based LLM with RLHF fine-tuning.
    category: generative-ai
    industry: tech
    year: '2022'
    source: OpenAI
    referenced_by_concepts:
      - history-8
      - history-9
      - term-9
      - risk-3
    related_examples:
      - ex-gpt4-claude-gemini
    tags:
      - LLM
      - ChatGPT
      - generative-AI
      - transformers
    markdown_file: content/examples/chatgpt.md
  - id: ex-gpt4-claude-gemini
    title: GPT-4, Claude, Gemini – Foundation Model Landscape
    slug: foundation-models
    summary: 'Top foundation models: GPT-4 (OpenAI), Claude 3 (Anthropic), Gemini (Google). Multimodal, 1T+ parameters, systemic risk per EU AI Act.'
    category: foundation-models
    industry: tech
    year: 2023-2024
    source: Company releases
    referenced_by_concepts:
      - history-9
      - history-12
      - history-13
      - term-9
      - term-12
      - future-1
    related_examples:
      - ex-chatgpt
    tags:
      - foundation-models
      - GPT-4
      - Claude
      - Gemini
      - multimodal
    markdown_file: content/examples/foundation-models.md
  - id: ex-deepfake-elections
    title: Deepfake Election Interference Examples
    slug: deepfake-elections
    summary: '2024 examples: fake Biden robocalls (NH primary), fake Slovakia PM audio. EU AI Act requires deepfake labeling.'
    category: deepfakes
    industry: politics
    year: '2024'
    source: News reports
    referenced_by_concepts:
      - risk-6
      - risk-7
      - resp-134
    related_examples: []
    tags:
      - deepfakes
      - elections
      - misinformation
      - democracy
    markdown_file: content/examples/deepfake-elections.md
resource_cards:
  - id: res-risk-assessment
    title: AI Risk Assessment Template
    slug: ai-risk-assessment
    type: template
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-75-ai-risk-assessment-templates-tools-for-practition.md
    referenced_by_concepts:
      - risk-1
      - resp-80
    category: risk-management
    tags:
      - risk-assessment
      - template
      - NIST
    markdown_file: content/resources/risk-assessment-template.md
  - id: res-bias-audit
    title: How to Perform a Bias Audit – Methodology and Tools
    slug: bias-audit-checklist
    type: checklist
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-152-how-to-perform-a-bias-audit-methodology-and-tool.md
    referenced_by_concepts:
      - risk-2
      - risk-13
      - resp-93
    category: testing
    tags:
      - bias-testing
      - fairness
      - audit
      - checklist
    markdown_file: content/resources/bias-audit-checklist.md
  - id: res-policy-templates
    title: AI Policy Templates
    slug: ai-policy-templates
    type: template
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-104-ai-policy-development-templates-and-best-practic.md
    referenced_by_concepts:
      - resp-104
    category: governance
    templates:
      - AI Acceptable Use Policy
      - AI Development Policy
      - Third-Party AI Policy
      - AI Risk Management Policy
      - AI Data Governance Policy
      - AI Ethics Policy Statement
      - Quick Reference Card
    tags:
      - templates
      - policies
      - governance
    markdown_file: content/resources/policy-templates.md
  - id: res-impact-assessment
    title: How to Conduct an AI Impact Assessment – Step by Step
    slug: ai-impact-assessment
    type: guide
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-150-how-to-conduct-an-ai-impact-assessment-step-by-s.md
    referenced_by_concepts:
      - resp-74
    category: compliance
    tags:
      - impact-assessment
      - DPIA
      - EU-AI-Act
    markdown_file: content/resources/impact-assessment.md
  - id: res-model-card
    title: How to Build a Model Card – Documentation Best Practices
    slug: model-card-template
    type: template
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-151-how-to-build-a-model-card-documentation-best-pra.md
    referenced_by_concepts:
      - resp-95
      - risk-14
    category: documentation
    tags:
      - model-cards
      - documentation
      - transparency
    markdown_file: content/resources/model-card-template.md
  - id: res-ethics-review
    title: How to Create an AI Ethics Review Process
    slug: ethics-review-process
    type: workflow
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-153-how-to-create-an-ai-ethics-review-process.md
    referenced_by_concepts:
      - resp-103
    category: governance
    tags:
      - ethics-review
      - governance
      - process
    markdown_file: content/resources/ethics-review-process.md
  - id: res-incident-response
    title: How to Respond to an AI Incident – Playbook and Checklist
    slug: ai-incident-response
    type: playbook
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-154-how-to-respond-to-an-ai-incident-playbook-and-ch.md
    referenced_by_concepts:
      - resp-99
    category: operations
    tags:
      - incident-response
      - playbook
      - operations
    markdown_file: content/resources/incident-response-playbook.md
  - id: res-vendor-eval
    title: How to Evaluate AI Vendors – Due Diligence Framework
    slug: vendor-evaluation
    type: framework
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-155-how-to-evaluate-ai-vendors-due-diligence-framewo.md
    referenced_by_concepts:
      - resp-106
      - resp-107
    category: procurement
    tags:
      - vendor-evaluation
      - third-party
      - due-diligence
    markdown_file: content/resources/vendor-evaluation.md
  - id: res-ai-roadmap
    title: How to Build an AI Governance Roadmap – 12-Month Plan
    slug: ai-governance-roadmap
    type: guide
    format:
      - markdown
      - pdf
    source_file: Derived from governance implementation articles
    referenced_by_concepts:
      - resp-101
    category: planning
    tags:
      - roadmap
      - governance
      - planning
    markdown_file: content/resources/governance-roadmap.md
  - id: res-aigp-study
    title: How to Pass the AIGP Certification – Complete Study Guide
    slug: aigp-study-guide
    type: guide
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-158-how-to-pass-the-aigp-certification-complete-stud.md
    referenced_by_concepts:
      - future-12
    category: career
    tags:
      - AIGP
      - certification
      - study-guide
      - career
    markdown_file: content/resources/aigp-study-guide.md
  - id: res-career-roadmap
    title: AI Governance Career Roadmap
    slug: ai-governance-careers
    type: guide
    format:
      - markdown
      - pdf
    source_file: content/articles/final/article-137-ai-governance-careers-building-your-professional.md
    referenced_by_concepts:
      - future-11
      - future-12
    category: career
    tags:
      - career
      - roadmap
      - skills
    markdown_file: content/resources/career-roadmap.md
cross_path_references:
  - from: history-1
    to:
      - path: terminology
        concepts:
          - term-1
        reason: OECD definition connects to ML fundamentals
      - path: responsibility
        concepts:
          - resp-1
        reason: Definitions inform legal frameworks
  - from: history-6
    to:
      - path: terminology
        concepts:
          - term-1
          - term-2
          - term-6
        reason: Deep dive into ML concepts
      - path: risk
        concepts:
          - risk-2
        reason: Training data bias origin
  - from: risk-2
    to:
      - path: responsibility
        concepts:
          - resp-38
          - resp-39
          - resp-93
        reason: Legal frameworks addressing bias
      - path: terminology
        concepts:
          - term-1
          - term-6
          - history-11
        reason: Technical foundations of bias
  - from: risk-14
    to:
      - path: responsibility
        concepts:
          - resp-45
          - resp-95
          - resp-111
        reason: Transparency regulations (GDPR, EU AI Act)
  - from: term-7
    to:
      - path: risk
        concepts:
          - risk-2
        reason: CNN bias in facial recognition
  - from: term-9
    to:
      - path: risk
        concepts:
          - risk-3
        reason: LLM hallucinations
seo_strategy:
  path_ownership:
    history:
      - AI history timeline
      - what is artificial intelligence
      - evolution of AI
    terminology:
      - what is machine learning
      - AI concepts explained
      - neural networks tutorial
    risk:
      - algorithmic bias examples
      - AI safety failures
      - deepfake risks
    responsibility:
      - EU AI Act explained
      - AI governance frameworks
      - GDPR and AI
    future:
      - future of AI regulation
      - AGI timeline
      - AI governance careers
  redirect_rules:
    - from: /articles/algorithmic-bias-how-ai-discriminates-and-why
      to: /learn/risk#algorithmic-bias
      status: 301
    - from: /articles/article-104-ai-policy-development-templates-and-best-practic
      to: /learn/responsibility#ai-policy-development
      status: 301
    - from: /articles/article-51-the-eu-ai-act-europes-landmark-regulation-explain
      to: /learn/responsibility#eu-ai-act
      status: 301
ui_configuration:
  card_expansion:
    animation: GSAP ScrollTrigger + Framer Motion
    collapsed_height: auto
    expanded_content: Full article with curved boxes, examples, cross-refs
  sidebar:
    sticky: true
    sections:
      - Mini-map (current path concepts)
      - Related Cards from Other Paths
      - Jump to Section (h2 headings)
  carousel:
    type: horizontal_blur
    focus_effect: Center card sharp, others blur(8px) opacity(0.7)
    navigation: Arrows (desktop), Swipe (mobile)
content_status:
  history_path:
    concepts_defined: 15
    markdown_files_created: 0
    completion: 0%
  terminology_path:
    concepts_defined: 14
    markdown_files_created: 0
    completion: 0%
  risk_path:
    concepts_defined: 20
    markdown_files_created: 1
    completion: 5%
  responsibility_path:
    concepts_defined: 75
    markdown_files_created: 2
    completion: 3%
  future_path:
    concepts_defined: 20
    markdown_files_created: 0
    completion: 0%
  example_cards:
    total: 28
    markdown_files_created: 0
    completion: 0%
  resource_cards:
    total: 10
    markdown_files_created: 0
    completion: 0%
next_steps:
  - Create markdown files for all concept cards (120 files)
  - Create markdown files for all example cards (28 files)
  - Create markdown files for all resource cards (10 files)
  - Implement knowledge graph navigation (React components)
  - Build card expansion animations (GSAP + Framer Motion)
  - Generate sitemap with learning path structure
  - Implement SEO 301 redirects from old articles
concept_cards_responsibility:
  - id: resp-1
    title: AI Accountability - Who Is Responsible When AI Causes Harm?
    slug: ai-accountability-who-is-responsible-when-ai-causes-harm
    path: responsibility
    source_file: content/articles/final/ai-accountability-who-is-responsible-when-ai-causes-harm.md
    tldr: When AI causes harm, existing laws already apply—anti-discrimination laws, product liability, privacy regulations, and more. Organizations can't wait for AI-specific laws; they must navigate a complex legal patchwork now. Liability gaps, enforcement uncertainty, and overlapping regulations make AI governance essential.
    content_sections:
      - Introduction
      - Core Concepts
      - Practical Applications
      - Key Takeaways
    tags:
      - ai-accountability
      - legal-liability
      - compliance
      - product-liability
      - anti-discrimination
      - copyright
    example_cards:
      - algorithmic-bias-case-studies
      - ai-privacy-violations-case-studies
      - responsible-ai-governance-case-studies
  - id: resp-2
    title: AI and Employment Law - Hiring Algorithms Under Scrutiny
    slug: ai-and-employment-law-hiring-algorithms-under-scrutiny
    path: responsibility
    source_file: content/articles/final/ai-and-employment-law-hiring-algorithms-under-scrutiny.md
    tldr: AI hiring tools operate within comprehensive employment discrimination framework - Title VII of Civil Rights Act 1964 prohibits discrimination based on race, color, religion, sex, national origin through both disparate treatment (intentional) and disparate impact (neutral practices producing discriminatory outcomes without business justification), Age Discrimination in Employment Act (ADEA) protects workers 40+ from age proxies like graduation year/experience caps/"digital native" requirements, Americans with Disabilities Act (ADA) requires accessible AI tools and reasonable accommodations with concerns about measuring disability-related characteristics rather than job-relevant qualifications. State/local laws add AI-specific requirements - NYC Local Law 144 (2023) mandates annual bias audits by independent auditors, published audit results, candidate notification of AI use, Illinois AI Video Interview Act (2020) requires notification/explanation/consent before AI video analysis, Maryland (2020) requires facial recognition consent. AI hiring discrimination occurs through training data reflecting historical bias (Amazon's resume screener penalized "women's" from male-dominated training data), proxies for protected characteristics (zip code/name/graduation year/employment gaps/hobbies correlating with race/gender/age/disability), measuring wrong characteristics (video analysis of facial expressions/eye contact reflecting cultural differences not qualifications, gamified assessments favoring gaming familiarity), and lack of validation against actual job performance. EEOC 2023 Technical Assistance Document establishes employers remain liable even using vendor tools, four-fifths rule applies (adverse impact if protected group selection rate under 80% of highest group), validation required for tools with adverse impact, less discriminatory alternatives must be considered. EEOC Strategic Enforcement Plan 2024-2028 identifies technology-related employment discrimination as priority area. NYC Local Law 144 implementation shows challenges - determining AEDT coverage boundaries, audit methodology inconsistencies, vendor unpreparedness, limited initial enforcement despite pushing industry toward transparency. Employers must conduct vendor due diligence (validation studies, adverse impact analyses, ongoing monitoring documentation), monitor selection rates by demographic group at each hiring stage, maintain human oversight never allowing AI final decisions without review, provide accommodation processes, keep comprehensive records, meet jurisdiction-specific disclosure requirements. Organizations cannot outsource Title VII responsibilities to vendors - must conduct own adverse impact analyses, cannot rely solely on vendor assurances, face liability regardless of vendor promises. Future regulatory expansion includes federal legislation proposals, additional state laws (California, New Jersey considering regulations), detailed EEOC guidance on specific tools, increasing class action litigation challenging AI hiring systems.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - employment-law
      - ai-hiring
      - discrimination
      - title-vii
      - adea
      - ada
    example_cards:
      - algorithmic-bias-case-studies
      - ai-governance-use-cases
      - ai-safety-incidents-case-studies
  - id: resp-3
    title: AI and Intellectual Property - Copyright, Patents, and Trade Secrets
    slug: ai-and-intellectual-property-copyright-patents-and-trade-secrets
    path: responsibility
    source_file: content/articles/final/ai-and-intellectual-property-copyright-patents-and-trade-secrets.md
    tldr: AI disrupts intellectual property fundamentals - Copyright Office requires human authorship meaning AI-generated content without meaningful human creative input cannot be copyrighted (Zarya of the Dawn 2023 case granted copyright to human-authored text and selection/arrangement but denied for individual AI images because Midjourney users cannot predict/control specific expression unlike cameras/Photoshop). Training data controversy centers on whether using copyrighted works to train AI constitutes infringement - copyright holders argue copying occurs during training, models are derivative works, market harm from substitute content, style theft profits from creators' life work; AI developers claim fair use through transformative purpose (learning patterns not copying), with major lawsuits testing questions (Getty v. Stability AI showing watermark reproductions, NYT v. OpenAI/Microsoft alleging verbatim reproduction threatening business model, Andersen v. Stability AI/Midjourney/DeviantArt challenging style copying, Tremblay v. OpenAI/Meta over book training). AI output infringement occurs through direct verbatim copying (ChatGPT reciting book portions, images containing distorted watermarks evidencing memorization) and substantial similarity raising questions whether style-matching output infringes despite style itself not being copyrightable. Patent law globally rejects AI as inventor (US, UK, EPO rejecting DABUS applications requiring "natural persons," Australia initial acceptance overturned) creating uncertainty where genuine AI inventive contributions may render inventions unpatentable with no proper inventor. Trade secrets gain importance for AI protecting training data/methods, model architecture/parameters, AI-generated innovations through confidentiality since copyright/patent protections fail, though lost upon disclosure and providing no protection against independent development/reverse engineering. Business implications include AI-generated content entering public domain enabling free competitor copying, requiring human creative contribution documentation for copyright claims, considering trade secret/contract/first-mover advantage alternatives. Training data licensing markets emerging if courts hold training constitutes infringement creating massive development barriers and existing model liability, or if fair use prevails creators lack remedies with severe economic impacts potentially prompting mandatory compensation regulations. Output liability questions whether AI developers providing infringement-enabling tools and/or users generating infringing content bear responsibility with vendors typically shifting liability through terms of service though contractual terms don't eliminate legal obligations. International approaches vary - EU Text and Data Mining exceptions for research (broad) and commercial (narrower with opt-outs) plus AI Act training data disclosure requirements, UK TDM exception for non-commercial research with commercial expansion abandoned after creator pushback, Japan broad exception for non-expressive information analysis, China Generative AI regulations requiring IP respect with developing enforcement. Future developments may include Congressional copyright law amendments creating training exceptions or licensing requirements, collective licensing organizations similar to music frameworks, content authentication/watermarking/detection technical solutions, international harmonization efforts despite difficulty. Organizations must understand uncertain AI-IP rules, document human creative contributions supporting IP claims, develop strategies accounting for copyright/patent limitations including trade secrets/contracts/first-mover advantage, prepare for rapid rule changes, consider ethical fairness of using creators' work without compensation/consent beyond legal compliance.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - intellectual-property
      - copyright
      - patents
      - trade-secrets
      - ai-generated-content
      - training-data
    example_cards:
      - ai-governance-use-cases
      - ai-safety-incidents-case-studies
      - generative-ai-systems-comparison
  - id: resp-4
    title: AI Governance Frameworks - Building Your Organization's Approach
    slug: ai-governance-frameworks-building-your-organizations-approach
    path: responsibility
    source_file: content/articles/final/ai-governance-frameworks-building-your-organizations-approach.md
    tldr: Building effective AI governance requires understanding foundational frameworks - OECD's updated AI definition (machine-based systems inferring how to generate outputs influencing environments), ISO/IEC 22989 standardized terminology now freely available for global alignment, and OECD's five-dimension classification framework (People & Planet impacts, Economic Context, Data & Input, AI Model characteristics, Task & Output). AI systems are socio-technical combining technology with human agents and institutions, requiring interdisciplinary expertise beyond computer science. Five core AI use cases include recognition/detection (patterns, faces, speech, anomalies), forecasting/prediction (future events, behaviors, risks), personalization (tailored content, recommendations), goal-driven optimization (routing, scheduling, resource allocation), and content generation (text, images, code, music). Organizations must recognize that AI governance requires expertise from UX designers, anthropologists, sociologists, ethicists, and domain experts addressing workflow integration and human factors from design outset. Different AI systems require distinct governance approaches - smartphone facial recognition versus mass surveillance, virtual assistants versus self-driving vehicles, each demanding sector-specific frameworks. The OECD Classification Framework enables AI inventories, risk assessment, incident reporting, and cross-jurisdictional policy discussions with common language. Understanding these building blocks provides governance professionals shared vocabulary to work across disciplines, organizations, and borders as AI becomes increasingly prevalent.
    content_sections:
      - Introduction
      - Core Concepts
      - Practical Applications
      - Key Takeaways
    tags:
      - oecd
      - ai-definition
      - iso-iec-22989
      - classification-framework
      - socio-technical
      - ai-governance
    example_cards:
      - ai-governance-use-cases
      - responsible-ai-governance-case-studies
      - ai-safety-incidents-case-studies
  - id: resp-5
    title: AI Regulatory Sandboxes - Testing Innovation Safely
    slug: ai-regulatory-sandboxes-testing-innovation-safely
    path: responsibility
    source_file: content/articles/final/ai-regulatory-sandboxes-testing-innovation-safely.md
    tldr: Regulatory sandboxes provide controlled environments for testing innovative AI products or services under regulatory supervision with reduced or modified regulatory requirements during testing period, direct engagement with regulators enabling guidance and feedback, limited scope constraining time/customer numbers/use cases, enhanced monitoring and reporting creating transparency, clear entry and exit criteria establishing boundaries differing from regular regulation where rules apply fully from day one, limited regulator involvement, compliance solely company responsibility, one-size-fits-all requirements, violations resulting in penalties, no time limits. Sandboxes NOT free passes (participants still have consumer protection/safety/data protection obligations), NOT permanent (temporary testing with post-sandbox compliance required), NOT guaranteed approval (testing might reveal problems requiring discontinuation), NOT available to everyone (eligibility criteria limiting participation). Typical five-step process includes application explaining AI system/innovation/regulatory uncertainties/consumer protection/learning goals, assessment evaluating innovation potential/regulatory uncertainty/consumer benefit/risk level/company capability, negotiation determining testing scope/modified requirements/reporting obligations/exit conditions/timeline, testing period with operation under agreed conditions/regular reporting/adjustment meetings/adaptations, exit evaluation with results assessment/regulatory treatment decision/company compliance/modification/market exit. EU AI Act Articles 57-62 require member states establish AI regulatory sandboxes by August 2026 mandating at least one AI sandbox per member state, national competent authority operation/supervision, special SME/startup provisions, innovation support ensuring safety, cross-border cooperation enabling testing across jurisdictions with participation allowed for companies developing AI systems prioritizing SMEs and startups, focusing high-risk AI systems, permitting real-world testing offering guidance on regulatory compliance, reduced requirements during testing, priority regulatory question processing, potential conformity assessment path while maintaining fundamental rights protections, informed consent from participants, exit mechanisms if risks materialize, liability for harms creating balanced approach.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - sandboxes
      - regulatory-innovation
      - eu-ai-act
      - testing
      - compliance
      - sme-support
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - sandbox-participation-case-studies
  - id: resp-6
    title: AI Transparency - What Users Deserve to Know
    slug: ai-transparency-what-users-deserve-to-know
    path: responsibility
    source_file: content/articles/final/ai-transparency-what-users-deserve-to-know.md
    tldr: Users deserve to know when AI is involved (pre-use disclosure), can distinguish AI from humans (real-time transparency), understand key factors in decisions (post-decision explanation), and access system-level information (performance, training data, governance). Regulations increasingly require transparency (GDPR Article 22, EU AI Act, NYC Local Law 144, Colorado AI Act). Implementation requires documentation practices (Model Cards, Datasheets), explanation techniques (feature attribution, counterfactuals), accessible interface design, and organizational processes. Transparency involves tradeoffs with IP protection, gaming prevention, security, comprehension, and liability management.
    content_sections:
      - Introduction
      - Core Concepts
      - Practical Applications
      - Key Takeaways
    tags:
      - transparency
      - explainability
      - disclosure
      - user-rights
      - model-cards
      - regulation
    example_cards:
      - ai-transparency-hiring-platform
      - ai-transparency-compliance-framework
      - ai-transparency-healthcare-infrastructure
  - id: resp-7
    title: Brazil's AI Bill - Regulation in Latin America
    slug: brazils-ai-bill-regulation-in-latin-america
    path: responsibility
    source_file: content/articles/final/brazils-ai-bill-regulation-in-latin-america.md
    tldr: Brazil establishes comprehensive AI regulatory framework building on Lei Geral de Proteção de Dados (LGPD) General Data Protection Law experience from 2020 teaching implementation matters (having law just start, enforcement requires resources), international alignment helps (LGPD-GDPR similarity facilitated global company compliance), local adaptation necessary (Brazilian specifics like different legal basis structure), consumer awareness grows (Brazilians increasingly understand data rights) with PL 2338/2023 introduced 2023 undergoing committee reviews 2024 expected enactment 2025 after stakeholder input from tech industry, civil society, academics, government agencies creating relatively balanced comprehensive proposal. Bill establishes risk-based classification similar to EU AI Act with four tiers - excessive risk prohibited (social scoring by public authorities, vulnerable group exploitation, subliminal manipulation, indiscriminate public facial recognition with exceptions), high risk requiring enhanced obligations (education admission/assessment, employment hiring/evaluation/termination, credit/financial services, healthcare, law enforcement, migration/border control, democratic processes), substantial risk moderate concerns (certain biometric systems, AI with vulnerable groups, other designated systems), low risk minimal obligations beyond general transparency. Strong individual rights framework distinguishes Brazil's approach creating comprehensive rights set - right to information (know when AI used in decisions), right to explanation (understand how AI decisions made), right to human review (request person review AI decisions), right to contest (challenge AI decisions), right to non-discrimination (AI cannot discriminate based on protected characteristics), right to data correction (correct inaccurate data used by AI) with example job applicant rejected by AI screening can know AI involved, understand factors considered, request human review, contest decision, ensure no discrimination, correct inaccurate data demonstrating practical application.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - brazil-ai
      - pl-2338
      - lgpd
      - anpd
      - latin-america
      - risk-based
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - algorithmic-bias-case-studies
  - id: resp-8
    title: California AI Regulations - The Golden State's Approach
    slug: california-ai-regulations-the-golden-states-approach
    path: responsibility
    source_file: content/articles/final/california-ai-regulations-the-golden-states-approach.md
    tldr: California establishes "Sacramento Effect" where state's AI regulations become de facto national standards due to 39 million residents (12% US population), GDP exceeding most countries, tech headquarters concentration, compliance complexity favoring national adoption over location-specific versions. Unlike EU's comprehensive AI Act, California pursues patchwork approach with multiple targeted laws addressing specific AI harms - generative AI transparency (AB 2013 requiring training data disclosure effective Jan 2026 documenting datasets/personal info/copyrighted material/sources), AI content detection (SB 942 requiring provenance tools and manifest metadata), bot disclosure (SB 1001 prohibiting bots misleading humans in commercial/voting contexts without disclosure, AB 587 social media bot transparency), deepfake regulations (AB 730 election deepfakes ban 60 days before voting, AB 602 non-consensual pornography civil liability, AB 1836 deceased individuals digital likeness protection, AB 2602 performer digital replica consent), vetoed AI safety bill (SB 1047 proposing large model safety testing/kill switches/third-party audits/whistleblower protections for $100M+ training compute rejected Sept 2024 as premature though influencing national conversation), employment AI (FEHA applying to discriminatory hiring tools, CCPA/CPRA automated decision-making rights including opt-out and logic access, Civil Rights Department developing specific rules), children protections (Age-Appropriate Design Code requiring best interests consideration and DPIAs for services accessed by minors). Patchwork approach advantages include faster passage of smaller bills, targeted harm addressing, easier updates, less per-bill opposition while disadvantages create coverage gaps, multiple compliance requirements, no unified framework, potential inconsistencies. Compliance considerations require businesses determine which laws apply mapping AI activities (generative AI development triggers AB 2013/SB 942, chatbots trigger SB 1001, hiring tools trigger FEHA/CCPA, content recommendation triggers AADC if minors, synthetic media triggers deepfake laws), assess California nexus (company location, serving residents, affecting residents), implement required measures (training data documentation, bot disclosures, discrimination testing, impact assessments), monitor rapid developments (new bills each session, ongoing agency rulemaking, developing court interpretations). Comparison with other jurisdictions shows California vs EU AI Act (multiple targeted laws vs single comprehensive, some use differentiation vs formal 4-tier, California residents vs EU market, penalties vary by law vs up to 7% revenue), vs Colorado (multiple issues vs consumer decisions focus, proposed assessments vs required, training data transparency vs not addressed), vs NYC LL 144 (rules in development vs bias audits required, statewide vs city, various requirements vs AEDT-specific). Future developments likely include AI safety requirements returning post-SB 1047 veto, employment AI specific rules, healthcare AI diagnosis/treatment regulations, financial services lending/insurance rules, government AI public sector restrictions, agency rulemaking from CPPA on automated decisions and Civil Rights Department on discrimination guidance. California's targeted measures create substantial compliance obligations demonstrating transparency theme (disclosure about AI use, training data, decision-making consistently required), specific harm addressing (deepfakes, discrimination, children's safety), evolving landscape requiring current monitoring, leadership position where Sacramento often becomes national practice making robust California-aligned governance programs position organizations well for future regulatory developments.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - california-ai
      - sacramento-effect
      - ab-2013
      - sb-942
      - deepfakes
      - bot-disclosure
    example_cards:
      - ai-governance-use-cases
      - algorithmic-bias-case-studies
      - ai-safety-incidents-case-studies
  - id: resp-9
    title: China's AI Governance - A Different Model
    slug: chinas-ai-governance-a-different-model
    path: responsibility
    source_file: content/articles/final/chinas-ai-governance-a-different-model.md
    tldr: China establishes distinctive AI governance model reflecting fundamentally different assumptions from Western approach - Western EU/US prioritizes individual rights as primary concern with government regulation constraining corporate power, privacy/autonomy as core values, suspicion of state surveillance while Chinese approach prioritizes social harmony/stability with government-business cooperation for national development, collective welfare alongside individual rights, state capacity managing information environment pursuing "regulation-innovation balance" where government creates controlled greenhouse environment enabling AI rapid growth in desired directions through industry-specific rules targeting particular applications, quickly enacted faster than democratic legislatures, adaptive adjusting based on industry response, strategically aligned supporting national AI development goals. Key regulations create comprehensive application-specific framework - Algorithm Recommendation Rules (2022) regulating social media feeds, e-commerce recommendations, news aggregation, search results requiring transparency (users understand why content recommended), opt-out rights (refuse personalized recommendations), no addiction promotion (algorithms can't deliberately create addictive patterns), labor protection (delivery/gig economy fair treatment), content alignment (recommendations match socialist core values) enforced by Cyberspace Administration of China (CAC). Deep Synthesis Regulations (2023) governing deepfakes, face-swapping, voice cloning, AI-generated images/video/audio requiring clear labeling (synthetic content labeled), consent (creating synthetic representations requires permission), identity verification (provider verify users), no harmful use (prohibit fake news/fraud/defamation), traceability (logs enabling content tracing), technical requirements (metadata/watermarks indicating synthetic nature). Generative AI Regulations (2023) controlling ChatGPT-style services requiring content safety (no subverting state power/harming national security/promoting terrorism/undermining social stability), training data requirements (legal data not infringing intellectual property), security assessments (before public launch), user verification (real-name registration), labeling (AI-generated content marked), complaints mechanism (report problematic content), core socialist values reflection (promoting patriotism/integrity/social harmony, avoiding CCP contradiction) with registration requirement effectively preventing Western services like ChatGPT operating without significant modifications creating separate Chinese AI market where over 100 services registered by late 2024.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - china-ai
      - regulation
      - cac
      - algorithm-rules
      - deepfakes
      - generative-ai
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - algorithmic-bias-case-studies
  - id: resp-10
    title: Consumer Protection and AI - FTC Section 5 Explained
    slug: consumer-protection-and-ai-ftc-section-5-explained
    path: responsibility
    source_file: content/articles/final/consumer-protection-and-ai-ftc-section-5-explained.md
    tldr: FTC Section 5 prohibits unfair or deceptive trade practices, providing broad authority over AI systems that harm consumers. Major enforcement actions include Rite Aid (facial recognition bias), Amazon/Ring (privacy violations), Weight Watchers/Kurbo (children's data), and Cambridge Analytica (data harvesting). The FTC's "algorithmic disgorgement" remedy requires deletion of AI models trained on improperly collected data. Practical compliance requires substantiating AI claims, testing for discrimination, reviewing data collection practices, monitoring deployed systems, and preparing for potential FTC investigations.
    content_sections:
      - Introduction
      - Core Concepts
      - Practical Applications
      - Key Takeaways
    tags:
      - ftc
      - section-5
      - consumer-protection
      - enforcement
      - algorithmic-disgorgement
      - regulation
    example_cards:
      - ftc-section-5-enforcement-actions
  - id: resp-11
    title: 'Cross-Border AI Compliance: Navigating Multiple Jurisdictions'
    slug: cross-border-ai-compliance-navigating-multiple-jurisdictions
    path: responsibility
    source_file: content/articles/final/cross-border-ai-compliance-navigating-multiple-jurisdictions.md
    tldr: Cross-border AI compliance represents one of the most complex challenges in AI governance as companies navigate extraterritorial laws, conflicting requirements, and varied enforcement regimes across multiple jurisdictions. Many AI laws explicitly apply beyond their borders through extraterritorial reach provisions—the EU AI Act applies to any provider placing AI systems on the EU market regardless of location, following the GDPR precedent that made Brussels Effect regulations global standards. US state laws like NYC Local Law 144 apply to anyone using automated employment decision tools for jobs in their jurisdiction even if headquartered elsewhere. Data flows create regulatory connections as AI systems move training data, processing, and outputs across borders, potentially triggering different countries' laws at each stage. Global customers mean global laws—serving international clients subjects companies to their local requirements. Major compliance challenge points include conflicting requirements (EU transparency vs. China algorithm registration vs. trade secret protection; China data localization vs. EU GDPR restrictions; China content standards vs. Western speech protections), varying AI definitions across EU AI Act/OECD/Colorado frameworks creating uncertainty about coverage, timing differences as laws take effect at staggered dates (EU prohibited practices Feb 2025, GPAI Aug 2025, high-risk Aug 2026; Colorado Feb 2026; NYC already effective), and enforcement uncertainty with dramatically different approaches across jurisdictions. Practical strategies for multi-jurisdictional compliance include mapping your exposure through comprehensive jurisdictional mapping documenting company locations/servers/customers/users/data sources, building to the highest standard leveraging Brussels Effect by meeting strictest requirements globally (typically EU standards satisfying other jurisdictions while future-proofing), creating modular compliance frameworks with core global modules plus regional add-ons for EU-specific/US-specific/China-specific requirements, establishing clear governance with central oversight and local expertise coordinated through regular cross-jurisdiction reviews, and planning for conflicts through structured conflict resolution frameworks assessing risks/exploring solutions/documenting decisions when requirements genuinely conflict. Industry-specific considerations vary—financial services faces complex existing international banking regulations requiring regulator engagement and strong model risk management; healthcare navigates medical device regulations, health data protection, and professional licensing across jurisdictions; employment/HR tech addresses NYC Local Law 144, EU AI Act high-risk designation, and anti-discrimination laws everywhere; retail/e-commerce manages consumer protection, advertising, pricing transparency, and accessibility requirements globally. Tools include regulatory tracking services (OECD AI Policy Observatory, IAPP tracker), legal support from global firms or industry associations, and compliance management systems for centralized documentation. The future likely involves partial harmonization—OECD principles, G7 discussions, UNESCO recommendations, and trade agreements suggest some alignment, but fundamental differences in political systems, economic interests, cultural values, and national security concerns mean most likely scenario combines principle-level harmonization with continued specifics differences and regional blocks (EU/US/China) with internal alignment. Companies succeeding in global AI markets treat cross-border compliance as competitive advantage through strong governance, clear documentation, and genuine responsible AI commitment that translates well across jurisdictions.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - cross-border-compliance
      - multi-jurisdictional
      - extraterritorial-reach
      - brussels-effect
      - jurisdictional-mapping
      - compliance-frameworks
    example_cards:
      - Japanese company selling AI diagnostic software to German hospital must comply with EU AI Act despite no European presence
      - Canadian AI vendor selling to French companies must meet EU AI Act requirements because French deployer needs compliant high-risk AI
      - AI hiring tool serving NYC jobs must comply with Local Law 144 regardless of employer headquarters location
  - id: resp-12
    title: Foundation Model Obligations - What the EU AI Act Requires
    slug: foundation-model-obligations-what-the-eu-ai-act-requires
    path: responsibility
    source_file: content/articles/final/foundation-model-obligations-what-the-eu-ai-act-requires.md
    tldr: EU AI Act Chapter V creates two-tier framework for general-purpose AI (GPAI) models - Tier 1 baseline obligations for ALL GPAI providers regardless of scale requiring technical documentation (training process/methods, model capabilities/limitations, testing/evaluation results), downstream provider information enabling compliance/high-risk requirement fulfillment, EU copyright law compliance policy respecting text/data mining rights reservation, publicly published training data summary helping rights holders understand content use with EU AI Office template. Tier 2 additional systemic-risk GPAI obligations for most powerful models defined by automatic 10^25 FLOPs training compute threshold (capturing GPT-4 scale) OR European Commission designation based on registered business users/downstream provider dependence/market impact/large-scale safety incident potential requiring model evaluations using standardized protocols with adversarial testing, systemic risk assessment/mitigation tracking sources/documenting/reporting serious incidents, adequate cybersecurity protecting model weights/endpoints/infrastructure, capabilities/limitations documentation with safety evaluation information/energy consumption. Open-source considerations provide exemptions for permissive-licensed models with publicly available parameters/architecture/weights from technical documentation/downstream information/copyright policy (still require training data summary) BUT systemic-risk open-source models above 10^25 FLOPs threshold or Commission-designated must comply with all systemic-risk requirements recognizing potential harm too great for exemption. Downstream providers using foundation models must classify applications independently (Llama model general-purpose but hiring tool using Llama is high-risk Annex III), receive entitled information about capabilities/limitations/risks/appropriate use from GPAI providers, remain responsible for integration/testing/documentation/user transparency regardless of base model with foundation model issues becoming application issues. EU AI Office oversees GPAI regulation facilitating codes of practice development providing detailed guidance for obligations (copyright compliance, risk evaluation) representing industry consensus with presumption of conformity, exercising powers including documentation/information requests, evaluations, corrective measures, non-compliance fines with August 2 2025 GPAI obligations applicable date, May 2 2025 codes of practice ready target, ongoing oversight. Foundation models are infrastructure like power grids/telecommunications where reliability/safety affects all dependents requiring tiered approach - all providers transparent about models, most powerful take systemic risk precautions, downstream users get better information/clearer expectations but responsibility for powerful tool use.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - foundation-models
      - gpai
      - eu-ai-act
      - systemic-risk
      - open-source-ai
      - copyright-compliance
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
  - id: resp-13
    title: Global AI Law Tracker - Who's Regulating What
    slug: global-ai-law-tracker-whos-regulating-what
    path: responsibility
    source_file: content/articles/final/global-ai-law-tracker-whos-regulating-what.md
    tldr: Global AI regulation creates complex landscape requiring systematic tracking as major jurisdictions establish distinctive approaches - European Union leads comprehensive binding regulation through AI Act (August 2024) with risk-based classification (prohibited/high/limited/minimal risk), strict high-risk requirements, foundation model obligations, massive penalties up to 7% global revenue, extraterritorial reach applying to non-EU companies serving EU markets creating phased implementation through 2027 regulating hiring/recruitment AI, credit/insurance scoring, law enforcement, educational assessment, critical infrastructure, biometric systems. United States follows patchwork approach with no comprehensive federal law but significant executive action (Executive Order 14110 October 2023), agency guidance (NIST AI RMF, FTC consumer protection), proposed legislation, plus state-level laws including Colorado AI Act 2024 (first comprehensive state law focusing high-risk consumer decisions requiring impact assessments effective February 2026), California multiple laws (AB 331 automated decision tools, SB 1047 AI safety amended, deepfake disclosure, multiple bills), NYC Local Law 144 (automated employment decision tools requiring annual bias audits already effective), Illinois (BIPA, AI Video Interview Act), Texas (proposed AI legislation government use) creating compliance complexity where multi-state hiring requires coordinating different bias audit/disclosure/impact assessment requirements. China establishes control model through multiple binding regulations (Algorithm Recommendation Regulation 2022, Deep Synthesis Deepfake Rules 2023, Generative AI Measures 2023) requiring algorithm registration with authorities, content moderation preventing illegal content, real-name user verification, socialist core values compliance, data localization with much heavier content control, government algorithm access, social stability focus, less individual rights emphasis than Western democracies. United Kingdom pursues pro-innovation principles-based approach via existing regulators applying five principles (safety, transparency, fairness, accountability, contestability) with sector-specific implementation through no single AI law, AI Safety Institute for advanced risks using multiple regulators (ICO, FCA, CMA, Ofcom, sector-specific) creating flexibility but potential consistency/enforcement questions.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - global-regulation
      - ai-tracker
      - eu-ai-act
      - us-patchwork
      - china-regulation
      - uk-framework
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - cross-border-ai-challenges
  - id: resp-14
    title: Human-Centered AI Design - Keeping People in the Loop
    slug: human-centered-ai-design-keeping-people-in-the-loop
    path: responsibility
    source_file: content/articles/final/human-centered-ai-design-keeping-people-in-the-loop.md
    tldr: Human-centered AI design applies established human-centered design principles to AI's unique challenges through six core principles - start with human needs (understand users/affected/served populations before building), design for augmentation not just automation (enhance human capabilities rather than replacement), maintain meaningful human control (humans retain agency and authority over important decisions), design for usability (systems understandable and operable by intended users), consider all stakeholders (not just direct users but all affected humans), and respect human dignity (treat people as ends not means). AI presents unique challenges requiring this approach - makes consequential decisions, operates opaquely making trust difficult, fails in unexpected ways unlike rule-based software, changes over time through learning, and involves probabilistic uncertainty. Six-level automation spectrum guides appropriate human involvement from Level 1 (human does everything) through Level 2 (AI suggests options), Level 3 (AI recommends one option), Level 4 (AI acts unless vetoed), Level 5 (AI acts then informs), to Level 6 (fully autonomous). Choosing the right level depends on stakes (higher stakes warrant more human involvement), time criticality (faster decisions may require higher automation), human expertise, AI reliability, error consequences, and user preferences. Effective human-AI collaboration leverages complementary strengths - AI excels at processing large data quickly, consistent pattern application, tireless operation, subtle pattern detection, and 24/7 availability while humans excel at context understanding, common sense application, novel situation adaptation, ethical judgment, human communication, creative problem-solving, and recognizing when something feels wrong. Communication design requires clarity, uncertainty indication, explanations, actionability, and appropriate detail for different users. Maintaining human agency prevents nudging/manipulation, learned helplessness from always-available AI answers, choice architecture abuse, invisible decision-making, and autonomy erosion. Implementation requires pre-development user research (interviews, observations, surveys), contextual inquiry of current workflows, stakeholder interviews beyond direct users, iterative design with early prototypes and real user testing, full experience design including error recovery and edge cases, and evaluation against human outcomes not just AI accuracy.
    content_sections:
      - Introduction
      - Core Concepts
      - Practical Applications
      - Key Takeaways
    tags:
      - human-centered-ai
      - human-in-the-loop
      - automation-levels
      - human-agency
      - usability
      - stakeholder-design
    example_cards:
      - human-centered-ai-implementation-examples
  - id: resp-15
    title: NYC Local Law 144 - Automated Employment Decision Tools
    slug: nyc-local-law-144-automated-employment-decision-tools
    path: responsibility
    source_file: content/articles/final/nyc-local-law-144-automated-employment-decision-tools.md
    tldr: NYC Local Law 144 (effective 2023) establishes first US regulation specifically targeting AI hiring tools requiring employers/agencies using Automated Employment Decision Tools (AEDTs) in NYC to conduct annual independent bias audits calculating selection rates and impact ratios across sex (male, female, non-binary/unknown) and race/ethnicity categories (White, Black, Hispanic/Latino, Asian, Native Hawaiian/Pacific Islander, American Indian/Alaska Native, two+ races) applying EEOC's four-fifths rule benchmark where protected group selection <80% highest group indicates potential adverse impact. Three core requirements mandate (1) independent auditor bias audit before use and annually thereafter testing disparate impact, (2) public disclosure of audit results on company website including dates, category distributions, selection/scoring rates, impact ratios with transparency enabling applicant review, (3) candidate notice at least 10 business days before AEDT use via job postings/email explaining assessed qualifications, alternative selection process availability, reasonable accommodation request procedures. Audit process involves data collection from historical candidates, category assignment identifying demographics, outcome tracking of selections, rate calculation per category, impact analysis comparing ratios, report generation documenting methodology. Real-world compliance challenges include data availability (candidates don't always provide demographics requiring historical data or proxy estimation), defining "substantially assist" (gray area determining what counts as substantial human judgment replacement), third-party tool responsibility (vendor vs employer audit obligations, data sharing refusals, shared audit questions), and audit costs ($5K-$50K+ creating burden for smaller employers). Enforcement by NYC Department of Consumer and Worker Protection imposes penalties up to $500 first violation, $1,500 subsequent violations with each day as separate violation though late 2024 enforcement remains light focusing on education/outreach. Law influences beyond NYC - Colorado AI Act includes employment as consequential decision, Illinois strengthens AI Video Interview Act enforcement, California considers similar disclosure requirements, EEOC references LL 144 in AI guidance, vendors increasingly offer built-in bias auditing, national employers adopt LL 144-like practices company-wide for simplicity and preparation for expected regulation elsewhere. Compliance best practices require employers to inventory hiring tools, assess AEDT coverage, engage auditors early, coordinate with vendors requesting bias documentation/audit support, update job postings with notice language, train recruiters on requirements, prepare for candidate questions about alternatives, document everything. NYC LL 144 milestone establishes accountability principle - when AI makes livelihood decisions, transparency and bias testing are mandatory, though law has limitations and critics from all sides (employers find burdensome/vague, advocates find narrow/weak enforcement, vendors worry about competitive disadvantage/gaming risk).
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - nyc-law-144
      - aedt
      - bias-audits
      - employment-ai
      - hiring-algorithms
      - compliance
    example_cards:
      - algorithmic-bias-case-studies
      - ai-governance-use-cases
      - ai-safety-incidents-case-studies
  - id: resp-16
    title: Singapore's AI Framework - The Business-Friendly Approach
    slug: singapores-ai-framework-the-business-friendly-approach
    path: responsibility
    source_file: content/articles/final/singapores-ai-framework-the-business-friendly-approach.md
    tldr: Singapore establishes distinctive AI governance model achieving outsized influence despite 6 million population through financial center status as Asia's leading hub with respected regulatory standards, tech hub with major companies' Asia-Pacific operations, government capacity for effective forward-thinking policy, bridge position connecting East and West regulatory approaches, ASEAN influence shaping 700 million people across Southeast Asia. Early mover beginning development 2018 before EU AI Act/US state laws enabling extensive stakeholder consultation, iterative refinement based on feedback, practical testing through industry pilots, real-world adjustment creating Model AI Governance Framework first released 2019 updated 2020 providing practical actionable framework for organizations deploying AI - importantly guidance not law (organizations not legally required to follow) though becoming de facto standard with compliance increasingly expected by regulators in specific sectors. Framework built on two overarching principles - organizations using AI should ensure decisions explainable/transparent/fair, AI solutions should be human-centric - organizing guidance around four priority areas including internal governance structures (roles/responsibilities, senior management accountability, staff training, regular review, documentation/audit trails), determining AI decision-making model (human-in-the-loop for high-stakes, human-over-the-loop for moderate-stakes, human-out-of-the-loop for low-stakes reversible decisions based on risk not blanket requirements), operations management (data management, performance monitoring, error handling, user feedback, audit capabilities), stakeholder interaction and communication (AI disclosure, decision explanation, question channels, vulnerable population consideration). A.I. Verify addresses implementation challenge where frameworks tell what to do but not how to verify compliance through testing framework and software toolkit helping organizations assess AI systems providing testing capabilities (robustness, fairness, transparency, safety, accountability evaluation), process guidance (step-by-step assessment instructions), reporting templates (standardized documentation), open source tools (freely available for use/adaptation) with typical assessment defining scope, selecting tests, running technical tests, conducting process review, generating report, addressing gaps, iterating regularly.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - singapore-ai
      - model-framework
      - ai-verify
      - feat-principles
      - asean
      - voluntary-guidance
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - ai-safety-incidents-case-studies
  - id: resp-17
    title: The EU AI Act - Europe's Landmark Regulation Explained
    slug: the-eu-ai-act-europes-landmark-regulation-explained
    path: responsibility
    source_file: content/articles/final/the-eu-ai-act-europes-landmark-regulation-explained.md
    tldr: EU AI Act establishes world's first comprehensive AI regulatory framework using risk-based approach with four levels - unacceptable risk practices banned (social scoring, subliminal manipulation, real-time public facial recognition with exceptions, exploiting vulnerabilities), high-risk systems heavily regulated (employment, credit scoring, education, law enforcement, critical infrastructure requiring risk management, data quality, documentation, logging, transparency, human oversight, accuracy/robustness/security), limited-risk systems requiring transparency (chatbots must disclose, emotion recognition must inform, deepfakes must label), minimal-risk systems facing no special rules (spam filters, games, inventory management). Act applies extraterritorially to providers placing systems on EU market, deployers using systems in EU, systems whose output used in EU regardless of location with phased enforcement timeline - prohibited practices February 2025, GPAI rules August 2025, high-risk requirements August 2026. Foundation models and general-purpose AI face two-tier obligations - all GPAI providers must create documentation, provide downstream information, comply with copyright, publish training data summary while systemic-risk GPAI (trained >10^25 FLOPs or designated) must additionally conduct evaluations, mitigate systemic risks, report incidents, ensure cybersecurity. High-risk AI requires conformity assessment (mostly self-assessment), ongoing monitoring, serious incident reporting, documentation updates when changed. Brussels Effect strategy aims to establish global baseline where companies apply EU standards worldwide rather than maintaining separate versions with history suggesting other jurisdictions will follow similar to GDPR adoption pattern.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - eu-ai-act
      - regulation
      - risk-based-approach
      - high-risk-ai
      - prohibited-ai
      - gpai
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - algorithmic-bias-case-studies
  - id: resp-18
    title: The Legal Patchwork - Existing Laws That Apply to AI
    slug: the-legal-patchwork-existing-laws-that-apply-to-ai
    path: responsibility
    source_file: content/articles/final/the-legal-patchwork-existing-laws-that-apply-to-ai.md
    tldr: You don't need AI-specific laws to be liable for AI discrimination—civil rights laws from the 1960s-1990s apply with full force to algorithmic decisions. Title VII, ECOA, Fair Housing Act, ADA, and ADEA prohibit discrimination whether committed by humans or machines. Disparate impact doctrine means unintentional discrimination is still illegal. Organizations must test AI for adverse impact using the four-fifths rule, validate job-relatedness, document efforts, and build anti-discrimination into AI development from the start.
    content_sections:
      - Introduction
      - Core Concepts
      - Practical Applications
      - Key Takeaways
    tags:
      - legal-framework
      - civil-rights
      - anti-discrimination
      - compliance
      - employment-law
      - lending-law
    example_cards:
      - algorithmic-bias-case-studies
  - id: resp-19
    title: The Right to Explanation - What GDPR Actually Requires
    slug: the-right-to-explanation-what-gdpr-actually-requires
    path: responsibility
    source_file: content/articles/final/the-right-to-explanation-what-gdpr-actually-requires.md
    tldr: GDPR "right to explanation" debate centers on whether Articles 13/14/15 requiring "meaningful information about the logic involved" plus Article 22(3) safeguards create individual decision explanation right or only general system information requirement - phrase "right to explanation" doesn't appear in binding articles though Recital 71 non-binding text mentions "right to obtain an explanation of the decision reached." GDPR clearly requires six transparency obligations - disclose automated decision-making existence at data collection (Articles 13/14) and on request (Article 15), provide meaningful information about logic involved explaining what factors considered/how weighted/decision process/thresholds or rules without disclosing proprietary algorithms/source code/mathematical formulas/trade secrets, explain significance and consequences describing decision effects/downstream impacts, enable individuals to contest decisions through clear accessible mechanisms with genuine consideration, provide human intervention on request where reviewer has authority to change decision with genuine not pro-forma review, allow individuals to express views by submitting additional information actually considered. Regulatory guidance increasingly supports individual explanations for consequential decisions - UK ICO states organizations should provide "basic explanation of decision reached" and "insight into main factors," EDPB emphasizes "meaningful information" should help "understand reasons behind decision," French CNIL requires explanations helping individuals understand decisions and exercise rights effectively. Individual explanations should address key influencing factors, direction of influence (helped/hurt), relative importance, actionable information about what could change for different outcome. AI explainability technical challenge creates tension - deep learning models genuinely difficult to explain with millions of parameters and complex non-linear relationships where even developers may not fully understand specific predictions, requiring accuracy vs simplicity balance, global vs local explanation distinction, post-hoc explanation techniques that may not capture actual reasoning. Explainability approaches include feature importance (SHAP, LIME identifying influential inputs), counterfactual explanations ("if income €5K higher would approve"), rule extraction approximating complex models with interpretable rules, attention mechanisms showing input focus areas, inherently interpretable models (decision trees, linear regression, rule-based) for high-stakes decisions. Organizations must choose appropriate models considering interpretability needs, build explanation capability incorporating SHAP/LIME/counterfactuals, test explanations on real users verifying actual meaningfulness, be honest about limitations if approximate/simplified, layer information providing simple explanations initially with detailed options. Sector-specific requirements add obligations - financial services Consumer Credit Directive requiring credit decision explanations with adverse decision reasons, employment facing NYC Local Law 144 bias audit disclosure and EU AI Act high-risk transparency/oversight, insurance Distribution Directive requiring decision explanations.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - gdpr
      - right-to-explanation
      - transparency
      - explainability
      - article-22
      - meaningful-information
    example_cards:
      - ai-governance-use-cases
      - algorithmic-bias-case-studies
      - ai-safety-incidents-case-studies
  - id: resp-20
    title: UK AI Regulation - The Pro-Innovation Framework
    slug: uk-ai-regulation-the-pro-innovation-framework
    path: responsibility
    source_file: content/articles/final/uk-ai-regulation-the-pro-innovation-framework.md
    tldr: UK establishes distinctive pro-innovation AI regulatory approach diverging from EU's comprehensive legislation model reasoning that overly prescriptive rules could stifle innovation where AI develops so quickly that detailed laws would be outdated before enactment, plus practical consideration that UK wants to attract AI companies and investment where choosing between heavy versus lighter regulation countries many startups choose easier path. Government stated goals include making UK global AI leader, encouraging responsible innovation, protecting people without creating unnecessary barriers, using existing regulatory expertise rather than building new bureaucracies creating framework around five cross-sector principles all regulators should apply - Safety/Security/Robustness (AI systems work reliably and securely, resistant to manipulation, handle errors gracefully), Appropriate Transparency/Explainability (people understand when AI used and how it works, honest about AI's role without requiring source code publication), Fairness (AI systems shouldn't discriminate unlawfully or create unfair outcomes connecting to existing equality laws applied algorithmically), Accountability/Governance (someone responsible when things go wrong, clear governance structures, known accountability for AI decisions), Contestability/Redress (people can challenge AI decisions affecting them, get meaningful responses, correct mistakes). Regulator-led approach assigns each existing sector regulator to apply five principles in their domain - Financial Conduct Authority (FCA) handles banking/investments/insurance AI including algorithmic trading/credit decisions/automated financial advice, Ofcom deals with communications/broadcasting AI including content moderation/deepfakes/AI-generated media, Competition and Markets Authority (CMA) examines how AI affects market competition investigating foundation model concerns, Information Commissioner's Office (ICO) oversees AI and data protection with significant influence since most AI needs data, Medicines and Healthcare products Regulatory Agency (MHRA) regulates healthcare AI from diagnostics to drug development, Health and Safety Executive (HSE) looks at workplace safety AI including autonomous vehicles and industrial robots creating distributed sectoral expertise rather than centralized AI regulator.
    content_sections:
      - Legal Framework Overview
      - Key Requirements
      - Compliance Implications
      - Enforcement and Penalties
    tags:
      - uk-ai
      - pro-innovation
      - five-principles
      - regulator-led
      - fca
      - ico
    example_cards:
      - ai-governance-use-cases
      - ai-regulatory-compliance-examples
      - ai-safety-incidents-case-studies
concept_cards_future:
  - id: future-1
    title: Artificial General Intelligence - Hype, Hope, and Governance
    slug: artificial-general-intelligence-hype-hope-and-governance
    path: future
    source_file: content/articles/final/artificial-general-intelligence-hype-hope-and-governance.md
    tldr: Artificial General Intelligence (AGI) is hypothetical AI system with human-level ability to understand, learn, and apply knowledge across any intellectual domain—fundamentally different from current Narrow AI excelling at specific tasks. Current best AI (GPT-4, Claude, Gemini) demonstrates impressive language/image capabilities but lacks genuine understanding, robust reasoning, common sense, learning efficiency, universal transfer learning, autonomous goal-directed behavior, self-improvement, physical world understanding, and long-term planning. Expert timeline predictions vary wildly - 2022 AI researcher survey found median 50% chance of human-level AI by 2059 with massive disagreement (10% before 2030, 10% after 2100, many "no idea") reflecting measurement uncertainty, unknown unknowns, exponential unpredictability, and incentive distortions. AGI hype driven by investment fuel, media appeal, Silicon Valley culture, genuine progress creates costs - misallocation of governance attention away from current AI harms, investment distortion, public confusion about actual capabilities, "boy who cried wolf" effect dismissing legitimate concerns. AGI matters for governance despite uncertainty through asymmetric risk (preparation cost limited if never happens, consequences severe if unprepared), pursuit effects (increasingly capable systems, concentrated capability, risk-taking culture, governance gaps), and potential catastrophic risks (misalignment pursuing wrong goals, control problems, power concentration, economic disruption, weaponization, unknown unknowns). Current governance approaches include leading labs (OpenAI explicit AGI mission with safety teams, Anthropic responsible scaling with Constitutional AI, DeepMind safety research), governments (US Executive Orders/NIST framework, EU AI Act limited AGI provisions, UK AI Safety Summit/Frontier AI Task Force, China strategic development), international efforts (UN discussions, G7 principles, OECD Observatory) but face limitations from narrow AI focus in most regulations, voluntary commitments without legal requirements, jurisdictional gaps enabling regulatory arbitrage, enforcement verification challenges, speed mismatches between slow governance and fast development. Organizations should focus on governing current AI while monitoring developments, conduct AGI scenario planning for industry impacts, build flexible governance frameworks adaptable to change, maintain fundamentals (transparency, accountability, human oversight), engage thoughtfully staying informed without hype capture. Separating signal from noise requires evaluating source credibility (peer-reviewed over social media, track record, conflicts of interest), looking for demonstration specifics and limitations, checking independent replication, considering capability baselines, recognizing red flags ("changes everything" without explanation, brain comparisons ignoring differences, linear-to-exponential extrapolations, dismissing skepticism, citing only optimists).
    content_sections:
      - Current State
      - Emerging Trends
      - Future Implications
      - Preparation Strategies
    tags:
      - agi
      - artificial-general-intelligence
      - narrow-ai
      - hype
      - timelines
      - ai-safety
    example_cards:
      - ai-safety-incidents-case-studies
      - ai-governance-use-cases
      - generative-ai-systems-comparison
  - id: future-2
    title: The Future of AI Regulation - What's Coming Next
    slug: the-future-of-ai-regulation-whats-coming-next
    path: future
    source_file: content/articles/final/the-future-of-ai-regulation-whats-coming-next.md
    tldr: AI regulation is accelerating globally. EU AI Act sets the standard with risk-based requirements. US has federal executive orders plus state laws. China implements content-control regulations. Key trends - risk-based regulation, transparency requirements, sector-specific rules, accountability mechanisms, and international fragmentation. Organizations must build governance infrastructure now.
    content_sections:
      - Current State
      - Emerging Trends
      - Future Implications
      - Preparation Strategies
    tags:
      - regulation
      - eu-ai-act
      - compliance
      - policy
      - legal-framework
      - governance
    example_cards:
      - ai-regulatory-landscape-global-comparison
      - ai-governance-use-cases
      - responsible-ai-governance-case-studies
